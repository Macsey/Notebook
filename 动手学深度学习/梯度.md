### 一、先明确：偏导在 SGD 里到底算什么？

在 SGD 中，我们要优化的是 “模型参数”（比如权重 w、偏置 b），目标是让 “损失函数” 最小化。而**偏导数**，就是衡量 “某一个参数变一点点时，损失会变多少” 的工具 —— 它像一把 “尺子”，告诉我们：要减小损失，这个参数该往大了调还是往小了调，调多少合适。

举个通俗的例子：假设损失函数 L 像一块 “高低不平的地面”，参数 w 是 “地面上的 x 轴”，参数 b 是 “地面上的 y 轴”。那么∂L/∂w（L 对 w 的偏导）就是 “沿着 x 轴方向走一步，地面高度的变化”；∂L/∂b 就是 “沿着 y 轴方向走一步，地面高度的变化”。SGD 就是靠这两个 “变化值”，调整 w 和 b 的大小。

### 二、求解偏导的核心逻辑：“固定其他参数，只看一个参数的影响”

偏导数的 “偏” 字，意思是 “只关注一个变量”。计算 L 对某个参数（比如 w）的偏导时，**要把其他所有参数（比如 b、其他权重）都当成 “固定不变的常数”**，只看 w 变化时 L 的变化率。

就像你在玩 “找茬游戏”：要找 “w 的变化对 L 的影响”，就先把 b 等其他参数 “冻结”，只动 w，观察 L 怎么变 —— 这个 “变化比例” 就是偏导。

### 三、案例 1：线性回归中偏导的具体计算（最基础场景）

线性回归是最简单的模型，它的预测公式和损失函数都很直观，适合入门偏导计算。我们结合 SGD 的 “单样本更新” 逻辑来算。

#### 1. 先明确线性回归的核心公式

- **预测公式**：对于单个样本（x_i, y_i），模型预测值 y_hat = w・x_i + b（w 是权重，b 是偏置，x_i 是样本输入，比如 “房屋面积”，y_i 是真实值，比如 “房屋价格”）；

- **损失函数**：用 “均方误差” 衡量预测值和真实值的差距，单个样本的损失 L = 1/2・(y_hat - y_i)²（乘 1/2 是为了后续求导更方便，不影响最终参数调整方向）。

#### 2. 计算 L 对 w 的偏导（∂L/∂w）

按照 “固定其他参数，只动 w” 的逻辑，步骤如下：

第一步：把 y_hat 代入损失函数，得到 L 关于 w 的表达式：

L = 1/2·(w·x_i + b - y_i)²

第二步：用 “链式法则” 求导（复合函数求导，就像剥洋葱，从外到内一层一层算）：

- 先把括号里的 “w・x_i + b - y_i” 看成一个整体，记为 A，那么 L = 1/2・A²；

- 对 L 求 A 的导数：dL/dA = 1/2 × 2A = A（根据幂函数求导法则：d (x^n)/dx = n・x^(n-1)）；

- 再对 A 求 w 的导数：dA/dw = x_i（因为 A 里只有 w・x_i 含 w，其他项 b、y_i 都是常数，导数为 0）；

- 根据链式法则，∂L/∂w = dL/dA × dA/dw = A × x_i = (w・x_i + b - y_i) × x_i。

#### 3. 计算 L 对 b 的偏导（∂L/∂b）

同样用 “固定其他参数，只动 b” 的逻辑：

第一步：还是 L = 1/2・(w・x_i + b - y_i)²，此时 w、x_i、y_i 都是常数；

第二步：链式法则求导：

- 还是把 A = w・x_i + b - y_i 看成整体，dL/dA = A；

- 对 A 求 b 的导数：dA/db = 1（因为 A 里只有 b 含 b，其他项都是常数，导数为 0）；

- 所以∂L/∂b = dL/dA × dA/db = A × 1 = w・x_i + b - y_i。

#### 4. 偏导的实际意义（关联 SGD 参数更新）

假设我们有一个具体样本：x_i=10（房屋面积 100㎡，简化为 10），y_i=50（真实价格 50 万），当前参数 w=4，b=5。

- 先算预测值 y_hat = 4×10 + 5 = 45；

- 损失 L = 1/2×(45-50)² = 1/2×25 = 12.5；

- 算偏导：∂L/∂w = (45-50)×10 = (-5)×10 = -5；∂L/∂b = 45-50 = -5。

偏导结果的含义：

- ∂L/∂w = -5：w 每增加 1，损失 L 会减少 5（因为偏导是负数），所以要减小损失，w 应该 “往大了调”；

- ∂L/∂b = -5：b 每增加 1，损失 L 会减少 5，所以 b 也应该 “往大了调”。

这就和 SGD 的参数更新公式对应上了：

新 w = 旧 w - 学习率 ×∂L/∂w = 4 - 0.01×(-5) = 4.05（学习率取 0.01）；

新 b = 旧 b - 学习率 ×∂L/∂b = 5 - 0.01×(-5) = 5.05；

更新后，y_hat = 4.05×10 + 5.05 = 45.55，损失 L = 1/2×(45.55-50)² ≈ 9.9，确实变小了！

### 四、案例 2：简单多层感知机中偏导的计算（含激活函数）

当模型有隐藏层和激活函数（比如 ReLU）时，偏导计算会多一步 “激活函数的导数”，但核心逻辑还是链式法则。我们以 “单隐藏层感知机” 为例：

#### 1. 明确模型公式

假设模型结构：输入 x_i → 隐藏层（权重 w1，偏置 b1，ReLU 激活）→ 输出层（权重 w2，偏置 b2）→ 预测值 y_hat。

- 隐藏层输出 h = ReLU (w1・x_i + b1)（ReLU 函数：h = max (0, w1・x_i + b1)）；

- 输出层预测 y_hat = w2・h + b2；

- 损失函数还是单样本均方误差 L = 1/2・(y_hat - y_i)²。

#### 2. 计算 L 对输出层参数 w2、b2 的偏导

这一步和线性回归类似，因为输出层没有激活函数（或视为线性激活）：

- ∂L/∂w2：先把 y_hat 代入 L，L = 1/2・(w2・h + b2 - y_i)²。固定 h、b2、y_i，对 w2 求导：

∂L/∂w2 = (w2·h + b2 - y_i) × h = (y_hat - y_i) × h；

- ∂L/∂b2 = (y_hat - y_i) × 1 = y_hat - y_i（和线性回归中∂L/∂b 逻辑一致）。

#### 3. 计算 L 对隐藏层参数 w1、b1 的偏导（关键：激活函数的导数）

这一步需要 “反向传播”，从输出层的偏导往回算，核心是多乘 “激活函数的导数”：

第一步：先算 “损失对隐藏层输出 h 的偏导”（记为∂L/∂h）：

从 y_hat = w2・h + b2 可知，∂y_hat/∂h = w2；而 L 对 y_hat 的偏导是∂L/∂y_hat = y_hat - y_i（从损失函数求导得出）。

根据链式法则，∂L/∂h = ∂L/∂y_hat × ∂y_hat/∂h = (y_hat - y_i) × w2。

第二步：计算∂L/∂w1（需考虑 ReLU 的导数）：

隐藏层 h = ReLU (z)，其中 z = w1・x_i + b1（z 是隐藏层的 “预激活值”）。

- 先算 ReLU 的导数：ReLU'(z) = 1 如果 z>0（此时 h=z，h 对 z 的导数是 1）；ReLU'(z) = 0 如果 z≤0（此时 h=0，h 对 z 的导数是 0）；

- 再算 z 对 w1 的导数：∂z/∂w1 = x_i（z 里只有 w1・x_i 含 w1，其他项是常数）；

- 链式法则串联：∂L/∂w1 = ∂L/∂h × ReLU'(z) × ∂z/∂w1 = [(y_hat - y_i) × w2] × ReLU'(z) × x_i。

第三步：计算∂L/∂b1：

逻辑和∂L/∂w1 类似，只是 z 对 b1 的导数是 1（z = w1・x_i + b1，b1 的导数是 1）：

∂L/∂b1 = ∂L/∂h × ReLU'(z) × ∂z/∂b1 = [(y_hat - y_i) × w2] × ReLU'(z) × 1。

#### 4. 简单例子验证

假设样本 x_i=2，y_i=10；当前参数 w1=3，b1=1，w2=2，b2=0。

- 隐藏层预激活 z = 3×2 + 1 = 7（z>0），所以 h = ReLU (7) = 7；

- 输出层 y_hat = 2×7 + 0 = 14；

- 损失 L = 1/2×(14-10)² = 8；

- 计算偏导：

∂L/∂w2 = (14-10)×7 = 4×7 = 28；

∂L/∂b2 = 14-10 = 4；

∂L/∂h = 4×2 = 8；

ReLU'(z) = 1（因为 z=7>0）；

∂L/∂w1 = 8 × 1 × 2 = 16；

∂L/∂b1 = 8 × 1 × 1 = 8。

偏导含义：w1 每增加 1，损失会增加 16，所以 w1 应该 “往小了调”（SGD 更新时用 “旧 w1 - 学习率 ×16”），这符合 “减少损失” 的目标。

### 五、不用手动算！框架如何帮我们求偏导？

实际训练模型时，我们不会手动算偏导（比如 100 层的网络，手动算几天也算不完），深度学习框架（如 PyTorch、TensorFlow）会用 “自动微分” 工具帮我们自动计算，核心原理还是 “计算图 + 链式法则”：

#### 1. 框架的工作逻辑

- 第一步：构建 “计算图”：把模型的预测过程（x→h→y_hat→L）拆成一个个 “计算节点”（比如乘法、加法、ReLU、平方），形成一张有向图；

- 第二步：前向传播：从输入 x 开始，按计算图顺序算出 y_hat 和 L，同时记录每个节点的 “中间结果”（比如 z、h 的值）；

- 第三步：反向传播：从损失 L 开始，按计算图的 “反向顺序”，用链式法则自动算出每个参数（w1、b1、w2、b2）的偏导，存到参数的 “grad” 属性里。

#### 2. 用 PyTorch 举个简单例子（对应线性回归）

```python
import torch
# 1. 定义样本和参数（参数需要加requires_grad=True，告诉框架要算偏导）
x_i = torch.tensor([10.0])  # 输入样本
y_i = torch.tensor([50.0])  # 真实值
w = torch.tensor([4.0], requires_grad=True)  # 权重，需要算偏导
b = torch.tensor([5.0], requires_grad=True)  # 偏置，需要算偏导
# 2. 前向传播：算y_hat和损失
Ly_hat = w * x_i + bL = 0.5 * (y_hat - y_i) ** 2
# 3. 反向传播：自动算偏导，结果存在w.grad和b.grad里
L.backward()
# 4. 查看偏导结果（和我们手动算的一致！）
print("∂L/∂w =", w.grad.item())  # 输出：
∂L/∂w = -5.0print("∂L/∂b =", b.grad.item())  # 输出：
∂L/∂b = -5.0
# 5. SGD参数更新（手动模拟，框架会自动做）
learning_rate = 0.01w.data = w.data - learning_rate * w.gradb.data = b.data - learning_rate * b.gradprint("更新后w =", w.item())  
# 输出：更新后
w = 4.05print("更新后b =", b.item())  # 输出：更新后b = 5.05
```

### 六、总结：求偏导的核心步骤和注意事项

#### 1. 核心步骤（无论模型多复杂）

1. **明确模型结构**：写出从输入到输出的预测公式（比如 y_hat = ...）；

2. **定义损失函数**：确定衡量误差的函数（比如均方误差、交叉熵）；

3. **用链式法则求导**：从损失函数开始，按 “输出层→隐藏层→输入层” 的顺序，固定其他参数，对每个参数单独求偏导，遇到激活函数就多乘其导数；

4. **框架自动计算**：实际应用中，用框架的自动微分（如 PyTorch 的.backward ()）代替手动计算，避免出错。

#### 2. 关键注意事项

- **激活函数的导数**：这是深层网络偏导计算的关键，比如 ReLU 的导数简单（0 或 1），而 sigmoid 的导数是 σ(x)(1-σ(x))（容易导致梯度消失）；

- **批量计算的偏导**：小批量 SGD 中，偏导是 “批量内所有样本偏导的平均值”（框架会自动处理，比如 PyTorch 的.backward () 会累加梯度，需要手动清零）；

- **不要纠结复杂公式**：只要理解 “链式法则” 和 “偏导的物理意义”，即使模型有 100 层，框架也能帮我们算对，重点是知道偏导如何指导参数更新。