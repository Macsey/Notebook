
在深度学习里，“读写文件” 不是简单存个文档，而是帮我们解决两个关键问题：**把训练好的模型 “存起来”，以后直接用**；**防止训练中途断电，之前的努力不白费**。下面用最通俗的语言，一步步拆明白怎么操作、为什么这么做。

## 先搞懂核心需求：为啥要 “读写文件”？

我们之前学了怎么处理数据、搭模型、训练模型，但有两个现实问题没解决：

1. **训练好的模型不能 “随身带”**：比如花 3 天训练出一个识别猫的模型，关了程序后，模型的 “记忆”（参数）就没了，下次想用还得重新训练 3 天 —— 这显然不合理。

2. **训练中途断电 = 白干**：如果训练要 10 天，第 9 天突然断电，之前算好的模型参数全丢了，只能从头再来 —— 这谁顶得住？

所以 “读写文件” 的核心就是：**把模型的 “记忆”（参数、张量）存到电脑硬盘里，需要时再读回来接着用**，就像我们写作文时 “保存文档”“打开文档” 一样。

## 第一部分：简单的开始 —— 存单个 / 多个 “数据块”（张量）

在深度学习里，“张量” 就是我们最常用的 “数据块”（比如一个向量、一个矩阵）。先学会存单个张量，再存多个，这是基础。

### 1. 存一个张量：像存一张照片一样简单

不管用 PyTorch、TensorFlow 还是其他框架，逻辑都一样：用 “保存函数” 把张量存成文件，用 “加载函数” 把文件读回内存。

举个 PyTorch 的例子（其他框架操作类似，只是函数名稍变）：

```python
# 1. 先创建一个简单的张量（比如[0,1,2,3]）

import torch

x = torch.arange(4) # 这是一个包含0、1、2、3的“数据块”

# 2. 把它存到硬盘，文件名叫“x-file”

torch.save(x, 'x-file') # 相当于“保存文档”

# 3. 下次要用时，从硬盘读回来

x2 = torch.load('x-file') # 相当于“打开文档”

print(x2) # 输出结果：tensor([0, 1, 2, 3])——和原来的一模一样！
```

**大白话解释**：就像你把手机里的照片存到电脑（save），下次想看再从电脑导回手机（load），照片内容完全没变。

### 2. 存多个张量：打包成 “文件夹” 一起存

有时候我们要存一组张量（比如模型的两个权重向量），不用单独存多个文件，直接打包成 “列表” 或 “字典” 一起存。

还是用 PyTorch 举例：

```python
# 1. 创建两个张量：x是[0,1,2,3]，y是[0,0,0,0]

x = torch.arange(4)

y = torch.zeros(4)

# 2. 把两个张量打包成“列表”，存成文件“x-files”

torch.save([x, y], 'x-files') # 相当于把两张照片放进一个文件夹保存

# 3. 读回来时，直接拿到整个列表

x2, y2 = torch.load('x-files') # 相当于打开文件夹，拿出两张照片

print(x2) # tensor([0, 1, 2, 3])

print(y2) # tensor([0., 0., 0., 0.])
```

如果想给每个张量 “起名字”（方便后续查找），可以用 “[[字典]]” 存：

```python
# 用字典存，key是名字，value是张量

mydict = {'第一个张量': x, '第二个张量': y}

torch.save(mydict, 'mydict') # 存成文件

# 读回来时，按名字取

mydict2 = torch.load('mydict')

print(mydict2['第一个张量']) # tensor([0, 1, 2, 3])
```

**大白话解释**：就像你把 “旅行照片”“工作文档” 分别打包成文件夹，或者给每个文件贴标签，下次要找 “旅行照片”，直接按标签拿，不用翻遍整个硬盘。

## 第二部分：关键操作 —— 存整个模型的 “记忆”（参数）

单个张量的读写很简单，但我们真正需要的是**存整个模型**（比如一个识别猫的多层感知机）。这里要注意一个重要区别：

**我们存的是模型的 “参数”（即模型训练好的 “记忆”），不是模型的 “架构”（即模型的 “骨架”）**。

为什么？因为模型的 “架构” 是用代码写的（比如 “输入层→隐藏层→输出层”），没法直接存成文件；但架构里的 “参数”（比如隐藏层的权重矩阵）是张量，可以存起来。

### 1. 步骤 1：先搭一个模型（确定 “骨架”）

我们先搭一个简单的 “多层感知机”（MLP），相当于先做好一个 “空的大脑骨架”：

```
import torchfrom torch import nnfrom torch.nn import functional as F# 定义模型架构（骨架）：输入20个特征→隐藏层256个节点→输出10个类别class MLP(nn.Module):    def __init__(self):        super().__init__()        self.hidden = nn.Linear(20, 256)  # 输入→隐藏层        self.output = nn.Linear(256, 10)  # 隐藏层→输出层    # 定义计算流程（怎么用骨架处理数据）    def forward(self, x):        return self.output(F.relu(self.hidden(x)))# 实例化模型（创建一个“空大脑”）net = MLP()# 给模型喂点数据，让它“激活”一下（相当于让空大脑先“思考”一次）X = torch.randn(size=(2, 20))  # 2个样本，每个样本20个特征Y = net(X)  # 模型的输出（此时模型参数是随机的，还没训练）
```

### 2. 步骤 2：存模型的 “参数”（保存 “记忆”）

假设我们已经训练好了这个模型（参数不再随机），现在要把参数存起来。用框架的 “保存参数” 函数，不用管架构，只存参数：

```
# 把模型的参数存成文件“mlp.params”（后缀名随便起，习惯用.params）torch.save(net.state_dict(), 'mlp.params')
```

**大白话解释**：net.state_dict() 就是 “把模型的所有参数打包成字典”（比如 “hidden 层的权重”“output 层的偏置”），然后把这个字典存成文件。就像你把大脑里的 “知识”（参数）抄下来，存到笔记本里。

### 3. 步骤 3：读参数（恢复 “记忆”）

下次要用模型时，我们需要先 “搭好原来的骨架”（再定义一次 MLP 类），然后把之前存的 “参数”（笔记本里的知识）灌进去，这样模型就和之前训练好的一模一样了：

```
# 1. 先搭好和原来一样的骨架（必须和之前的架构完全相同！）clone = MLP()# 2. 把存好的参数读进骨架里clone.load_state_dict(torch.load('mlp.params'))# 3. 验证一下：给同一个输入，输出应该和原来一样Y_clone = clone(X)print(Y_clone == Y)  # 输出全是True，说明模型完全恢复了！
```

**关键注意点**：

- 必须先搭好和原来 “一模一样的架构”（比如原来隐藏层是 256 个节点，现在不能改成 128 个），否则参数 “装不进去”（就像给苹果手机的电池，装不进安卓手机）。

- 读参数后，要调用 clone.eval()（PyTorch 里），告诉模型 “现在是用来预测的，不是训练的”，避免模型参数意外变化。

## 第三部分：总结 + 常见问题解答

### 核心知识点（一句话记住）

1. 存单个 / 多个张量：用save存、load读，像存照片一样简单；

2. 存模型：只存 “参数”（用state_dict()），下次用要先 “搭架构” 再 “读参数”；

3. 架构不能存，必须用代码重新写（所以要把架构代码保存好，别弄丢了）。

### 常见问题（帮你解决实际困惑）

#### 1. 即使不换设备，存模型还有啥用？

- 防止中途断电：训练 10 天，第 9 天断电，存了参数就能从第 9 天继续，不用从头来；

- 方便调试：训练过程中，每天存一次参数，如果第 10 天的模型效果不好，可以倒回第 8 天的参数接着调；

- 分享给别人：把参数文件发给同事，他用同样的架构就能直接用你的模型，不用再训练。

#### 2. 想复用模型的一部分（比如只要前两层），怎么弄？

比如原来的模型是 “输入→隐藏层 1→隐藏层 2→输出”，现在想把 “输入→隐藏层 1→隐藏层 2” 拿出来用：

1. 先按原来的架构，定义一个 “只包含前两层” 的新模型（比如叫MLP_Part）；

2. 加载原来的完整参数文件，然后把 “隐藏层 1”“隐藏层 2” 的参数，对应装进新模型里。

举个例子：

```
# 1. 定义只包含前两层的新架构class MLP_Part(nn.Module):    def __init__(self):        super().__init__()        self.hidden1 = nn.Linear(20, 256)  # 和原来的hidden层一样        self.hidden2 = nn.Linear(256, 128)  # 和原来的第二层一样    def forward(self, x):        return self.hidden2(F.relu(self.hidden1(x)))# 2. 加载原来完整模型的参数full_params = torch.load('mlp_full.params')  # 原来的完整参数# 3. 把需要的参数挑出来，装进新模型part_net = MLP_Part()part_params = {    'hidden1.weight': full_params['hidden.weight'],  # 对应拿hidden1的权重    'hidden1.bias': full_params['hidden.bias'],      # 对应拿hidden1的偏置    'hidden2.weight': full_params['hidden2.weight'], # 对应拿hidden2的权重    'hidden2.bias': full_params['hidden2.bias']      # 对应拿hidden2的偏置}part_net.load_state_dict(part_params)  # 这样新模型就有了原来的前两层参数
```

#### 3. 能不能同时存架构和参数？有啥限制？

理论上可以，但有风险，因为**模型架构包含代码，直接存代码可能有安全问题（比如恶意代码），也容易因为框架版本变化导致读不出来**。

如果非要存架构，常见的方法是：

- 用框架的 “完整模型保存” 函数（比如 PyTorch 的torch.save(net, 'full_model.pth')），但这样会把整个模型对象（包括代码）存起来；

- 限制：下次读的时候，必须用和保存时 “完全相同的框架版本”“相同的 Python 环境”，否则可能报错（比如 PyTorch 1.8 存的模型，用 2.0 读可能不兼容）。

**更推荐的做法**：永远分开存 “架构代码” 和 “参数文件”—— 把架构代码写成一个.py 文件（比如[model.py](http://model.py)），参数存成mlp.params，下次用的时候先导入[model.py](http://model.py)里的架构，再加载参数，这样兼容性最好。

## 最后：一句话总结

深度学习的 “读写文件”，本质是 “给模型做备份”：存张量是备份小数据，存参数是备份模型的 “记忆”，只要架构代码不丢，随时能把模型恢复到之前的状态 —— 再也不怕断电，也不用重复训练！