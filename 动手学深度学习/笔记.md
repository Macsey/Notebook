
## 13.1 图像增广 (Image Augmentation)

**核心逻辑**：在训练集不足的情况下，通过“人为造数据”来提升模型的泛化能力。

- **作用**：
    
    - **扩充规模**：缓解小数据集下的过拟合。
        
    - **降低依赖**：减少模型对位置、颜色、光照的特定依赖。
        

|**类别**|**具体方法**|**作用原理**|
|---|---|---|
|**几何变换**|随机翻转（左右/上下）、裁剪|不改变类别，模拟物体在不同方位的情况，降低位置依赖。|
|**光色变换**|亮度、对比度、饱和度、色调抖动|模拟不同光照环境，降低模型对颜色的敏感度。|
|**组合使用**|`Compose` (如 翻转+裁剪+变色)|实战标配，最大限度增加样本多样性。|


## 13.2 微调 

场景：手中数据少（如几千张），但任务复杂。

核心思想：站在巨人的肩膀上——利用在大规模数据集（如 ImageNet）上训练好的模型参数进行迁移学习。

### 微调四步法

1. **源模型 (Source Model)**：下载在大数据集上预训练好的模型（如 ResNet）。
    
2. **构建目标模型**：复制源模型的所有层（Body），但**剔除并重置输出层 (Head)**。
    
3. **初始化输出层**：根据你的目标类别数（如 10 类），随机初始化新的输出层参数。
    
4. **差异化训练**：
    
    - **特征层 (Body)**：使用**小学习率**微调（因为它已经学到了边缘、纹理等通用特征）。
        
    - **输出层 (Head)**：使用**大学习率**从头训练（快速适配新任务）。
        

> **关键操作**：微调时，输入图像的标准化（均值、方差）必须与预训练模型保持一致。


## 13.3 目标检测基础：边界框 
**任务升级**：从“图片里有什么”（分类）升级为“什么在哪里”（分类+定位）。

### 坐标体系

- **原点**：图像左上角 $(0,0)$。
    
- **方向**：向右为 $x$ 轴正向，**向下为 $y$ 轴正向**。
    

### 两种表示法

|**方法**|**格式**|**说明**|
|---|---|---|
|**边角式**|$(x_1, y_1, x_2, y_2)$|左上角坐标 + 右下角坐标|
|**中心式**|$(c_x, c_y, w, h)$|中心点坐标 + 宽 + 高|


## 13.4 锚框 

核心痛点：目标可能出现在任何位置、任何大小。如何让机器去“猜”？

解决方案：先在图上撒下一大堆形状各异的框（候选区域），然后让模型去判断这些框里有没有物体，以及框的位置准不准。

### 1. 生成逻辑

对于图像上的每个像素，都以其为中心生成多个锚框。

- **参数**：缩放比 (Scale, $s$) 和 宽高比 (Ratio, $r$)。
    
- **数量优化**：组合数为 $n+m-1$，而非 $n \times m$，避免锚框爆炸。
    

### 2. 评估工具：交并比 (IoU)

衡量两个框重叠程度的指标（Jaccard系数）。

$$IoU = \frac{\text{Area}(A \cap B)}{\text{Area}(A \cup B)}$$

- 范围 $[0, 1]$，0 为不重叠，1 为完全重合。
    

### 3. 训练标注 

训练时需要告诉模型：这个锚框是“背景”还是“物体”？如果是物体，它离真实框偏了多少？

- **类别标签**：与真实框 IoU 最大的锚框赋为对应类别，IoU 很小的赋为背景 (0)。
    
- **偏移量标签 (Offsets)**：计算锚框中心与宽高的修正值。为了数据分布稳定，通常会对偏移量做**标准化**。
    

### 4. 预测去重：非极大值抑制 

预测时模型会输出很多重叠的框，NMS 用于只保留最好的那一个。

- **流程**：
    
    1. 按置信度（得分）排序。
        
    2. 选出得分最高的框 $A$，保留它。
        
    3. 将所有与 $A$ 的 $IoU > \text{阈值}$ 的框剔除（认为是重复预测）。
        
    4. 重复上述步骤，直到处理完所有框。
        

---

## 13.5 多尺度目标检测

**核心思想**：利用 CNN 的金字塔特征结构。

- **小特征图 (深层)**：感受野大 $\rightarrow$ 负责检测**大目标**。
    
- **大特征图 (浅层)**：感受野小，细节多 $\rightarrow$ 负责检测**小目标**。
    

---

## 13.7 单发多框检测 (SSD)

定位：单阶段检测器 (One-stage)，追求速度与精度的平衡。

架构特点：

1. **多尺度特征块**：在不同分辨率的特征图上分别生成锚框并预测。
    
2. **直接预测**：没有生成候选区域（Region Proposal）的过程，直接输出类别和回归偏移量。
    
3. **锚框设计**：越底层的特征图，生成的锚框越小；越顶层的特征图，锚框越大。
    

---

## 13.8 R-CNN 系列 (两阶段检测)

**演进逻辑**：从“慢但准”一步步优化到“又快又准”。

|**模型**|**核心改进**|**优点**|**缺点**|
|---|---|---|---|
|**R-CNN**|CNN提取特征 + SVM分类|引入深度学习，精度大增|2000个区域分别过CNN，**极慢**|
|**Fast R-CNN**|**RoI Pooling** + 特征共享|整图只过一次CNN，解决了重复计算|仍依赖选择性搜索生成区域|
|**Faster R-CNN**|**RPN (区域提议网络)**|彻底端到端，用网络生成候选区域|相比 SSD 速度稍慢|
|**Mask R-CNN**|**RoI Align** + FCN分支|像素级对齐，增加分割掩码分支|精度最高，支持实例分割|

> **RoI Pooling vs RoI Align**:
> 
> - **Pooling**: 坐标取整（量化），导致像素错位，不适合分割。
>     
> - **Align**: 双线性插值，保留浮点坐标，**像素级精准**。
>     

---

## 13.9 - 13.11 语义分割与 FCN

**任务**：像素级分类（Pixel-level Classification），判断每个像素属于草地、猫还是天空。

### 核心技术：转置卷积 (Transposed Convolution)

- **别名**：反卷积 (Deconvolution) —— _注：数学上并不严谨，建议叫转置卷积。_
    
- **作用**：**上采样 (Upsampling)**。将缩小的特征图放大回原图尺寸。
    
- **性质**：形状上是卷积的逆操作（$H \times W \rightarrow \text{Small}$ 变为 $\text{Small} \rightarrow H \times W$），但数值上不还原。
    

### 全卷积网络 (FCN)

- **结构**：去除全连接层，全部替换为卷积层。
    
- **流程**：
    
    1. **骨干网络**：提取特征，图像缩小（如 1/32）。
        
    2. **$1\times1$ 卷积**：将通道数变为类别数。
        
    3. **转置卷积**：将尺寸放大 32 倍，恢复原图大小。
        
- **输出**：一张形状为 $(H, W, \text{Classes})$ 的概率图。
    

---

## 13.12 风格迁移 (Style Transfer)

定义：内容图 (Content) + 风格图 (Style) = 合成图 (Generated)。

特殊性：不更新网络参数，而是更新输入图像（合成图）的像素。

### 损失函数设计

$$Loss_{total} = \alpha Loss_{content} + \beta Loss_{style} + \gamma Loss_{TV}$$

1. **内容损失**：让合成图与内容图在 CNN 深层特征尽可能一致（保留骨架）。
    
2. **风格损失**：让合成图与风格图的**格拉姆矩阵 (Gram Matrix)** 尽可能一致（复制纹理、笔触）。
    
    - _Gram 矩阵：衡量不同通道特征之间的相关性（即风格）。_
        
3. **全变分损失 (TV Loss)**：降噪，让生成的图像更平滑自然。
    

---
