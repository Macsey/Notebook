[[注意力提醒]]
[[注意力汇聚]]
[[注意力评分函数]]
[[Bahdanau]]
[[多头注意力]]
[[自注意力和位置编码]]
[[Transformer]]











































## 注意力的底层逻辑

### 1. 生物学基础
这就是注意力机制的生物学基础：**在信息过载的环境中，将稀缺的计算资源分配给最有价值的信息。**

### 2. 两种提示：
我们将这种选择性关注分为两类：

* **非自主性提示 ：**
    * *关键词*：**被动、环境驱动**。
    * *场景*：你正在看书，突然有人打碎了一个杯子，你的视线会瞬间被吸引过去。这是由环境的“突出性”（Salience）驱动的本能反应，无需意识参与。
* **自主性提示 ：**
    * *关键词*：**主动、任务驱动**。
    * *场景*：你在人群中寻找穿红衣服的朋友。你的大脑带着“红色”这个意图去扫描环境。这是由主观意志驱动的聚焦。

**核心**：深度学习中的注意力机制，本质上就是引入了**自主性提示**。传统的全连接层或池化层是“一视同仁”的，而注意力机制允许模型根据当前的“任务需求”（Query），去动态地加权处理“输入信息”。


## 注意力机制的核心框架

注意力的“三位一体”：

1.  **查询 (Query, $Q$)**：**自主性提示**。代表“我想要寻找什么”，即当前的任务目标。
2.  **键 (Key, $K$)**：**索引标签**。代表输入数据的特征标识，用于和 Query 进行匹配。
3.  **值 (Value, $V$)**：**实际内容**。代表输入数据本身，是我们最终要提取的信息。

### 核心运作流程
注意力汇聚（Attention Pooling）的本质是一个**加权平均**的过程：

$$
\text{Attention}(Q, K, V) = \sum \text{Similarity}(Q, K) \times V
$$

1.  **匹配**：拿 $Q$ 去和所有的 $K$ 算相似度（评分）。
2.  **归一化**：将相似度转化为概率分布（权重）。
3.  **汇聚**：根据权重对 $V$ 进行加权求和。

---

##  从核回归到深度学习

### 1. NW)核回归
这是注意力机制的“史前形态”，它展示了如何从“简单平均”进化到“加权平均”。

| 方法 | 核心逻辑 | 特点 | 效果 |
| :--- | :--- | :--- | :--- |
| **平均汇聚** | $f(x) = \frac{1}{n}\sum y_i$ | 无视输入特征，众生平等 | 也就是一条水平线，完全无法拟合数据 |
| **非参数 NW 回归** | 利用**高斯核**计算新输入($x$)与训练数据($x_i$)的距离，距离越近，权重越大 | 数据驱动，无学习参数 | 曲线平滑，能拟合真实函数，但在数据稀疏处效果差 |
| **带参数 NW 回归** | 在高斯核中引入可学习参数 $w$，即 $K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2w^2})$ | **模型可自主学习关注的“宽窄”** | 拟合能力更强，能适应复杂的噪声分布 |

**可视化结论：**
* **非参数模型**：注意力权重图通常表现为一条斜向的平滑色带，表示“临近的数据点相关性最高”。
* **带参数模型**：权重热图会变得更窄、更锐利，说明模型学会了更精准地“聚焦”，忽略无关的邻居。

### 2. 现代注意力评分函数
在深度学习中，我们需要处理向量，计算 $Q$ 和 $K$ 的相似度有多种方式。

#### A. 掩蔽 Softmax
* **痛点**：处理变长序列时，Batch 中会有填充（Padding）的无效数据（通常填 0）。如果不处理，Softmax 会给它们分配非零权重，引入噪声。
* **解法**：在 Softmax 之前，将无效位置的评分设为极小的负数（如 $-10^9$）。
* **效果**：$e^{-10^9} \approx 0$，彻底屏蔽无效信息。

#### B. 加性注意力 
* **场景**：当 $Q$ 和 $K$ 的特征向量**长度不同**时使用。
* **原理**：既然维度不同无法直接点积，那就各自过一个线性变换层（$W_q, W_k$），映射到统一的隐藏维度 $h$，相加并通过激活函数 $\tanh$，最后再映射为一个标量评分。
* **公式**：
$$
a(q, k) = w_v^T \tanh(W_q q + W_k k)
$$

#### C. 缩放点积注意力 
* **场景**：$Q$ 和 $K$ **长度相同**（均为 $d$），追求极致的计算效率。
* **原理**：直接计算向量内积。内积越大，方向越一致，相关性越高。
* **关键细节（缩放）**：为什么要除以 $\sqrt{d}$？
    * 当向量维度 $d$ 很大时，点积结果方差会很大，导致 Softmax 的梯度极小（梯度消失），模型难以训练。除以 $\sqrt{d}$ 可以稳定数值范围。
* **公式**：
$$
\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d}})V
$$


## Bahdanau 注意力


### 1. 传统 Seq2Seq 的缺点
传统模型采用“编码-解码”架构，编码器被迫把整句长文压缩成**一个固定长度**的上下文向量（Context Vector）。
* **问题**：这就像要求翻译官读完一整本《红楼梦》，然后仅凭脑子里的一个“模糊印象”开始翻译。细节（Value）极易丢失，且无法处理长句。

### 2. Bahdanau 的创新：动态上下文
Bahdanau 提出：**翻译不应该盯着整句看，而应该“译到哪里，看到哪里”。**

* **Q (Query)**：解码器在上一步的隐状态（比如刚翻译完“I”，状态变为“准备翻译 am”）。
* **K/V (Key/Value)**：编码器对原文每个词的隐状态。
* **过程**：
    1.  拿“准备翻译 am”的状态去和原文所有词算相关度。
    2.  发现和原文的“是”相关度最高。
    3.  加权生成一个**当前步专属**的上下文向量，辅助翻译。



## 多头与自注意力

### 1. 多头注意力 
如果说普通注意力是“独眼龙”，多头注意力就是“千手观音”。

* **设计哲学**：文本的依赖关系是复杂的。同一个词，既有语法依赖，又有语义依赖，还有指代关系。单一的注意力头无法同时捕捉这么多维度的特征。
* **实现**：
    1.  将 $Q, K, V$ 投影到 $h$ 个不同的低维子空间。
    2.  每个“头”独立计算注意力，互不干扰。
    3.  **拼接 (Concat)** 所有头的输出，再经过一次线性变换融合信息。
* **优势**：极大地丰富了模型的表达能力，能够并行捕捉局部和全局、语法和语义的多种模式。

### 2. 自注意力
* **定义**：$Q, K, V$ 全部来自**同一个**输入序列。即“自己看自己”。
* **威力**：
    * **RNN**：只能按顺序看，距离越远，联系越弱。
    * **CNN**：只能看局部窗口，感受野受限。
    * **自注意力**：**上帝视角**。序列中任意两个词（无论距离多远）的距离都是 1（一步直达）。这使得它捕捉长距离依赖的能力是 $O(1)$ 的。

### 3. 位置编码 
* **缺陷**：自注意力机制是全并行的，它天然**没有顺序概念**。打乱输入句子的词序，自注意力的输出结果是一样的（排列不变性）。
* **补丁**：必须手动将位置信息“注入”到输入中。
* **方法**：使用不同频率的正弦/余弦函数生成位置向量，直接**加**到词嵌入向量上。这使得模型既能感知绝对位置，也能通过线性变换感知相对位置。

## Transformer 架构

Transformer 彻底抛弃了 RNN 和 CNN，打造了一个纯注意力的各种组件堆叠而成的摩天大楼。

### 1. 整体架构
* **编码器 (Encoder)**：负责理解输入，提取丰富的语义特征。由 $N$ 个 EncoderBlock 堆叠。
* **解码器 (Decoder)**：负责生成输出。由 $N$ 个 DecoderBlock 堆叠。

### 2. 核心组件

#### A. 编码器块
每个块包含两个子层：
1.  **多头自注意力**：让词与词之间建立联系，理解语境。
2.  **位置前馈网络 (Position-wise FFN)**：一个两层的 MLP。注意，它是**对每个位置独立进行**的，作用是增加非线性变换能力，提取更深层特征。
* **关键机制**：每个子层后都有 **残差连接 (Residual Connection)** 和 **层归一化 (LayerNorm)**，这是训练深层网络的“稳定器”。

#### B. 解码器块
比编码器多了一个层，且有特殊限制：
1.  **掩蔽多头自注意力：
    * *极为重要*：在预测第 $t$ 个词时，解码器**绝不能**看到 $t$ 之后的词（不能作弊）。
    * *实现*：通过掩码将未来位置的注意力权重强制置为 0。
2.  **编码器-解码器注意力：
    * *桥梁*：$Q$ 来自解码器（我当前想说什么），$K, V$ 来自编码器输出（原文说了什么）。这是模型“看原文”的时刻。
3.  **位置前馈网络**：同上。

### 3. 为什么是 LayerNorm 而不是 BatchNorm？
* **BatchNorm**：沿 Batch 维度归一化。但在 NLP 中，句子长度参差不齐，Padding 很多，Batch 统计量不稳定。
* **LayerNorm**：沿 Feature 维度归一化。对每句话、每个词独立计算，不受 Batch 大小和序列长度影响，更适合 NLP 任务。
# 总计：
1.  **并行计算**：抛弃 RNN 的串行，利用 GPU 并行加速。
2.  **长距依赖**：利用自注意力，无论多远的信息都能瞬间关联。
3.  **深度堆叠**：利用残差和归一化，使得网络可以堆叠得非常深（如 GPT-4 的上百层）。
