![[Pasted image 20251209202722.png]]
描述了一个具该图有$L$个隐藏层的深度循环神经网络， 每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步
## 为什么多层
目的：**提升模型对序列数据复杂模式的建模能力**，深度循环神经网络的层级结构，能让不同层专注于不同粒度的特征，
假设在时间步 $t$ 有一个小批量的输入数据 $\boldsymbol{X}_t$（样本数：$n$，每个样本中的输入数：$d$）。
我们将 $L$ 个隐藏层（隐藏层索引 $l = 1, 2, \dots, L$）
隐状态设为 $\boldsymbol{H}_t^{(l)}$（隐藏单元数：$h_l$），输出层变量设为 $\boldsymbol{O}_t$（输出数：$q$）。
初始化设定：
设置 $\boldsymbol{H}_t^{(0)} = \boldsymbol{X}_t$。
## 函数依赖关系
### 1. 隐藏层计算 

假设在时间步 $t$ 有一个小批量的输入数据 $\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$（样本数：$n$，每个样本中的输入数：$d$）。同时，将第 $l$ 隐藏层（$l=1, \dots, L$）的状态设为 $\boldsymbol{H}_t^{(l)} \in \mathbb{R}^{n \times h}$（隐藏单元数：$h$），输出层变量设为 $\boldsymbol{O}_t \in \mathbb{R}^{n \times q}$（输出数：$q$）。

设置 $\boldsymbol{H}_t^{(0)} = \boldsymbol{X}_t$。第 $l$ 个隐藏层的状态使用激活函数 $\phi_l$，则其计算公式为：

$$\boldsymbol{H}_t^{(l)} = \phi_l(\boldsymbol{H}_t^{(l-1)} \boldsymbol{W}_{xh}^{(l)} + \boldsymbol{H}_{t-1}^{(l)} \boldsymbol{W}_{hh}^{(l)} + \boldsymbol{b}_h^{(l)}) \tag{9.3.1}$$
**参数说明：**

$\boldsymbol{W}_{xh}^{(l)} \in \mathbb{R}^{h \times h}$：连接上一层即第 $l-1$ 层隐状态的权重参数。
$\boldsymbol{W}_{hh}^{(l)} \in \mathbb{R}^{h \times h}$：连接当前层上一时刻即 $t-1$ 时刻隐状态的权重参数。   
$\boldsymbol{b}_h^{(l)} \in \mathbb{R}^{1 \times h}$：第 $l$ 个隐藏层的偏置参数。
**注意：** 权重 $\boldsymbol{W}_{xh}^{(l)}$, $\boldsymbol{W}_{hh}^{(l)}$ 和偏置 $\boldsymbol{b}_h^{(l)}$ 都是第 $l$ 个隐藏层的模型参数。

### 2. 输出层计算

最后，输出层的计算仅基于第 $L$ 个隐藏层最终的隐状态：

$$\boldsymbol{O}_t = \boldsymbol{H}_t^{(L)} \boldsymbol{W}_{hq} + \boldsymbol{b}_q \tag{9.3.2}$$

**参数说明：**

- $\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}$：输出层的权重参数。
    
- $\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}$：输出层的偏置参数。
    

**注意：** 权重 $\boldsymbol{W}_{hq}$ 和偏置 $\boldsymbol{b}_q$ 都是输出层的模型参数。

### 3. 超参数和拓展

与多层感知机一样，**隐藏层数目 $L$** 和 **隐藏单元数目 $h$** 都是**超参数**，可以调整。
