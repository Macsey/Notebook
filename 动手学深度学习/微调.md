## 1. 核心概念与背景

**为什么要微调？** 在深度学习中，训练一个强大的模型（如 ResNet）需要海量数据（如 ImageNet 的 120 万张图）和昂贵的算力（几周的 GPU 时间）。
- **现实困境**：我们手头的任务通常只有几千张图（比如识别“是不是热狗”），数据量太小，直接训练深层网络会导致严重的**过拟合**。
- **解决方案**：利用别人在大数据集上训练好的模型参数，迁移到我们的任务上。

### 核心直觉
神经网络的层级特征具有**通用性**：

- **底层（靠近输入）**：提取边缘、纹理、颜色。这些特征在识别猫、狗、热狗时是**通用**的。
- **高层（靠近输出）**：提取语义形状（如眼睛、轮子）。这些特征与具体任务相关。
- **策略**：我们将预训练模型（Source Model）的底层参数“借”过来，只重新训练高层参数来适应新任务（Target Dataset）。
    

---

## 2. 微调的标准步骤 (四步走)

书中以 **ResNet18** 为例，任务是将 ImageNet (1000类) 迁移到 Hotdog Dataset (2类)。

1. **加载预训练模型**：
    
    - 下载一个在 ImageNet 上训练好的 ResNet18。
        
    - 读取其所有层的权重（Model Parameters）。
        
2. **修改输出层 (Head Replacement)**：
    
    - 原模型的输出层是 `nn.Linear(512, 1000)`。
        
    - 我们的任务只有 2 类（是热狗/不是热狗）。
        
    - **操作**：把最后一层去掉，换成一个新的 `nn.Linear(512, 2)`。
        
3. **参数初始化**：
    
    - **特征提取层 (Backbone)**：使用预训练好的参数（Pre-trained Weights）。
        
    - **新输出层 (New Head)**：使用随机初始化（Random Initialization）。
        
4. **差异化训练 (关键点)**：
    
    - **特征提取层**：使用**较小的学习率**（Small LR）。因为这些参数已经很好了，我们只想微调一下，不想破坏它。
        
    - **新输出层**：使用**较大的学习率**（Large LR）。因为它是随机初始化的，需要快速学习。
        

---

## 3. 代码实战 (PyTorch)

这是 d2l 书中代码的精炼版，重点展示了**如何只替换最后一层**以及**如何设置不同的学习率**。

Python

```
import torch
import torchvision
from torch import nn

# 1. 获取预训练模型
# pretrained=True 会自动下载 ImageNet 的权重
pretrained_net = torchvision.models.resnet18(pretrained=True)

# 2. 修改输出层
# ResNet 的最后一层名叫 'fc' (Fully Connected)
# 输入维度 pretrained_net.fc.in_features 必须保持不变 (512)
# 输出维度改为 2 (Hotdog vs Not Hotdog)
pretrained_net.fc = nn.Linear(pretrained_net.fc.in_features, 2)

# 初始化这层新的参数 (Xavier 初始化)
nn.init.xavier_uniform_(pretrained_net.fc.weight)

# 3. 定义优化器 (差异化学习率的核心)
# 目标：输出层的学习率是特征层的 10 倍
learning_rate = 5e-5 # 基础学习率

# 把参数分成两组：
# 组A: 最后一层的参数 (fc.weight, fc.bias)
# 组B: 除了最后一层之外的所有参数
output_params = list(map(id, pretrained_net.fc.parameters()))
feature_params = filter(lambda p: id(p) not in output_params, pretrained_net.parameters())

optimizer = torch.optim.SGD([
    {'params': feature_params, 'lr': learning_rate},       # 其它层：LR = 1x
    {'params': pretrained_net.fc.parameters(), 'lr': learning_rate * 10} # 最后一层：LR = 10x
], weight_decay=0.001)

# 4. 开始训练 (后续就是标准的 train loop)
# ...
```

---

## 4. 常见问题与技巧 (FAQ)

### Q1: 固定参数 (Freezing) vs 微调 (Fine-tuning)？

- **微调 (Fine-tuning)**：如上文所述，所有层的参数都会更新，只是学习率不同。d2l 推荐这种方式，因为它可以让特征层也稍微适应一下新数据。
    
- **固定参数 (Freezing)**：将特征提取层的 `requires_grad` 设为 `False`，只训练最后一层。
    
    - _适用场景_：你的数据量极少，或者算力极差时。
        

### Q2: 为什么新层的学习率要设大一点？

因为特征层（Backbone）已经是“老司机”了，稍微熟悉一下路况（新数据）就行；而新层（Head）是“新手”，脑子一片空白（随机初始化），必须猛学。

### Q3: 预训练模型去哪里找？

- `torchvision.models` (CV 常用：ResNet, VGG, MobileNet)
    
- `HuggingFace Transformers` (NLP 常用：BERT, GPT)
    
- `timm` (PyTorch Image Models，更全的视觉模型库)
    

---

## 5. 章节总结 (One-Liner)

**微调就是把预训练模型当作一个极其强大的“特征提取器”，我们只需要重塑它的“输出头”，并用较小的步伐（学习率）去调整它的身体，用较大的步伐去训练它的新头脑。**