**目的**：旨在解决随机梯度下降（SGD）在嘈杂梯度下学习率选择困难的问题，
**作用**：通过引入 “动量”（过去梯度的加权平均），能加速收敛、减少振荡，尤其适用于条件不佳的优化问题（如狭窄峡谷型目标函数）。

该算法可与随机梯度下降、小批量随机梯度下降结合使用，需额外维护速度（动量）这一辅助变量。

## 泄漏平均值
- **定义**：用过去梯度的加权平均（泄漏平均值）替代瞬时梯度，公式为：
$$\mathbf{v}_t = \beta \mathbf{v}_{t-1} + \mathbf{g}_{t, t-1}$$
其中，vt​ 为 t 时刻的动量，β∈(0,1) 为动量参数（控制过去梯度的权重），gt,t−1​ 为  
t−1 到 t刻的梯度。

递归拓展:$$$\begin{aligned}
\mathbf{v}_t = \beta^2 \mathbf{v}_{t-2} + \beta \mathbf{g}_{t-1, t-2} + \mathbf{g}_{t, t-1}
= \ldots, = \sum_{\tau = 0}^{t-1} \beta^{\tau} \mathbf{g}_{t-\tau, t-\tau-1}.
\end{aligned}$$
即动量是过去所有梯度的指数加权平均，β 越大，对早期梯度的记忆越强。
### 动量法的适用场景
**问题示例**：以目标函数 $f(\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.$为例，该函数在 x1​ 方向极平坦（梯度小）、x2​ 方向陡峭（梯度大），常规梯度下降面临两难：
- 学习率过小：x1​ 方向收敛极慢；
- 学习率过大：x2​ 方向振荡发散
动量法改进：通过聚合梯度，在 x1​ 方向累积对齐梯度以加速收敛，在 x2​ 方向抵消振荡以稳定步长，即使 η=0.6 也能有效收敛
### 动量法更新规则
$$\begin{split}\begin{aligned}
\mathbf{v}_t &\leftarrow \beta \mathbf{v}_{t-1} + \mathbf{g}_{t, t-1}, \\
\mathbf{x}_t &\leftarrow \mathbf{x}_{t-1} - \eta_t \mathbf{v}_t.
\end{aligned}\end{split}$$
1. 动量更新：累积过去梯度；
2. 参数更新：xt​←xt−1​−ηt​vt​（沿动量方向更新参数）。
	注：β=0 时退化为常规梯度下降，初始时刻 v0​=0。
- **参数影响**：
- β 越小（如 0.25），对过去梯度记忆弱，收敛效果差；
- β 适中（如 0.5,0.9），能平衡收敛速度与稳定性。