## 核心思想
让模型在相同查询（q）、键（k）、值（v）集合下，学习不同行为并组合知识，从而捕获序列内不同范围的依赖关系
每一个注意力汇聚都被称作一个头
![](assets/多头注意力/file-20251213174952429.png)
**流程**：独立学习得到的h组不同的 线性投影来变换查询、键和值--->送到注意力汇聚--->将这h个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出


## 单个头
# 多头注意力的数学形式化描述

## 1. 单个注意力头的计算

给定查询 $\(\mathbf{q} \in \mathbb{R}^{d_q}\)$、键 $\(\mathbf{k} \in \mathbb{R}^{d_k}\)、值 \(\mathbf{v} \in \mathbb{R}^{d_v}\)，第 \(i\) 个注意力头（共 \(h\) 个）的计算方式为：

\(\mathbf{h}_i = f(\mathbf{W}_i^{(q)} \mathbf{q}, \mathbf{W}_i^{(k)} \mathbf{k}, \mathbf{W}_i^{(v)} \mathbf{v}) \in \mathbb{R}^{p_v}\)

其中：

- \(\mathbf{W}_i^{(q)} \in \mathbb{R}^{p_q \times d_q}\)、\(\mathbf{W}_i^{(k)} \in \mathbb{R}^{p_k \times d_k}\)、\(\mathbf{W}_i^{(v)} \in \mathbb{R}^{p_v \times d_v}\) 是可学习参数

- \(f\) 是注意力汇聚函数（如加性注意力、缩放点积注意力）

## 2. 多头注意力的最终输出

将 \(h\) 个注意力头的结果拼接后，通过一个线性变换得到最终输出，对应的可学习参数为 \(\mathbf{W}_o \in \mathbb{R}^{p_o \times h p_v}\)：

\(\mathbf{W}_o \begin{bmatrix} \mathbf{h}_1 \\ \vdots \\ \mathbf{h}_h \end{bmatrix} \in \mathbb{R}^{p_o}\)

## 3. 设计优势

每个注意力头可关注输入的不同部分，能表达比 “简单加权平均” 更复杂的函数关系。

要不要我帮你补充一份**多头注意力参数维度对应表**，方便你更清晰地梳理各变量的尺寸关系？