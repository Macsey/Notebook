## 核心思想
让模型在相同查询（q）、键（k）、值（v）集合下，学习不同行为并组合知识，从而捕获序列内不同范围的依赖关系
每一个注意力汇聚都被称作一个头
![](assets/多头注意力/file-20251213174952429.png)
**流程**：独立学习得到的h组不同的 线性投影来变换查询、键和值--->送到注意力汇聚--->将这h个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出


## 多头注意力的数学形式化
在实现多头注意力之前，我们需要用数学语言定义其输入、参数及计算过程。
### 1. 输入定义
假设给定以下输入数据：
**查询 (Query):** $\mathbf{q} \in \mathbb{R}^{d_q}$
**键 (Key):** $\mathbf{k} \in \mathbb{R}^{d_k}$
**值 (Value):** $\mathbf{v} \in \mathbb{R}^{d_v}$
### 2. 单个注意力头的计算
对于 $h$ 个注意力头中的每一个头 $\mathbf{h}_i$ ($i = 1, \dots, h$)，其计算方法如下：
$$\mathbf{h}_i = f(\mathbf{W}_i^{(q)}\mathbf{q}, \mathbf{W}_i^{(k)}\mathbf{k}, \mathbf{W}_i^{(v)}\mathbf{v}) \in \mathbb{R}^{p_v}$$
> **参数与函数说明：**
> **可学习参数 (投影矩阵):**
 $\mathbf{W}_i^{(q)} \in \mathbb{R}^{p_q \times d_q}$
$\mathbf{W}_i^{(k)} \in \mathbb{R}^{p_k \times d_k}$
$\mathbf{W}_i^{(v)} \in \mathbb{R}^{p_v \times d_v}$
> **注意力汇聚函数 $f$:** 代表具体的注意力机制（如加性注意力或缩放点积注意力）。


### 3. 多头输出的融合

多头注意力的最终输出并非简单的叠加，而是需要经过线性转换。

将 $h$ 个头的结果连结在一起。
通过可学习的线性变换矩阵 $\mathbf{W}_o \in \mathbb{R}^{p_o \times h p_v}$ 进行映射。

计算公式如下：
$$\mathbf{W}_o \begin{bmatrix} \mathbf{h}_1 \\ \vdots \\ \mathbf{h}_h \end{bmatrix} \in \mathbb{R}^{p_o}$$

### 4. 设计意义
基于这种设计：
- **关注点多样化：** 每个头 (Head) 可能会关注输入的不同部分。
- **表达能力增强：** 相比于简单的加权平均值，多头机制可以表示更复杂的函数关系。




这是一个非常棒的问题。如果说 **Self-Attention** 是 Transformer 的“灵魂”，那么 **多头注意力 (Multi-Head Attention)** 就是让这个灵魂变得“聪明且博学”的关键。

简单一句话概括：**与其让一个人（一个头）盯着看，不如请几个专家（多个头）从不同的角度同时看，最后把大家的意见汇总起来。**

---

### 一、 为什么要搞“多头”？（直观理解）

我们在讲 Self-Attention 时举过例子：读句子时要找词与词的关系。但问题是，**词与词的关系是多种多样的**。

举个经典的例子：

"The animal didn't cross the street because it was too tired."

（这只动物没过马路，因为它太累了。）

当我们看到 **"it"** 这个词时，我们需要关注什么？

1. **语法专家（头1）**：关注句子结构。它发现 "it" 是主语，跟前面的 "animal" 是名词指代关系。
    
2. **逻辑专家（头2）**：关注因果关系。它看到后面的 "tired"（累），结合常识判断，“累”通常形容生物，所以 "it" 指的是 "animal" 而不是 "street"。
    
3. **位置专家（头3）**：关注相邻词。它可能只关注 "it" 前后的词。
    

如果是单头注意力 (Single-Head)：

它只能学到一种特征。如果你让它学了语法关系，它可能就忽略了语义逻辑。这就像只用黑白相机拍照，虽然清晰，但丢了色彩信息。

如果是多头注意力 (Multi-Head)：

我们将向量切分成 $h$ 份（比如 8 份），让 8 个头分别去学不同的特征。有的看语法，有的看语义，有的看时态... 最后拼起来，理解就立体了。

---

### 二、 核心架构图解

我们来看看多头注意力具体是怎么操作的。假设输入向量维度 $d_{model} = 512$，我们要用 $h=8$ 个头。

#### 1. 独立投影 (Linear Projections)

不像单头那样直接把 512 维拿去算。我们先定义 8 组不同的权重矩阵 $(W_Q^i, W_K^i, W_V^i)$。

- 把输入的 512 维向量，投影到 8 个不同的 **子空间 (Subspaces)** 里。
    
- 每个头只处理 $d_k = 512 / 8 = 64$ 维的数据。
    
- _目的_：给每个头一副不同的“有色眼镜”，让它们看不同的东西。
    

#### 2. 并行计算 (Scaled Dot-Product Attention)

这 8 个头**同时**干活，互不干扰。

- 头1：算出它的 Attention 结果（关注语法）。
    
- 头2：算出它的 Attention 结果（关注逻辑）。
    
- ...
    
- 头8：算出它的 Attention 结果（关注标点）。
    

#### 3. 拼接 (Concatenation)

大家都算完了，每个头手里都拿着一个 64 维的输出向量。

我们将这 8 个 64 维的向量首尾拼接 (Concat) 回去：

$$64 \times 8 = 512$$

这就恢复了原来的维度。

#### 4. 最终融合 (Final Linear)

拼接后的结果可能有点“生硬”（像是强行把几块拼图凑在一起）。

所以最后再过一个大的线性层 $W_O$ (Linear Layer)，把这 8 个头的信息混合一下，输出最终结果。

---

### 三、 数学与计算量的“魔法”

你可能会担心：**搞 8 个头，计算量岂不是变成了原来的 8 倍？**

**答案是：几乎没有增加。**

这是 Transformer 设计最精妙的地方：

- **单头**：算 1 次 $512 \times 512$ 的矩阵运算。
    
- **多头**：算 8 次 $64 \times 64$ 的矩阵运算。
    

由于 $d_k = d_{model} / h$，所以在数学上，多头的总计算量与单头大维度的计算量是**基本持平**的。我们是用“并行度”换取了“特征丰富度”，而没有牺牲太多速度。

---

### 四、 PyTorch 代码实现逻辑

在代码里，我们通常不会真的写 8 个 `for` 循环，那是很慢的。我们利用**矩阵分块**和 `view/transpose` 操作来实现并行。

这是最核心的代码段，请注意看维度变化（注释非常重要）：

Python

```python
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0, "维度必须能被头数整除"
        
        self.d_k = d_model // num_heads  # 每个头的维度 (例如 512/8 = 64)
        self.h = num_heads               # 头数 (8)
        
        # 定义 W_q, W_k, W_v
        # 注意：这里我们用一个大矩阵一次性生成所有头的 Q, K, V，然后再切分
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        
        # 最后的输出融合层
        self.fc_out = nn.Linear(d_model, d_model)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.shape[0]
        
        # 1. 线性变换 (Linear Projections)
        # Q, K, V 形状: [batch_size, seq_len, d_model]
        Q = self.w_q(query)
        K = self.w_k(key)
        V = self.w_v(value)
        
        # 2. 切分成多头 (Split Heads)
        # 变换形状: [batch, seq_len, h, d_k] -> [batch, h, seq_len, d_k]
        # 为什么要 transpose(1, 2)？为了让 h (头) 这个维度靠前，方便并行计算
        Q = Q.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        
        # 3. 计算注意力 (Scaled Dot-Product Attention)
        # 这一步和单头注意力一模一样，只是多了一个 h 维度
        # scores 形状: [batch, h, seq_len, seq_len]
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention = torch.softmax(scores, dim=-1)
        
        # x 形状: [batch, h, seq_len, d_k]
        x = torch.matmul(attention, V)
        
        # 4. 拼接 (Concat)
        # 先把 h 和 seq_len 换回来: [batch, seq_len, h, d_k]
        x = x.transpose(1, 2).contiguous()
        # 拼接最后一维: [batch, seq_len, d_model] (h * d_k = d_model)
        x = x.view(batch_size, -1, self.h * self.d_k)
        
        # 5. 最终线性融合
        return self.fc_out(x)
```

### 五、 总结

1. **为什么需要它？** 单个头容易“偏科”，多头注意力允许模型在不同的**子空间**（Subspaces）里并行捕捉不同的特征（语法、语义、韵律等），实现了**“兼听则明”**。
    
2. **怎么做到的？** 投影 $\rightarrow$ 切分 $\rightarrow$ 并行Attention $\rightarrow$ 拼接 $\rightarrow$ 融合。
    
3. **效率如何？** 通过拆分维度，在几乎不增加计算量的前提下，大幅提升了模型的表达能力。
    

这就是为什么 Transformer 在处理复杂语言任务时如此强大的核心原因之一。

[[如果分配到不同的子空间，那么每个头不就学不全了吗]]
