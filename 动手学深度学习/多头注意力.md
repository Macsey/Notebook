## 核心思想
让模型在相同查询（q）、键（k）、值（v）集合下，学习不同行为并组合知识，从而捕获序列内不同范围的依赖关系
每一个注意力汇聚都被称作一个头
![](assets/多头注意力/file-20251213174952429.png)
**流程**：独立学习得到的h组不同的 线性投影来变换查询、键和值--->送到注意力汇聚--->将这h个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出


## 多头注意力的数学形式化
在实现多头注意力之前，我们需要用数学语言定义其输入、参数及计算过程。
### 1. 输入定义
假设给定以下输入数据：
**查询 (Query):** $\mathbf{q} \in \mathbb{R}^{d_q}$
**键 (Key):** $\mathbf{k} \in \mathbb{R}^{d_k}$
**值 (Value):** $\mathbf{v} \in \mathbb{R}^{d_v}$
### 2. 单个注意力头的计算
对于 $h$ 个注意力头中的每一个头 $\mathbf{h}_i$ ($i = 1, \dots, h$)，其计算方法如下：
$$\mathbf{h}_i = f(\mathbf{W}_i^{(q)}\mathbf{q}, \mathbf{W}_i^{(k)}\mathbf{k}, \mathbf{W}_i^{(v)}\mathbf{v}) \in \mathbb{R}^{p_v}$$
> **参数与函数说明：**
> **可学习参数 (投影矩阵):**
 $\mathbf{W}_i^{(q)} \in \mathbb{R}^{p_q \times d_q}$
$\mathbf{W}_i^{(k)} \in \mathbb{R}^{p_k \times d_k}$
$\mathbf{W}_i^{(v)} \in \mathbb{R}^{p_v \times d_v}$
> **注意力汇聚函数 $f$:** 代表具体的注意力机制（如加性注意力或缩放点积注意力）。


### 3. 多头输出的融合

多头注意力的最终输出并非简单的叠加，而是需要经过线性转换。

将 $h$ 个头的结果连结在一起。
通过可学习的线性变换矩阵 $\mathbf{W}_o \in \mathbb{R}^{p_o \times h p_v}$ 进行映射。

计算公式如下：
$$\mathbf{W}_o \begin{bmatrix} \mathbf{h}_1 \\ \vdots \\ \mathbf{h}_h \end{bmatrix} \in \mathbb{R}^{p_o}$$

### 4. 设计意义
基于这种设计：
- **关注点多样化：** 每个头 (Head) 可能会关注输入的不同部分。
- **表达能力增强：** 相比于简单的加权平均值，多头机制可以表示更复杂的函数关系。
