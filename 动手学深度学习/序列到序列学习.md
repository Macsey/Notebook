![[Pasted image 20251209220931.png]]
## 原理
**编码器**：将长度可变的输入序列转换为固定形状的隐状态，编码输入序列信息。
**解码器**：基于编码器输出的隐状态（上下文变量）和已生成的输出词元，逐步预测下一个词元，直至生成 “\<eos\>”（序列结束词元）。
**关键标识**：解码器初始输入为 “\<bos\>”（序列开始词元），依赖编码器最终隐状态初始化自身隐状态。

## 编码器
![[Pasted image 20251210205620.png]]
从技术上讲，编码器将长度可变的输入序列转换成 形状固定的上下文变量c， 并且将输入序列的信息在该上下文变量中进行编码。
设输入序列是$x_1, \ldots, x_T$其中$x_t$是输入文本序列中的第t个词元。 在时间步t，循环神经网络将词元$x_t$的输入特征向量 $x_t$和$\mathbf{h} _{t-1}$（即上一时间步的隐状态） 转换为$\mathbf{h}_t$（即当前步的隐状态）。
$$\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1}).$$编码器通过选定的函数q， 将所有时间步的隐状态转换为**上下文变量**：
$$\mathbf{c} =  q(\mathbf{h}_1, \ldots, \mathbf{h}_T).$$
比如，当选择$q(\mathbf{h}_1, \ldots, \mathbf{h}_T) = \mathbf{h}_T$时 ， 上下文变量仅仅是输入序列在最后时间步的隐状态$\mathbf{h}_T$。
### 设计
1. **单向**循环神经网络来设计编码器， 其中隐状态只依赖于输入子序列(由输入序列的开始位置到隐状态所在的时间步的位置 （包括隐状态所在的时间步）组成)
2. **双向**循环神经网络构造编码器， 其中隐状态依赖于两个输入子序列,隐状态对整个序列的信息都进行了编码
## 解码器
**初始化**:直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态
**解码器隐藏层变换**:
$$\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1})$$
在输出序列上的任意时间步$t^\prime$， 循环神经网络将来自上一时间步的输出$y_{t^\prime-1}$和上下文变量$\mathbf{c}$作为其输入， 然后在当前时间步将它们和上一隐状态 $\mathbf{s}_{t^\prime-1}$转换为 隐状态$\mathbf{s}_{t^\prime}$。
**预测输出词元的概率分布**:在循环神经网络解码器的最后一层使用全连接层来变换隐状态,解码器输出$y_{t'}$的概率取决于先前的输出子序列$y_1, \ldots, y_{t'-1}$和上下文变量$\mathbf{c}$， 即$P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})$

## 损失函数
可以使用softmax来获得分布， 并通过计算交叉熵损失函数来进行优化
**排除填充词元的损失**：零值化屏蔽不相关的项， 以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。

## BELU
$$\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},$$
本质：**看模型翻译的句子，和标准答案有多像**
1. **BLEU 的核心思路：比 “n-gram 的重合度”**
    
    “n-gram” 就是连续的 n 个词。比如句子 “i love you”，1-gram 是`i`、`love`、`you`；2-gram 是`i love`、`love you`。
    
    BLEU 的逻辑很简单：**模型翻译的句子里，有多少个 n-gram，是和标准答案里的 n-gram 重合的？重合越多，翻译越准**。
    
1. **两个关键细节
    
    - **短序列惩罚**：如果模型耍小聪明，只翻译一个很短的词（比如把 “我爱你” 翻译成 “i”），虽然这个词大概率和标准答案重合，但翻译得太烂了。所以 BLEU 会给短句子扣分，句子越短，惩罚越重。
    - **n-gram 的上限**：通常看 1 到 4 个词的重合度（1-gram 到 4-gram），综合这几个维度的重合度，算出最终分数。
2. **大白话举例**
    
    - 标准答案：`va !`
    - 模型翻译 1：`va !` → 1-gram（`va`、`!`）全重合，2-gram（`va !`）也重合 → BLEU 接近 1，翻译完美。
    - 模型翻译 2：`va` → 1-gram 只重合`va`，没有`!`，而且句子短 → BLEU 低很多。
    - 模型翻译 3：`je va` → 只有`va`重合 → BLEU 也不高。