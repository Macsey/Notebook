![[Pasted image 20251209220931.png]]
## 原理
**编码器**：将长度可变的输入序列转换为固定形状的隐状态，编码输入序列信息。
**解码器**：基于编码器输出的隐状态（上下文变量）和已生成的输出词元，逐步预测下一个词元，直至生成 “\<eos\>”（序列结束词元）。
**关键标识**：解码器初始输入为 “\<bos\>”（序列开始词元），依赖编码器最终隐状态初始化自身隐状态。

## 编码器
![[Pasted image 20251210205620.png]]
从技术上讲，编码器将长度可变的输入序列转换成 形状固定的上下文变量c， 并且将输入序列的信息在该上下文变量中进行编码。
设输入序列是$x_1, \ldots, x_T$其中$x_t$是输入文本序列中的第t个词元。 在时间步t，循环神经网络将词元$x_t$的输入特征向量 $x_t$和$\mathbf{h} _{t-1}$（即上一时间步的隐状态） 转换为$\mathbf{h}_t$（即当前步的隐状态）。
$$\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1}).$$编码器通过选定的函数q， 将所有时间步的隐状态转换为**上下文变量**：
$$\mathbf{c} =  q(\mathbf{h}_1, \ldots, \mathbf{h}_T).$$
比如，当选择$q(\mathbf{h}_1, \ldots, \mathbf{h}_T) = \mathbf{h}_T$时 ， 上下文变量仅仅是输入序列在最后时间步的隐状态$\mathbf{h}_T$。
### 设计
1. **单向**循环神经网络来设计编码器， 其中隐状态只依赖于输入子序列(由输入序列的开始位置到隐状态所在的时间步的位置 （包括隐状态所在的时间步）组成)
2. **双向**循环神经网络构造编码器， 其中隐状态依赖于两个输入子序列,隐状态对整个序列的信息都进行了编码
## 解码器
**初始化**:直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态
**解码器隐藏层变换**:
$$\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1})$$
在输出序列上的任意时间步$t^\prime$， 循环神经网络将来自上一时间步的输出$y_{t^\prime-1}$和上下文变量$\mathbf{c}$作为其输入， 然后在当前时间步将它们和上一隐状态 $\mathbf{s}_{t^\prime-1}$转换为 隐状态$\mathbf{s}_{t^\prime}$。
**预测输出词元的概率分布**:在循环神经网络解码器的最后一层使用全连接层来变换隐状态,解码器输出$y_{t'}$的概率取决于先前的输出子序列$y_1, \ldots, y_{t'-1}$和上下文变量$\mathbf{c}$， 即$P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})$

## 损失函数
可以使用softmax来获得分布， 并通过计算交叉熵损失函数来进行优化
**排除填充词元的损失**：零值化屏蔽不相关的项， 以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。

## BELU
$$\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},$$
本质：**看模型翻译的句子，和标准答案有多像**
1. **BLEU 的核心思路：比 “n-gram 的重合度”**
    
    “n-gram” 就是连续的 n 个词。比如句子 “i love you”，1-gram 是`i`、`love`、`you`；2-gram 是`i love`、`love you`。
    
    BLEU 的逻辑很简单：**模型翻译的句子里，有多少个 n-gram，是和标准答案里的 n-gram 重合的？重合越多，翻译越准**。
    
1. **两个关键细节
    
    - **短序列惩罚**：如果模型耍小聪明，只翻译一个很短的词（比如把 “我爱你” 翻译成 “i”），虽然这个词大概率和标准答案重合，但翻译得太烂了。所以 BLEU 会给短句子扣分，句子越短，惩罚越重。
    - **n-gram 的上限**：通常看 1 到 4 个词的重合度（1-gram 到 4-gram），综合这几个维度的重合度，算出最终分数。
2. **大白话举例**
    
    - 标准答案：`va !`
    - 模型翻译 1：`va !` → 1-gram（`va`、`!`）全重合，2-gram（`va !`）也重合 → BLEU 接近 1，翻译完美。
    - 模型翻译 2：`va` → 1-gram 只重合`va`，没有`!`，而且句子短 → BLEU 低很多。
    - 模型翻译 3：`je va` → 只有`va`重合 → BLEU 也不高。




### 一、 为什么要发明 Seq2Seq？

在 Seq2Seq 出现之前（2014年以前），神经网络只能处理“定长输入”到“定长输出”的任务，比如：

- **图像分类**：一张图（固定大小） $\rightarrow$ 一个标签（固定大小）。
    
- **简单的序列标注**：输入5个词 $\rightarrow$ 输出5个词性。
    

**但现实世界的大多数问题不是这样的：**

- **机器翻译**：
    
    - Input: "How are you?" (3个词)
        
    - Output: "你好吗？" (3个字)
        
    - Output: "Ca va?" (2个词)
        
    - **问题**：输入和输出长度不一样，而且不对应。
        
- **对话系统**：
    
    - Input: "今天天气真不错，我们要不要去公园玩？" (长句子)
        
    - Output: "好啊。" (短句子)
        

**Seq2Seq 就是为了解决“输入序列长度 $N$ 和输出序列长度 $M$ 不一致”的问题（Many-to-Many）。**

---

### 二、 核心架构：Encoder-Decoder (编码器-解码器)

Seq2Seq 不强制要求你用哪种神经网络（RNN, LSTM, GRU, 甚至 CNN 都可以），但它规定了必须由两部分组成：

#### 1. Encoder (编码器) —— 负责“读”和“压缩”

- **动作**：它接收输入序列 $x_1, x_2, ..., x_T$。
    
- **过程**：它就像一个阅读者，读完整个句子，把所有的信息压缩成**一个**固定的向量。
    
- **产出**：**Context Vector (上下文向量)**。
    
    - 在 RNN 中，这通常是最后一个时间步的隐藏状态 ($h_T$)。
        
    - _比喻_：这就像你读完一段英文，脑子里留下的那个“概念”或“意思”。
        

#### 2. Decoder (解码器) —— 负责“写”和“展开”

- **动作**：它拿着 Encoder 给它的 Context Vector，开始逐个生成输出序列 $y_1, y_2, ..., y_{T'}$。
    
- **过程**：
    
    - 它是一个**自回归 (Auto-regressive)** 模型。
        
    - 它生成的第一个词，会作为生成第二个词的输入。
        
- **产出**：翻译后的句子或回答。
    
    - _比喻_：这就像你根据脑子里的那个“意思”，用中文把它写出来。
        

---

### 三、 详细工作流程 (Step-by-Step)

假设我们要训练一个模型把 "Hello" 翻译成 "你好"。

#### 第一步：Encoder 编码

1. 输入 "Hello"。
    
2. RNN 处理完 "Hello"，得到最终隐藏状态 $C$ (Context Vector)。
    
3. 这个 $C$ 包含了 "Hello" 的语义信息。
    

#### 第二步：Decoder 解码 (最关键的循环)

Decoder 需要三个东西来开始工作：

1. **初始状态**：通常就是 Encoder 给的 $C$。
    
2. **初始输入**：一个特殊的**开始符号 `<SOS>`** (Start Of Sentence)。
    
3. **循环生成**：
    
    - **时刻 1**：
        
        - 输入：`<SOS>` + 状态 $C$
            
        - RNN 计算...
            
        - 输出概率最高的词：**"你"**。
            
    - **时刻 2**：
        
        - 输入：**"你"** (上一步的输出变成了这一步的输入) + 上一步的状态。
            
        - RNN 计算...
            
        - 输出概率最高的词：**"好"**。
            
    - **时刻 3**：
        
        - 输入：**"好"** + 上一步的状态。
            
        - RNN 计算...
            
        - 输出概率最高的词：**`<EOS>`** (End Of Sentence)。
            
    - **结束**：看到结束符号，停止生成。
        

---

### 四、 两个必须要懂的技术细节

#### 1. 训练时的作弊：Teacher Forcing (教师强制)

在**训练**阶段，如果 Decoder 在第一步就把 "你" 预测成了 "我"，那第二步基于 "我" 去预测，后面就全乱了。这就很难训练。

- **Teacher Forcing**：在训练时，不管 Decoder 上一步预测得对不对，**我们在下一步输入给它的，永远是标准答案（Ground Truth）。**
    
- _比喻_：老师教学生写字，学生写错了第一笔，老师立刻握着他的手纠正，让他基于正确的笔画写第二笔，而不是让他错下去。
    

#### 2. 推理时的搜索：Beam Search (集束搜索)

在**预测**阶段（考试时），没有标准答案了。

- **Greedy Search (贪婪搜索)**：每一步都选概率最大的那个词。但局部最优不等于全局最优（可能后面路走不通）。
    
- **Beam Search**：每一步都保留概率最高的 $K$ 个候选词（比如前3名），走几步后再看看哪条路总分最高。
    

---

### 五、 Seq2Seq 的主要缺陷 (引出 Attention)

我们刚才讲的是 **基础版 Seq2Seq** (Basic Encoder-Decoder)。它有一个巨大的**死穴**：

**Context Vector 的瓶颈 (Information Bottleneck)**。

- Encoder 必须把一整句话（无论多长，也许是100个词）都压缩成**一个**固定长度的向量（比如长度为 256）。
    
- 这就像让你用 256 个字概括整本《红楼梦》。
    
- **后果**：句子越长，丢失的信息越多，翻译效果越差。
    

解决办法：

就是你刚刚学过的 Bahdanau Attention！

- 不再强行压缩成一个向量。
    
- Decoder 每生成一个词，就去查看 Encoder 的所有历史状态（所有词的特征）。
    
- 所以：**Seq2Seq + Attention = 现代 NLP 的雏形**。
    

### 六、 代码逻辑简述 (PyTorch)

Python

```
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, source, target, teacher_forcing_ratio=0.5):
        # 1. Encoder 跑一遍，拿到 context vector
        # hidden 就是那个包含了整句话意思的 Context
        encoder_outputs, hidden = self.encoder(source) 

        # 2. 准备 Decoder 的第一个输入 <SOS>
        input_token = target[0, :] 
        
        outputs = []
        
        # 3. Decoder 循环生成
        for t in range(1, target_len):
            # 这里的 forward 可能会包含 Attention 机制
            output, hidden = self.decoder(input_token, hidden, encoder_outputs)
            
            outputs.append(output)
            
            # 决定下一个输入是用标准答案(Teacher Force)还是自己的预测
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1) 
            
            input_token = target[t] if teacher_force else top1
            
        return outputs
```

### 总结

1. **Seq2Seq** 是一个解决输入输出长度不一致问题的**框架**。
    
2. 它由 **Encoder（读/压）** 和 **Decoder（写/解）** 组成。
    
3. 最原始的 Seq2Seq 依赖中间的一个 **Context Vector** 传递信息，但这会导致长句子信息丢失。
    
4. 为了解决这个问题，后来加上了 **Attention** 机制。
    
5. **Transformer** 本质上也是一个 Seq2Seq 模型，只是它的 Encoder 和 Decoder 都不用 RNN 了，全换成了 Self-Attention。