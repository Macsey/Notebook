## 1.3 梯度下降 

### 1. 算法原理
**直觉**：把损失函数想象成一座山，我们在山上某一点。为了最快下山，我们应该沿着**当前坡度最陡峭的反方向**走一步。

**一维更新公式**：
$$x \leftarrow x - \eta f'(x)$$

其中 $\eta$ (eta) 是**学习率 (Learning Rate)**，决定了步子迈多大。
**停止条件**：当梯度绝对值∣f′(x)∣足够小，或迭代次数达到预设值时停止

### 影响因素
1. 学习率
2. 局部最小值：非凸函数

### 2. 多维梯度下降

- **公式**：
$$x \leftarrow x - \eta \nabla f(x)$$
$\nabla f(x)$ 是梯度向量，指向函数值增加最快的方向，所以我们要减去它。
 缺陷：缓慢

## 3.牛顿法
$x←x−ηH−1∇f(x)$
优势：凸函数中收敛快
局限：非凸函数中易发散