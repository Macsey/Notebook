### 为什么需要 RMSProp？

在此之前，**Adagrad** 算法虽然引入了自适应学习率，但存在一个致命缺陷：

- Adagrad 累积了**从开始到现在所有**梯度的平方和。
- 随着训练的进行，累积和 $s_t$ 会越来越大。
- 导致分母中的 $\sqrt{s_t}$ 越来越大，最终学习率 $\frac{\eta}{\sqrt{s_t}}$ 会急剧收缩直至无限接近于 0。
- **结果**：模型在还未收敛到最优解时，就因为学习率过小而停止了更新。
**RMSProp 的改进思路**：
不累积全部的历史梯度，而是使用**指数加权移动平均**来计算梯度的平方。这意味着算法“遗忘”了很久之前的梯度历史，只关注最近一段时间的梯度情况。



## 算法原理与公式 

### 符号定义

- $\theta$：模型参数
- $g_t$：第 $t$ 步的梯度，即 $\nabla_\theta J(\theta_t)$
- $s_t$：梯度平方的指数加权移动平均值 (Squared Gradient Accumulation)
- $\eta$：全局初始学习率 (Global Learning Rate)
- $\beta$：衰减率 (Decay Rate)，控制历史信息的保留程度
- $\epsilon$：平滑项 (Epsilon)，防止分母为 0

### 核心步骤

在第 $t$ 步迭代中：

1. 计算梯度：$$g_t = \nabla_\theta J(\theta_{t-1})$$
2. 计算梯度平方的移动平均 (核心区别于 Adagrad)：$$s_t = \beta s_{t-1} + (1 - \beta) g_t^2$$注意：这里的 $g_t^2$ 是指梯度的逐元素平方 (element-wise square)。
3. 参数更新：$$\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{s_t + \epsilon}} \odot g_t$$
## 直观理解

RMSProp 通过调整不同参数的学习率来适应地形：

- **在陡峭的方向 (梯度大)**：
    - $g_t^2$ 很大 $\rightarrow$ $s_t$ 变大 $\rightarrow$ 分母变大 $\rightarrow$ **学习率自动减小**。
    - **效果**：抑制震荡，防止步子迈得太大飞出该方向的最优解。
- **在平坦的方向 (梯度小)**：
    - $g_t^2$ 很小 $\rightarrow$ $s_t$ 变小 $\rightarrow$ 分母变小 $\rightarrow$ **学习率自动增大**。
    - **效果**：加速收敛，在平缓区域走得更快。

**总结**：RMSProp 使得参数更新在各个维度上更加平稳，避免了“锯齿状”的更新路径。

##  超参数设置 

通常建议的默认值如下：
- **衰减率 ($\beta$ / `alpha` in PyTorch)**：通常设为 **0.9**。
    - 这意味着 $s_t$ 主要依赖于前 10 步左右的梯度平均值。
- **学习率 ($\eta$)**：通常设为 **0.001**。
- **平滑项 ($\epsilon$)**：通常设为 $10^{-8}$。
## 与其他算法的对比 

|**算法**|**梯度积累方式**|**特点**|**缺点**|
|---|---|---|---|
|**SGD**|无积累|更新方向完全依赖当前 batch，震荡大|收敛慢，容易陷入局部极小值|
|**Momentum**|累积梯度的方向 (一阶矩)|像小球滚下山，有惯性，加速收敛|有时会冲过头|
|**Adagrad**|累积所有历史梯度平方和 (二阶矩)|适合稀疏数据，自适应学习率|**学习率单调递减，后期过早停止训练**|
|**RMSProp**|**指数加权移动平均** (二阶矩)|**解决了 Adagrad 学习率过早消失的问题**|依然可能陷入局部最优 (不如 Adam 综合)|
|**Adam**|结合 Momentum + RMSProp|同时利用一阶矩(动量)和二阶矩(自适应)|目前最常用的算法|


