
# 14. 自然语言处理：预训练 

## 14.1 词嵌入 (Word Embeddings)

**核心变革**：从“符号表示”到“分布表示”的跨越。

### 1. 独热向量 (One-hot) vs. 词嵌入

|**特性**|**独热向量 (One-hot)**|**词嵌入 (Word Embedding)**|
|---|---|---|
|**形式**|$[0, 0, 1, 0, \dots]$ 稀疏|$[0.1, -0.5, 0.8, \dots]$ 稠密|
|**维度**|等于词表大小 $|V|
|**语义**|**正交** (任意两词余弦相似度为0)|**相似** (向量距离反映语义距离)|
|**缺点**|无法度量词义相似性，空间浪费|需要大量语料训练|

### 2. Word2Vec：自监督学习的典范

**核心思想**：利用**上下文 (Context)** 来定义**词 (Word)**。“一个词的含义由它周围的词决定”。

- **自监督**：无需人工标签，文本本身就是标签（用 A 预测 B）。
    

#### 两大架构对比

|**模型**|**跳元模型 (Skip-Gram)**|**连续词袋模型 (CBOW)**|
|---|---|---|
|**输入 $\to$ 输出**|**中心词 $\to$ 上下文**|**上下文 $\to$ 中心词**|
|**概率假设**|$P(w_{context}|w_{center})$|
|**逻辑**|给定“loves”，预测“the”, “man”...|给定“the”, “man”，预测“loves”|
|**适用场景**|**小语料、生僻词**效果更好|**大语料、高频词**训练更快|
|**最终产出**|通常取**中心词向量**|通常取**上下文词向量**|

---

## 14.2 近似训练 (Approximate Training)

痛点：Word2Vec 的原始损失函数包含 Softmax：

$$P(w_o | w_c) = \frac{\exp(u_o^\top v_c)}{\sum_{w \in V} \exp(u_w^\top v_c)}$$

分母需要遍历整个词表 $V$（数百万级），导致单步计算复杂度为 $O(|V|)$，训练不可行。

### 解决方案

|**方法**|**核心逻辑**|**复杂度**|
|---|---|---|
|**负采样 (Negative Sampling)**|**多分类 $\to$ 二分类**。<br><br>  <br><br>不再计算全词表概率，只让模型区分：<br><br>  <br><br>1. 正例 (真实上下文)<br><br>  <br><br>2. 负例 (随机采样的 $k$ 个噪声词)|$O(k)$<br><br>  <br><br>($k$ 通常取 5-20)|
|**分层 Softmax (Hierarchical)**|**扁平结构 $\to$ 二叉树结构**。<br><br>  <br><br>将词表构建为霍夫曼树，预测词变成“从根节点走到叶节点”的路径选择过程。|$O(\log|

---

## 14.5 GloVe (全局向量词嵌入)

**核心定位**：Word2Vec (局部窗口) + 矩阵分解 (全局统计) 的结合体。

### 核心改进

Word2Vec 每次只看一个窗口，像“管中窥豹”；GloVe 先统计**全局共现矩阵 (Global Co-occurrence Matrix)**，直接对统计结果进行拟合。

- 损失函数：加权平方损失
    
    $$J = \sum_{i,j} f(X_{ij}) (w_i^\top \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2$$
    
    - **含义**：让词向量的点积 $w_i^\top \tilde{w}_j$ 去拟合它们共现次数的对数 $\log X_{ij}$。
        
- **特性**：
    
    - **对称性**：中心词和上下文词地位等同。最终向量通常取 $w + \tilde{w}$。
        
    - **可解释性**：$w_{ice} - w_{steam} \approx w_{solid} - w_{gas}$ (向量差值体现关系)。
        

---

## 14.6 子词嵌入 (Subword Embeddings)

**痛点**：Word2Vec 和 GloVe 都是**词级 (Word-level)** 的。

1. **OOV 问题**：遇到没见过的词（Out of Vocabulary）就瞎了。
    
2. **形态学丢失**：无法理解 `happy`, `happily`, `unhappy` 之间的词根联系。
    

### 1. fastText：固定 N-gram

**原理**：单词 = 字符 N-gram 的集合。

- **例子**：`where` ($n=3$) $\to$ `<wh`, `whe`, `her`, `ere`, `re>`, `<where>`。
    
- **计算**：词向量 = 所有 N-gram 向量 **求和**。
    
- **优势**：
    
    - **解决生僻词**：新词 `unhappyly` 可以通过 `un`, `happ`, `yly` 的向量拼出来。
        
    - **适合形态丰富语言**：如德语、俄语、法语。
        

### 2. 字节对编码 (BPE)：统计压缩

**原理**：一种**数据压缩算法**。不预设固定长度，而是根据语料统计，迭代合并**最频繁出现的字符对**。

- **流程**：`l o w`, `l o w e r` $\to$ 合并 `e r` $\to$ 合并 `l o` $\dots$
    
- **结果**：高频词保留完整（如 `the`），低频词切分为子词（如 `sub`, `word`）。
    
- **地位**：是 GPT, BERT, RoBERTa 等现代大模型的**标准分词方式**。
    

|**对比**|**fastText**|**BPE (Byte Pair Encoding)**|
|---|---|---|
|**切分依据**|滑动窗口 (N-gram)|频率统计 (合并高频对)|
|**子词长度**|固定 (如 3-6)|可变 (常用词短，生僻词长)|
|**词表大小**|不可控 (巨大)|**可控** (通过超参设定)|
|**适用**|静态词向量任务|**大模型 Tokenization**|

---

## 14.8 BERT：双向编码器表示

全称：Bidirectional Encoder Representations from Transformers

历史地位：NLP 领域的“ImageNet 时刻”，开启了大规模预训练+微调的范式。

### 1. 核心突破

解决传统模型（Word2Vec）**“一词多义”无法区分**的问题（Context-independent），同时突破 GPT 只能单向看文的限制。

- **Word2Vec**：`bank` (银行) 和 `bank` (河岸) 向量完全一样。
    
- **BERT**：根据上下文动态生成向量，**双向**（同时看左边和右边）理解语境。
    

### 2. 输入表示 (Input Representation)

BERT 的输入极其特殊，由三个嵌入**相加**而成：

1. **词元嵌入 (Token Embeddings)**：单词/子词本身的含义（使用 WordPiece/BPE）。
    
2. **片段嵌入 (Segment Embeddings)**：区分句子对。
    
    - 句子 A 全是 $E_A$，句子 B 全是 $E_B$。
        
3. **位置嵌入 (Position Embeddings)**：
    
    - **注意**：与 Transformer 原文不同，BERT 使用**可学习的位置嵌入** (Learned)，而非固定的正弦函数。
        

### 3. 预训练任务 (Pre-training Tasks)

- **掩码语言模型 (Masked LM, MLM)**：类似完形填空。随机遮盖 15% 的词，利用上下文预测它。**这是实现“双向”的关键**。
    
- **下一句预测 (Next Sentence Prediction, NSP)**：判断句子 B 是否紧接在句子 A 之后。学习句子间的逻辑关系。
    

---

### 💡 笔记复习小贴士

1. **Word2Vec vs GloVe**：一个是用“局部窗口”做预测，一个是用“全局矩阵”做拟合。
    
2. **fastText vs BPE**：fastText 是为了更好地通过词根猜词义；BPE 是为了更高效地压缩词表，让大模型能读懂所有词。
    
3. **BERT 输入**：面试常考题，**Token + Segment + Position** 三者相加，特别是位置编码是**可学习**的这一点。