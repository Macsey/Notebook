## 为什么需要
解决痛点：梯度消失和梯度爆炸
## 9.1.1门控隐状态
门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控
作用：有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态（这个机制是可学习的）
### 重置门和更新门
![](附件/Pasted%20image%2020251209165115.png)
重置门作用：控制“可能还想记住”的过去状态的数量
更新门作用：控制新状态中有多少个是旧状态的副本
重置门和更新门的计算：
$$
\begin{split}\begin{aligned}
\mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r),\\
\mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z),
\end{aligned}\end{split}
$$
### 候选隐状态
![](附件/Pasted%20image%2020251209165058.png)
$$\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h),$$
符号⊙是Hadamard积（按元素乘积）运算符。 在这里，我们使用tanh非线性激活函数来确保候选隐状态中的值保持在区间
$\mathbf{R}_t$和 $\mathbf{H}_{t-1}$的元素相乘可以减少以往状态的影响。 每当重置门$\mathbf{R}_t$中的项接近1时， 我们恢复一个普通的循环神经网络。

### 隐状态
![](附件/Pasted%20image%2020251209165957.png)
门控循环单元的最终更新公式：
$$\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1}  + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t.$$
更新门
$\mathbf{Z}_t$接近1时，模型就倾向只保留旧状态，此时，来自$\mathbf{X}_t$的信息基本上被忽略， 从而有效地跳过了依赖链条中的时间步t
$\mathbf{Z}_t$接近0时， 新的隐状态$\mathbf{H}_t$就会接近候选隐状态$\tilde{\mathbf{H}}_t$t,

总之，门控循环单元具有以下两个显著特征：

- 重置门有助于捕获序列中的短期依赖关系；
- 更新门有助于捕获序列中的长期依赖关系。
