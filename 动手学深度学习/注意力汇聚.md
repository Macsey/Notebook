注意力汇聚：查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚
作用：注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出
## 平均汇聚
$$f(x) = \frac{1}{n}\sum_{i=1}^n y_i,$$
![[Pasted image 20251211191033.png]]
## Nadaraya-Watson核回归
$$f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i,$$
## 非参数注意力汇聚

$$f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i,$$
注意力汇聚是$y_i$的加权平均。 将查询$x$和键$x_i$之间的关系建模为 **注意力权重**（attention weight）$\alpha(x, x_i)$
![[Pasted image 20251211191557.png]]
预测线是平滑的，并且比平均汇聚的预测更接近真实
为方便理解，考虑一个高斯核：$K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2})$.

$$\begin{split}\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}\end{split}$$
**优点**：一致性： 如果有足够的数据，此模型会收敛到最优结果
## 带参数注意力汇聚
$$\begin{split}\begin{aligned}f(x) &= \sum_{i=1}^n \alpha(x, x_i) y_i \\&= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_j)w)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i.\end{aligned}\end{split}$$
![[Pasted image 20251211191924.png]]
新的模型更不平滑:与非参数的注意力汇聚模型相比， 带参数的模型加入可学习的参数后， 曲线在注意力权重较大的区域变得更不平滑