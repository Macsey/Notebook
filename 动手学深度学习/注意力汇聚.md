注意力汇聚：查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚
作用：注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出
## 平均汇聚
$$f(x) = \frac{1}{n}\sum_{i=1}^n y_i,$$
![[Pasted image 20251211191033.png]]
## Nadaraya-Watson核回归
$$f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i,$$
## 非参数注意力汇聚

$$f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i,$$
注意力汇聚是$y_i$的加权平均。 将查询$x$和键$x_i$之间的关系建模为 **注意力权重**（attention weight）$\alpha(x, x_i)$
![[Pasted image 20251211191557.png]]
预测线是平滑的，并且比平均汇聚的预测更接近真实
为方便理解，考虑一个高斯核：$K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2})$.

$$\begin{split}\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}\end{split}$$
**优点**：一致性： 如果有足够的数据，此模型会收敛到最优结果
## 带参数注意力汇聚
$$\begin{split}\begin{aligned}f(x) &= \sum_{i=1}^n \alpha(x, x_i) y_i \\&= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_j)w)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i.\end{aligned}\end{split}$$
![[Pasted image 20251211191924.png]]
新的模型更不平滑:与非参数的注意力汇聚模型相比， 带参数的模型加入可学习的参数后， 曲线在注意力权重较大的区域变得更不平滑

### 一、 通俗理解：从“盲目平均”到“有的放矢”

假设你想预测 “这部新出的科幻电影好不好看？” (这是你的任务，即 Query)。

你手头有 3 个朋友对不同电影的评分（这是你的数据，Keys & Values）：

- 朋友 A（爱看动作片）：给《战狼》打了 10 分。
    
- 朋友 B（爱看科幻片）：给《流浪地球》打了 9 分。
    
- 朋友 C（爱看爱情片）：给《泰坦尼克号》打了 8 分。
    

#### 1. 平均汇聚 (Average Pooling) —— 也就是 Global Average Pooling

- **做法**：不管三七二十一，把大家的分数加起来除以 3。
    
- **结果**：$(10+9+8)/3 = 9$ 分。
    
- **问题**：完全忽略了你的需求。你明明想问“科幻片”，结果被“动作片”和“爱情片”的分数干扰了。
    

#### 2. 注意力汇聚 (Attention Pooling)

- **做法**：看看哪个朋友的口味（Key）跟你的需求（Query）最像，给他的分数（Value）更高的**权重**。
    
- **过程**：
    
    1. **计算相关性 (Attention Score)**：
        
        - 你的需求“科幻” vs 朋友A“动作” $\rightarrow$ 相似度低 (权重 0.1)
            
        - 你的需求“科幻” vs 朋友B“科幻” $\rightarrow$ **相似度极高 (权重 0.8)**
            
        - 你的需求“科幻” vs 朋友C“爱情” $\rightarrow$ 相似度低 (权重 0.1)
            
    2. 加权求和：
        
        $10 \times 0.1 + 9 \times 0.8 + 8 \times 0.1 = 1 + 7.2 + 0.8 = 9.0$
        
        (这里数字凑巧一样，但逻辑变了：主导结果的是最相关的那个 Value)。
        

**一句话总结**：注意力汇聚就是**根据 Query 和 Key 的相似度，对 Value 进行加权平均**。

---

### 二、 核心案例：Nadaraya-Watson 核回归

书中用了一个经典的统计学算法 —— **Nadaraya-Watson 核回归** 来演示这个过程。这是注意力机制的“鼻祖”。

#### 1. 问题设定

我们要根据一堆已知的点 $(x_i, y_i)$，来预测一个新的 $x$ (Query) 对应的 $y$ 是多少。

- **Query**: 新的输入 $x$。
    
- **Key**: 已知点的坐标 $x_i$。
    
- **Value**: 已知点的值 $y_i$。
    

#### 2. 公式解析 (非参数版本)

我们怎么衡量 $x$ 和 $x_i$ 近不近？用一个**核函数 (Kernel)**，最常用的就是**高斯核**（正态分布曲线）。

$$f(x) = \sum_{i=1}^n \underbrace{\text{softmax}\left( -\frac{1}{2}(x - x_i)^2 \right)}_{\text{注意力权重 } \alpha(x, x_i)} \times y_i$$

- **$(x - x_i)^2$**：距离越近，这个值越小。
    
- **负号**：距离越近，结果越大（相似度越高）。
    
- **Softmax**：把所有相似度归一化，变成概率（总和为1）。
    
- **$y_i$**：加权求和。
    

这不就是 Transformer 里的 Attention 吗？

没错！

- $Q$ 就是 $x$
    
- $K$ 就是 $x_i$
    
- $V$ 就是 $y_i$
    
- 只是这里的计算相似度没用点积，而是用了距离（欧氏距离）。
    

#### 3. 进阶：带参数版本 (Parametric Attention)

上面的公式里没有任何可以学习的参数（死算距离）。如果数据很复杂，死算距离可能不准。

于是我们在距离计算里乘上一个可学习的权重 $w$：

$$-\frac{1}{2}((x - x_i)w)^2$$

- 通过训练，神经网络会自己调整 $w$。
    
- **如果 $w$ 很大**：曲线变窄，模型只关注非常非常近的点（“钻牛角尖”）。
    
- **如果 $w$ 很小**：曲线变宽，模型会关注更远的点（“目光长远”）。
    
- 这就是深度学习中注意力机制的雏形：**让模型自己学会该关注多大范围。**
    

---

### 三、 学习笔记 

#### 10.2 注意力汇聚 (Attention Pooling) 笔记

1. 概念定位

注意力汇聚是连接传统池化与现代 Transformer 的桥梁。

- **传统池化**：无视 Query，对所有输入等权重处理（如平均池化）。
    
- **注意力汇聚**：引入 Query，根据 Query 与 Key 的匹配程度，动态计算 Value 的加权平均。
    

2. 核心公式 (Nadaraya-Watson)

$$f(x) = \sum_{i} \alpha(x, x_i) y_i$$

其中 $\alpha(x, x_i)$ 是注意力权重。

3. 映射关系 (必背)

在回归问题中：

- **Query ($q$)**: 待预测的输入位置 $x$。
    
- **Key ($k$)**: 训练数据的输入特征 $x_{train}$。
    
- **Value ($v$)**: 训练数据的标签值 $y_{train}$。
    
- **Score**: 衡量 $q$ 和 $k$ 的距离（如高斯核）。
    

**4. 两种模式**

- **非参数 (Non-parametric)**：直接根据距离计算权重，无须训练。类似于 K-近邻算法 (KNN) 的平滑版。
    
- **带参数 (Parametric)**：引入可学习参数 $w$，让网络根据数据特征自动调整“距离的度量方式”或“关注的视野范围”。