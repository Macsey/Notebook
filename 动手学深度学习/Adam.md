
- **核心思想**：它结合了 **Momentum** (动量法) 和 **RMSProp** 的优势，既考虑了梯度的方向（一阶矩），又考虑了梯度的大小（二阶矩）。

## 核心机制

Adam 可以被看作是以下两种算法的结合体：
1. **动量**：通过累积梯度的**一阶矩**（平均值），让参数更新具有“惯性”，在正确的方向上加速。
2. **RMSProp**：通过累积梯度的**二阶矩**（未中心化的方差/平方和），实现**自适应学习率**，让更新幅度适应不同参数的梯度大小。
## 算法原理与公式 

### 符号定义
- $g_t$：第 $t$ 步的梯度。
- $m_t$：**一阶矩估计** (梯度的指数移动平均，类似 Momentum)。
- $v_t$：**二阶矩估计** (梯度平方的指数移动平均，类似 RMSProp)。
- $\beta_1, \beta_2$：衰减率.

### 详细步骤 
1. 计算梯度：$$g_t = \nabla_\theta J(\theta_{t-1})$$
2. 更新一阶矩 (动量部分)：$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$ 这部分负责记录“方向和惯性”。
3. 更新二阶矩 (RMSProp 部分)：$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$
   这部分负责记录“梯度的震荡幅度/大小”。
4. 偏差修正 (Bias Correction) —— Adam 的独特之处：
    由于 $m_0$ 和 $v_0$ 初始化为 0，在训练初期（$t$ 较小时），$m_t$ 和 $v_t$ 会严重偏向 0。
    为了修正这个偏差，除以 $(1 - \beta^t)$：$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$
5. 参数更新：$$\theta_t = \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$
## 直观理解 

想象一个小球滚下山坡：
- **SGD**：小球像个醉汉，每一步只看脚下，容易跌跌撞撞。
- **RMSProp**：给小球穿上了“防滑鞋”，在陡峭的地方增加阻力（减小步长），在平坦的地方减少阻力（增大步长）。
- **Adam**：既穿上了“防滑鞋”（自适应步长），又给小球加了一个“助推器”（动量惯性）。
    - **惯性**让它在平缓的谷底也能冲向最优点。
    - **自适应**让它在狭窄的山谷壁上不会来回剧烈震荡。


## 超参数设置 
Adam 的超参数在很多任务中都非常稳定，论文推荐的默认值如下：
- **学习率 ($\eta$ / lr)**：通常设为 **0.001** (1e-3)。
- **$\beta_1$ (动量衰减)**：通常设为 **0.9**。
- **$\beta_2$ (平方衰减)**：通常设为 **0.999**。
- **$\epsilon$ (Epsilon)**：通常设为 $10^{-8}$。

## 6. Adam 的一个重要变体：AdamW

在使用 Adam 时，有一个著名的坑需要注意：**权重衰减 (Weight Decay)**。

- **问题**：在 Adam 中直接使用 L2 正则化（Weight Decay）效果不如在 SGD 中好，因为 L2 正则项也被放入了自适应学习率的计算中，导致正则化力度被削弱。
- **区别**：AdamW 将权重衰减项直接作用于参数更新步骤，而不是加在梯度上。
- **结论**：如果你在训练 Transformer (BERT, GPT) 或现代 CNN，请优先使用 AdamW 而不是 Adam。


##  总结对比 

| **算法**           | **组成部分**           | **优点**          | **缺点**            |
| ---------------- | ------------------ | --------------- | ----------------- |
| **SGD**          | 梯度                 | 简单，理论收敛性好       | 收敛慢，怕鞍点           |
| **SGD+Momentum** | 梯度 + 一阶矩           | 加速收敛，冲出局部极值     | 需要调节超参数           |
| **RMSProp**      | 梯度 + 二阶矩           | 解决学习率消失，自适应     | 依然可能陷入局部最优        |
| **Adam**         | **梯度 + 一阶矩 + 二阶矩** | **收敛快，超参数鲁棒性高** | 某些情况下泛化能力可能不如 SGD |
