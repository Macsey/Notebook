注意力评分函数：高斯核的指数部分
![[Pasted image 20251211193217.png]]
注意力评分函数--->softmax--->键和值的概率分布（注意力权重）--->输出(基于这些注意力权重的值的加权和)
注意力汇聚的输出本质是 “值的加权和”，权重由 “注意力评分函数（a (q,k)）计算查询（q）与键（k）的关联度，再经 softmax 归一化” 得到。

## 掩蔽 softmax 操作
目的：过滤无意义数据（如文本序列填充的特殊词元），仅保留有效序列长度内的 “键 - 值” 对参与注意力计算。
实现：将超出有效长度的元素替换为极小负值，经 softmax 后输出近似 0，不影响加权和结果。

## 加性注意力
适用场景:查询（q）与键（k）维度不同
评分函数:
$$a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},$$其中$\mathbf W_q\in\mathbb R^{h\times q}$、$\mathbf W_k\in\mathbb R^{h\times k}$、$\mathbf w_v\in\mathbb R^{h}$为可学习参数，h 为隐藏层单元数（超参数）
两层线性层$\mathbf W_q\in\mathbb R^{h\times q}$$\mathbf W_k\in\mathbb R^{h\times k}$分别将查询、键映射到 h 维隐藏空间

## 缩放点积注意力
查询与键维度相同（q=k=d）时，通过点积快速计算关联度，计算效率高于加性注意力。
评分函数:
$$a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}.$$
缩放点积注意力:
$$\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}.$$
查询和键的长度为$d$，值的长度为$v$。 查询$\mathbf Q\in\mathbb R^{n\times d}$、 键$\mathbf K\in\mathbb R^{m\times d}$和值$\mathbf V\in\mathbb R^{m\times v}$
