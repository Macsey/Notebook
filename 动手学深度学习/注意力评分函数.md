注意力评分函数：高斯核的指数部分
![[Pasted image 20251211193217.png]]
注意力评分函数--->softmax--->键和值的概率分布（注意力权重）--->输出(基于这些注意力权重的值的加权和)
注意力汇聚的输出本质是 “值的加权和”，权重由 “注意力评分函数（a (q,k)）计算查询（q）与键（k）的关联度，再经 softmax 归一化” 得到。

## 掩蔽 softmax 操作
目的：过滤无意义数据（如文本序列填充的特殊词元），仅保留有效序列长度内的 “键 - 值” 对参与注意力计算。
实现：将超出有效长度的元素替换为极小负值，经 softmax 后输出近似 0，不影响加权和结果。

## 加性注意力
适用场景:查询（q）与键（k）维度不同
评分函数:
$$a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},$$其中$\mathbf W_q\in\mathbb R^{h\times q}$、$\mathbf W_k\in\mathbb R^{h\times k}$、$\mathbf w_v\in\mathbb R^{h}$为可学习参数，h 为隐藏层单元数（超参数）
两层线性层$\mathbf W_q\in\mathbb R^{h\times q}$$\mathbf W_k\in\mathbb R^{h\times k}$分别将查询、键映射到 h 维隐藏空间

## 缩放点积注意力
查询与键维度相同（q=k=d）时，通过点积快速计算关联度，计算效率高于加性注意力。
评分函数:
$$a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}.$$
缩放点积注意力:
$$\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}.$$
查询和键的长度为$d$，值的长度为$v$。 查询$\mathbf Q\in\mathbb R^{n\times d}$、 键$\mathbf K\in\mathbb R^{m\times d}$和值$\mathbf V\in\mathbb R^{m\times v}$













我们在上一节通过“注意力汇聚”知道了公式：

$$f(x) = \sum_{i} \text{softmax}(\alpha(q, k_i)) v_i$$

这里的 $\alpha(q, k_i)$ 就是 注意力评分函数 (Attention Scoring Function)。

它的任务非常简单粗暴：**给出一个数字，告诉我在当前的 $q$ 下，这个 $k$ 有多重要**（匹配度有多高）。

如果没有这个函数算出分数，Softmax 就没法工作，Value 也就没法加权。

深度学习中最常用的有两种评分函数：**加性注意力 (Additive)** 和 **缩放点积注意力 (Scaled Dot-Product)**。

---

### 一、 两个主角的对比

在讲数学之前，先建立一个直观印象：

|**特性**|**加性注意力 (Additive)**|**缩放点积注意力 (Scaled Dot-Product)**|
|---|---|---|
|**形象比喻**|**复杂的面试**：把 Q 和 K 叫到一个房间，通过一系列复杂的问答（全连接层+激活函数）来打分。|**简单的对暗号**：Q 和 K 直接碰一下（点积），看看方向对不对得上。|
|**计算量**|大（涉及矩阵运算和 Tanh 激活）|小（高度优化的矩阵乘法）|
|**适用场景**|当 Q 和 K 的向量长度 **不一样** 时。|当 Q 和 K 的向量长度 **一样** 时（Transformer 的标配）。|
|**历史地位**|早期 Seq2Seq 模型 (Bahdanau Attention) 的核心。|现代 Transformer 的核心。|

---

### 二、 详细剖析

#### 1. 加性注意力 (Additive Attention)

这是 Bahdanau 等人在 2014 年做机器翻译时最早提出的。

- **场景**：假设你的 Query 向量长度是 $d_q$，Key 向量长度是 $d_k$。这就尴尬了，长度不一样，没法直接做点积。
    
- **解决办法**：用一个单隐藏层的 MLP（多层感知机）强行把它们融合。
    
- 数学公式：
    
    $$a(\mathbf{q}, \mathbf{k}) = \mathbf{w}_v^\top \tanh(\mathbf{W}_q\mathbf{q} + \mathbf{W}_k\mathbf{k})$$
    
    - **$\mathbf{W}_q, \mathbf{W}_k$**：两个可学习的矩阵。作用是把 $q$ 和 $k$ 强行拉伸/压缩到同一个维度 $h$（隐藏单元数）。
        
    - **$+$**：把变换后的 $q$ 和 $k$ 加起来。
        
    - **$\tanh$**：激活函数，增加非线性（让模型更聪明）。
        
    - **$\mathbf{w}_v^\top$**：最后用一个向量做一个线性变换，把结果压扁成一个标量（分数）。
        
- **通俗解读**：既然 $q$ 和 $k$ 尺寸不合，我就给你们各自配一个“整形医生”（$W_q, W_k$），整成一样高，然后合体，过一遍安检（$\tanh$），最后打分。
    

#### 2. 缩放点积注意力 (Scaled Dot-Product Attention)

这是 Transformer (Vaswani, 2017) 采用的方案，因为它**快**且**省内存**。

- **场景**：Query 和 Key 的长度必须相同，都是 $d$。
    
- 数学公式：
    
    $$a(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^\top \mathbf{k}}{\sqrt{d}}$$
    
    - **$\mathbf{q}^\top \mathbf{k}$**：点积。在线性代数里，点积代表两个向量的相似度（方向是否一致）。
        
    - **$\sqrt{d}$ (关键点)**：这就是“缩放”的来源。
        
- **为什么一定要除以 $\sqrt{d}$？(必考面试题)**
    
    - 当向量维度 $d$ 很大时（比如 BERT 是 768 或 1024），点积的结果 $\mathbf{q}^\top \mathbf{k}$ 可能会非常大（比如几百、几千）。
        
    - **Softmax 的副作用**：如果输入 Softmax 的数值很大（比如一个 100，其他是 1），Softmax 算出来的概率分布就会极其尖锐（100 的那个接近 1，其他全是 0）。
        
    - **梯度消失**：在 Softmax 极其尖锐的地方，梯度的导数几乎为 0。模型就学不动了。
        
    - **作用**：除以 $\sqrt{d}$ 将数值拉回到一个比较正常的范围（方差为1），保证梯度流动顺畅。
        

---

### 三、 PyTorch 代码实现

为了加深理解，我们分别看看它们在 PyTorch 里长什么样。

#### 1. 实现加性注意力

你需要定义三个可学习的参数层。

Python

```python
import torch
import torch.nn as nn

class AdditiveAttention(nn.Module):
    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
        super(AdditiveAttention, self).__init__(**kwargs)
        # 将 Key 转换维度的线性层
        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)
        # 将 Query 转换维度的线性层
        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)
        # 最后的评分向量
        self.w_v = nn.Linear(num_hiddens, 1, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        # queries: [batch_size, query_len, query_size]
        # keys:    [batch_size, key_len, key_size]
        
        # 1. 线性变换并扩维以便相加
        # 结果形状: [batch_size, query_len, num_hiddens]
        q = self.W_q(queries) 
        # 结果形状: [batch_size, key_len, num_hiddens]
        k = self.W_k(keys)    

        # 2. 核心计算: tanh(W_q q + W_k k)
        # 这是一个广播加法，计算这一步稍微有点绕，
        # 需要把 q 变成 [batch, q_len, 1, hidden]
        # 把 k 变成 [batch, 1, k_len, hidden] 才能相加
        # 这里为了简化代码逻辑展示，略去具体的unsqueeze操作
        # features = q.unsqueeze(2) + k.unsqueeze(1) 
        features = torch.tanh(q.unsqueeze(2) + k.unsqueeze(1))

        # 3. 算出分数
        scores = self.w_v(features).squeeze(-1)
        
        # 4. 放入 Softmax 得到权重 (略去 mask 处理)
        attention_weights = nn.functional.softmax(scores, dim=-1)
        
        return torch.bmm(self.dropout(attention_weights), values)
```

#### 2. 实现缩放点积注意力 (简单很多)

没有可学习的参数（除非你把线性投影算进来，但评分函数本身没有参数）。

Python

```python
import math
import torch
import torch.nn as nn

class DotProductAttention(nn.Module):
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens=None):
        # queries: [batch, q_len, d]
        # keys:    [batch, k_len, d]
        
        d = queries.shape[-1]
        
        # 1. 核心计算: Q * K转置 / sqrt(d)
        # torch.bmm 是批量矩阵乘法
        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)
        
        # 2. Softmax
        attention_weights = nn.functional.softmax(scores, dim=-1)
        
        # 3. 加权求和
        return torch.bmm(self.dropout(attention_weights), values)
```

### 四、 总结

1. **评分函数的作用**：是 Attention 的“计算器”，决定了 $Q$ 和 $K$ 到底有多像。
    
2. **加性注意力**：像个万能转换插头，不管 $Q$ 和 $K$ 尺寸咋样，都能强行通过 MLP 算个分。它更灵活，但算得慢。
    
3. **缩放点积注意力**：是 Transformer 的引擎。它要求 $Q, K$ 维度一致，利用矩阵乘法疯狂加速，并且引入 $\sqrt{d}$ 缩放来防止梯度消失。
    

在实际应用中（包括现在的 LLM），我们**几乎 100% 使用缩放点积注意力**。加性注意力更多是作为历史背景或者特殊的小模型场景出现。