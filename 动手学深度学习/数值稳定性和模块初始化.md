
### 梯度消失和梯度爆炸

- **基本概念**：梯度是神经网络中误差对权重的导数，它能告诉我们如何调整参数来降低误差。梯度消失是指在反向传播时，梯度逐层变小，导致靠近输入层的权重几乎无法更新。梯度爆炸则是梯度在反向传播时不断放大，使得参数更新幅度过大，模型无法稳定训练。

- **[[梯度消失]]示例**：文章中提到 sigmoid 函数曾很流行，它类似阈值函数。但从绘制的函数图像和其梯度图像可以看到，当 sigmoid 函数的输入很大或很小时，它的梯度会变得很小，接近 0。如果神经网络有很多层，反向传播经过这些层时，梯度就可能像嚼泡泡糖一样，越嚼越没味道，越来越小，最终消失，导致前面的层难以学习到有用信息。

- **[[梯度爆炸]]示例**：文章通过将 100 个高斯随机矩阵与初始矩阵相乘，展示了矩阵乘积会发生爆炸。在深度神经网络中，如果初始化不当，也可能出现类似情况，导致梯度变得极大，就像滚雪球一样，越滚越大，最终难以控制，使模型训练不稳定。

- **打破[[对称性]]**：神经网络存在参数化的对称性问题。比如一个多层感知机有一个隐藏层和两个隐藏单元，如果把第一层的权重和输出层的权重进行重排列，得到的函数是一样的。如果将隐藏层的所有参数都初始化为相同的值，那么在前向传播时，两个隐藏单元产生的激活值相同，反向传播时梯度的元素值也相同，这样就无法打破对称性，网络的表达能力就会受限。不过，暂退法正则化可以打破这种对称性。

### 参数初始化

- **默认初始化**：在前面的内容中，曾使用正态分布来初始化权重值。如果不指定初始化方法，深度学习框架会使用默认的随机初始化方法，对于一些不是特别难的问题，这种方法通常也能起作用。

- [[**Xavier 初始化]]**：假设一个全连接层的输出是由输入和权重相乘得到的。如果权重和输入都有一定的均值和方差，那么可以计算出输出的均值和方差。为了让输出的方差不受输入数量的影响，同时让梯度的方差不受输出数量的影响，需要对权重的方差进行设置。Xavier 初始化就是基于这个原理，通常从均值为 0，方差为\(\frac{2}{n_{in} + n_{out}}\)的高斯分布中采样权重，也可以从特定范围的均匀分布中抽取权重，这个范围是\((-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}})\)。虽然推导过程中假设没有非线性，但这种初始化方法在实际中很有效。

### 小结

- 梯度消失和梯度爆炸是深度网络训练时常见的问题，在初始化参数时要特别注意，不然就难以控制梯度和参数。

- 可以用一些巧妙的初始化方法，让初始梯度大小合适。

- ReLU 这类激活函数能缓解梯度消失问题，帮助模型更快收敛。

- 随机初始化参数很重要，它能打破神经网络的对称性。

- Xavier 初始化能让每一层输出和梯度的方差更稳定，不受输入输出数量的太大影响。