**核心原理**：深度学习框架通过后端自动构建计算图，识别任务间的依赖关系，**对无依赖的任务进行并行执行**，以提升计算效率。
**适用场景**：并行化在单设备（如单个 CPU/GPU）上作用有限（因单个操作符通常已占用该设备全部计算资源），在多设备（多个 GPU、CPU 与 GPU 组合）场景中价值显著，同时可结合设备间通信实现计算与通信并行。

### 并行计算与通信
**关键依赖**：需先完成`y[i]`的计算，才能复制`y[i]`到 CPU，但框架通过 “计算`y[i]`+ 复制`y[i-1]`” 的重叠调度，减少总耗时，核心是利用了 CPU 与 GPU 间的 PCI-Express 总线资源。
**串行执行**：先在 GPU1 上运行`run`函数（计时），再执行`copy_to_cpu`（计时），总耗时为两步骤之和。
**并行执行**：删除计算与通信间的设备同步函数，框架可在计算当前矩阵乘法结果（如`y[i]`）时，同步复制上一结果（`y[i-1]`），总耗时显著降低，证明计算与通信可自动并行。

优势：
1. **设备与资源利用**：现代系统的多设备（多 GPU、多 CPU）和多通信资源（PCI-Express、网络带宽）可通过自动并行技术异步、并行使用。
2. **框架优势**：无需用户编写复杂调度代码，框架基于计算图自动优化，例如两层多层感知机在 “1CPU+2GPU” 上的训练，可自动处理设备间依赖与并行（如图 12.3.1 所示）。
3. **性能提升本质**：通过并行化消除任务等待时间，同时优化计算与通信的资源分配，最大化硬件利用率。
