编码器-解码器架构:循环神经网络编码器将长度可变的序列转换为固定形状的上下文变量， 然后循环神经网络解码器根据生成的词元和上下文变量 按词元生成输出（目标）序列词元
## 解决的问题
每个解码步骤使用固定上下文变量

## 核心改进
将传统编码器 - 解码器中固定的上下文变量c，替换为随解码时间步t′动态变化的ct′​
$$\mathbf{c}_{t'} = \sum_{t=1}^T \alpha(\mathbf{s}_{t' - 1}, \mathbf{h}_t) \mathbf{h}_t,$$
- st′−1​：解码时间步t′−1的解码器隐状态（作为**查询**）；
- ht​：编码器隐状态（同时作为**键**和**值**）；
- α：注意力权重，通过加性注意力打分函数计算。
![600](assets/Bahdanau/file-20251213173627507.png)

## 差异
与传统 seq2seq 架构相比，Bahdanau 注意力在解码器中引入注意力机制，动态生成上下文变量，而非依赖固定编码结果。











**《Neural Machine Translation by Jointly Learning to Align and Translate》**。

如果说 Transformer 是现代摩天大楼，那么 Bahdanau Attention 就是奠定这一切的地基。要理解它，我们得回到 Transformer 出现之前的 **RNN/LSTM 时代**。

### 一、 那个时代的痛点：固定长度的“噩梦”

在 Bahdanau 出现之前，做机器翻译（Seq2Seq）的模型是这样的：

1. **Encoder（编码器）**：读完整个英文句子 "I love deep learning"。
    
2. **Context Vector（中间变量）**：把这句话压缩成**唯一的一个**固定长度的向量 $C$。
    
3. **Decoder（解码器）**：拿着这个 $C$，试图翻译成中文。
    

问题在哪里？

这就好比我让你读一本《红楼梦》，然后让你把整本书的内容死记硬背成这一个向量 $C$（不管句子多长，向量长度固定）。

- 如果你要翻译第 1 个字，你看的是 $C$。
    
- 如果你要翻译第 100 个字，你看的还是这个 $C$。
    
- **结果**：句子一长，前面的信息就丢了（忘掉了），翻译效果极差。这被称为 **Information Bottleneck（信息瓶颈）**。
    

---

### 二、 Bahdanau 的天才想法：不要死记，要“边看边翻”

Bahdanau 提出：**为什么我要把整句话压成一个死板的向量呢？解码器在翻译每个词的时候，能不能回去看一眼原句？**

他的核心改进：

解码器每生成一个词时，都会动态地去 Encoder 那里“搜索”一下，看看原句里的哪个词跟当前要翻译的词最相关。

这就是 **“对齐” (Align)** 的概念。

---

### 三、 核心实现：加性注意力 (Additive Attention)

我们之前讲过“加性注意力”，Bahdanau Attention 就是**加性注意力**的代名词。

#### 1. 场景设定

- **Encoder 的隐藏状态 ($h$)**：这是原句子里每个词的特征（Keys & Values）。
    
- **Decoder 的上一个状态 ($s_{t-1}$)**：这是我已经翻译出来的意思，也是我现在的意图（Query）。
    

#### 2. 评分公式 (The Score Function)

Bahdanau 并没有用点积（那时候点积还不流行），而是用了一个单层神经网络（MLP）来计算匹配度。

$$score(s_{t-1}, h_j) = v_a^\top \tanh(W_a s_{t-1} + U_a h_j)$$

让我们拆解这个看起来很复杂的公式：

1. **$W_a s_{t-1} + U_a h_j$**：
    
    - $s_{t-1}$ 和 $h_j$ 的维度可能不一样。
        
    - 所以用两个矩阵 $W_a$ 和 $U_a$ 把它们**线性变换**（整形）成相同的形状。
        
    - 然后**相加**（这就是为什么叫“加性”注意力）。
        
2. **$\tanh$**：
    
    - 把加完的结果过一个激活函数，引入非线性，把数值压到 $[-1, 1]$ 之间。
        
3. **$v_a^\top$**：
    
    - 这是一个可学习的向量。它把上面那个向量**投影**成一个标量（单纯的分数）。
        

#### 3. 为什么即使慢也要用它？

你可能会问，既然点积那么快，Bahdanau 为什么非要用这么复杂的 MLP？

- **历史原因**：当时还没有缩放点积的概念。
    
- **维度灵活性**：在 RNN 模型中，Encoder 和 Decoder 的隐藏层维度经常设得不一样（比如 Encoder 256维，Decoder 512维）。加性注意力自带 $W$ 和 $U$ 矩阵，可以完美处理维度不匹配的问题。
    

---

### 四、 流程图解（翻译 "I love you" -> "我爱你"）

假设正在翻译第二个字 "爱"：

1. **Query ($s_{t-1}$)**：解码器现在的状态（刚刚翻译完了"我"，心里正想着下一个词）。
    
2. **Keys ($h_1, h_2, h_3$)**：Encoder 对 "I", "love", "you" 三个词的编码。
    
3. **Score 计算 (Bahdanau Attention)**：
    
    - 拿着 Query 去问 $h_1$ ("I")：不相关 $\rightarrow$ 低分。
        
    - 拿着 Query 去问 $h_2$ ("love")：**相关！** $\rightarrow$ **高分**。
        
    - 拿着 Query 去问 $h_3$ ("you")：不相关 $\rightarrow$ 低分。
        
4. **Context Vector**：
    
    - 根据分数加权：$0.1 \times h_1 + 0.8 \times h_2 + 0.1 \times h_3$。
        
    - 这就得到了一个**聚焦于 "love"** 的上下文向量。
        
5. **Output**：解码器结合这个上下文向量，自信地输出了 "爱"。
    

---

### 五、 Bahdanau vs. Luong (对比记忆)

在 Bahdanau 之后不久，Luong 等人提出了另一种注意力（通常指**点积注意力**或**乘性注意力**）。这是两个最经典的流派。

|**特性**|**Bahdanau Attention**|**Luong Attention**|
|---|---|---|
|**别名**|Additive (加性)|Multiplicative / Dot-Product (乘性)|
|**计算方式**|MLP ($W_1 h + W_2 s$)|点积 ($h^T W s$)|
|**速度**|**慢** (涉及大矩阵运算和 tanh)|**快** (矩阵乘法极度优化)|
|**应用位置**|作用于**上一步**的 Decoder 状态 (看原句 -> 算状态 -> 输出)|作用于**当前**的 Decoder 状态 (算状态 -> 看原句 -> 输出)|
|**适用性**|维度不同时首选|维度相同时首选 (Transformer 选了它)|

### 六、 总结

Bahdanau Attention 的历史地位在于：**它打破了“定长向量”的诅咒。**

它告诉深度学习界：不要试图把整个世界压缩进一个瓶子里；相反，给你一个**探照灯**，当你需要什么信息时，就去原数据里照亮那一块。

这就是**Attention Is All You Need**的前夜。