要理解 “自定义层”，我们可以先从一个日常比喻入手：把深度学习模型想象成一台 “数据加工流水线”。其中，“层” 就是流水线上的 “加工机器”—— 有的机器负责 “筛选数据”（比如 ReLU 激活层），有的负责 “放大 / 缩小数据维度”（比如全连接层）。但如果现成的机器满足不了我们的需求（比如想让数据 “减去自身平均值”，或自定义一种新的计算规则），就需要自己动手造一台机器 —— 这就是 “自定义层”。

接下来，我们用最通俗的语言，分 3 个核心部分把 “自定义层” 讲透，每个部分都结合代码例子和实际效果，确保零基础也能看懂。

## 一、先搞懂：为什么需要自定义层？

深度学习框架（比如 PyTorch、TensorFlow）已经给我们提供了很多 “现成的层”，比如全连接层、卷积层、Dropout 层等。但实际开发中，总会遇到 “现成层不够用” 的情况：

- 比如你想让数据经过一个 “减去自身平均值” 的操作（让数据更稳定），现成层里没有；

- 比如你想实现一种新的计算逻辑（比如输入两个数，输出它们的平方和），现成层也没有；

- 再比如研究新算法时，需要设计论文里提出的 “新型层”，框架还没来得及封装。

这时候，“自定义层” 就派上用场了 —— 它相当于给你一套 “造机器的图纸”，让你按自己的需求，打造专属的 “数据加工机器”。

## 二、自定义层的两种常见情况（附代码和效果）

自定义层主要分两类：**不带参数的层**（机器不需要调整 “旋钮”，固定执行一个操作）和**带参数的层**（机器有可调节的 “旋钮”，参数会通过训练不断优化）。

我们以最常用的 PyTorch 为例（其他框架逻辑类似），逐个拆解。

### 情况 1：不带参数的层（固定操作，无 “旋钮”）

这类层的核心特点是：**输入数据后，执行一个固定的计算，不需要调整任何参数**。

比如开头提到的 “让数据减去自身平均值”—— 不管输入什么数据，都先算它的平均值，再用原数据减去这个平均值。

#### 步骤 1：怎么 “造” 这个层？（代码拆解）

我们要造一个叫CenteredLayer（“中心化层”）的机器，逻辑很简单：

1. 继承框架的 “基础层类”（比如 PyTorch 的nn.Module）—— 相当于拿到 “造机器的基础模板”；

2. 在__init__里 “搭架子”—— 因为没有参数要调，所以这里只需要 “继承模板” 就行；

3. 在forward里写 “核心操作”—— 定义数据怎么加工（输入 X，返回 X - X 的平均值）。

代码如下（每一行都标了通俗解释）：

```python
# 1. 导入PyTorch的基础工具

import torch

from torch import nn # nn里有各种“基础层模板”

# 2. 自定义“中心化层”：继承基础层模板nn.Module

class CenteredLayer(nn.Module):

# 3. 搭架子：__init__是“构造函数”，初始化层的基本信息

def __init__(self):

super().__init__() # 必须写：把基础模板的功能“继承”过来

# 因为没有参数要调，这里啥都不用加！

# 4. 写核心操作：forward是“前向传播”，定义数据怎么加工

def forward(self, X): # X是输入的数据

return X - X.mean() # 核心逻辑：输入 - 输入的平均值
```

#### 步骤 2：怎么用这个层？（实际效果）

造好机器后，就可以像用 “现成层” 一样用它了：

```python
# 1. 实例化这个层（相当于“造出一台中心化机器”）

my_layer = CenteredLayer()

# 2. 给机器喂数据（比如输入[1,2,3,4,5]）

input_data = torch.FloatTensor([1, 2, 3, 4, 5]) # 把列表转成PyTorch能处理的“张量”

output_data = my_layer(input_data) # 数据经过机器加工

# 3. 看结果

print("输入数据：", input_data) # 输入数据： tensor([1., 2., 3., 4., 5.])

print("输出数据：", output_data) # 输出数据： tensor([-2., -1., 0., 1., 2.])

print("输出的平均值：", output_data.mean()) # 输出的平均值： tensor(7.4506e-09)（接近0，因为浮点数精度问题）
```

效果很明显：输入数据的平均值是 3，输出数据就是每个数减 3，最终输出的平均值几乎为 0—— 完全符合我们的需求。

#### 进阶：把自定义层放进复杂模型

更实用的场景是：把这个 “中心化层” 和其他现成层组合，比如搭一个 “全连接层 + 中心化层” 的模型：

```python
# 搭一个模型：先过全连接层（把8维数据转成128维），再过自定义的中心化层

model = nn.Sequential(

nn.Linear(8, 128), # 现成的全连接层：输入8维，输出128维

CenteredLayer() # 我们自定义的中心化层

)

# 测试模型：输入4个“8维数据”

test_data = torch.rand(4, 8) # 随机生成4行8列的数据

model_output = model(test_data)

# 看结果：模型输出的平均值几乎为0

print("模型输出的平均值：", model_output.mean()) # 结果接近0，比如 tensor(7.4506e-09)
```

这说明：自定义层可以和现成层无缝配合，融入任何复杂模型。

### 情况 2：带参数的层（可调节 “旋钮”，参数会训练优化）

这类层是更常用的场景 —— 层里有 “可调节的参数”（相当于机器的 “旋钮”），这些参数会在训练中不断调整，让模型效果越来越好。

最典型的例子是 “全连接层”（比如把 5 维数据转成 3 维），它有两个核心参数：

- 权重（weight）：比如 5 行 3 列的矩阵，负责 “放大 / 缩小数据维度”；

- 偏置（bias）：比如 3 个数值，负责 “微调数据整体偏移”。

我们就自己造一个 “自定义全连接层”，理解参数怎么定义、怎么用。

#### 步骤 1：怎么 “造” 这个层？（代码拆解）

我们要造一个叫MyLinear（“自定义全连接层”）的机器，逻辑是：输入数据 → 权重相乘 + 偏置 → ReLU 激活（把负数变成 0）。

1. 继承基础层模板nn.Module；

2. 在__init__里定义 “可训练的参数”（权重和偏置）；

3. 在forward里写计算逻辑（数据 × 权重 + 偏置 → ReLU）。

代码如下（重点看参数怎么定义）：

```python
import torch

import torch.nn.functional as F # 里有ReLU等激活函数

from torch import nn

class MyLinear(nn.Module):

# 初始化时指定：输入维度（in_units）和输出维度（units）

def __init__(self, in_units, units):

super().__init__()

# 1. 定义“权重参数”：用nn.Parameter包装，告诉框架“这是要训练的参数”

# 形状是（输入维度，输出维度），比如输入5维、输出3维，就是5×3的矩阵

self.weight = nn.Parameter(torch.randn(in_units, units)) # randn：随机初始化（符合正态分布）

# 2. 定义“偏置参数”：同样用nn.Parameter包装

# 形状是（输出维度，），比如输出3维，就是3个数值

self.bias = nn.Parameter(torch.randn(units,)) # 逗号不能少，代表“1维张量”

# 前向传播：计算逻辑

def forward(self, X):

# 第一步：数据 × 权重 + 偏置（全连接层的核心计算）

linear_result = torch.matmul(X, self.weight) + self.bias # matmul：矩阵乘法

# 第二步：ReLU激活（把负数变成0，增加非线性）

final_result = F.relu(linear_result)

return final_result
```

这里有个关键：**用****nn.Parameter****包装参数**。这一步是告诉框架：“这些参数是要通过训练优化的”，框架会自动计算它们的梯度、更新它们的值 —— 我们不用自己管！

#### 步骤 2：怎么用这个层？（实际效果）

先看看参数的样子，再测试层的计算：

```python
# 1. 实例化自定义全连接层：输入5维，输出3维

my_linear = MyLinear(in_units=5, units=3)

# 2. 查看层的参数（刚初始化时是随机值）

print("权重参数（5×3矩阵）：")

print(my_linear.weight) # 输出一个5行3列的随机矩阵

print("偏置参数（3个数值）：")

print(my_linear.bias) # 输出3个随机数值

# 3. 测试层的计算：输入2个“5维数据”

input_data = torch.rand(2, 5) # 2行5列的数据（2个样本，每个样本5维）

output_data = my_linear(input_data)

# 4. 看结果：输出是2行3列（2个样本，每个样本3维），且没有负数（因为ReLU）

print("输入数据形状：", input_data.shape) # 输入数据形状： torch.Size([2, 5])

print("输出数据形状：", output_data.shape) # 输出数据形状： torch.Size([2, 3])

print("输出数据（无负数）：")

print(output_data) # 比如 tensor([[0.0000, 0.5678, 0.0000], [0.1234, 0.0000, 0.7890]])
```

#### 进阶：训练时参数会自动优化

如果把这个层放进模型里训练，参数会自动更新。比如我们用一个简单的任务（预测随机标签）来模拟训练：

```python
# 1. 搭模型：自定义全连接层（5→3） + 自定义全连接层（3→1）

model = nn.Sequential(

MyLinear(5, 3), # 第一个自定义全连接层

MyLinear(3, 1) # 第二个自定义全连接层（输出1维，比如预测一个数值）

)

# 2. 定义损失函数和优化器（告诉框架“怎么优化参数”）

loss_fn = nn.MSELoss() # 均方误差损失（适合回归任务）

optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # SGD优化器，学习率0.01

# 3. 模拟训练（100次迭代）

for i in range(100):

# 生成随机输入和标签（模拟真实数据）

	input_data = torch.rand(10, 5) # 10个样本，每个5维

	target = torch.rand(10, 1) # 10个标签，每个1维（要预测的值）

	# 前向传播：模型预测

	pred = model(input_data)

	# 计算损失（预测值和真实值的差距）

	loss = loss_fn(pred, target)
	
	# 反向传播+更新参数（框架自动做）

	optimizer.zero_grad() # 清空上一轮的梯度

	loss.backward() # 计算参数的梯度

	optimizer.step() # 根据梯度更新参数

# 每10次迭代看一下损失（应该越来越小）

if (i+1) % 10 == 0:

print(f"第{i+1}次迭代，损失：{loss.item():.4f}")
```

运行后会发现：损失值逐渐变小 —— 这说明模型里的 “权重” 和 “偏置” 参数在不断优化，越来越适合这个任务。

## 三、核心总结：自定义层的本质和关键

看到这里，你已经掌握了自定义层的核心逻辑，我们用 3 句话总结：

1. **本质是 “自定义数据加工规则”**：不管带不带参数，自定义层的核心都是在forward函数里写 “输入数据怎么变成输出数据”；

2. **参数要靠****nn.Parameter****包装**：如果层有要训练的参数，必须用nn.Parameter包装 —— 这样框架才会自动优化它们；

3. **和现成层完全兼容**：自定义层可以像搭积木一样，和框架提供的现成层（如卷积层、Dropout 层）组合，融入任何模型。

## 最后：试试小练习（巩固理解）

1. 设计一个 “平方和层”：输入两个数 x1、x2，输出 x1² + x2²（不带参数）；

2. 设计一个 “自定义缩放层”：输入数据 X，输出 W×X + b（W 和 b 是可训练参数，带参数）。

试着写一下代码，运行后看看效果 —— 你会发现，自定义层其实一点都不难！