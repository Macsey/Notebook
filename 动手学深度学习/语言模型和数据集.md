咱们接着把这个 “机器学语言的教程” 拆得更细，就像教小朋友说话一样，一步一步讲清楚每个逻辑，全程用生活里的例子帮你理解。

### 一、先搞透 “语言模型”：机器的 “说话说明书”

咱们先想个场景：你跟朋友聊天，朋友说 “我中午要去”，你大概能猜到后面是 “吃饭”“逛街”“回家”，不会是 “飞上天”—— 这是因为你懂中文的说话规律。机器的 “语言模型”，本质就是让机器学会这种规律，具体要干两件事：

1. **判断 “像不像人话”**

比如机器听到两句英文：“to recognize speech”（识别语音）和 “to wreck a nice beach”（毁了一个漂亮的海滩），发音很像，但后者语义奇怪。语言模型会算出来：前一句是 “人话” 的概率有 90%，后一句只有 1%，所以语音识别时会选前一句 —— 这就是它在帮机器 “避坑”。

2. **猜下一个词**

就像你玩 “成语接龙”，给定 “一心一意”，你会接 “意气风发”。机器学完规律后，给它 “我喜欢”，它能算出 “看书” 的概率是 30%、“运动” 是 25%、“睡觉” 是 20%，然后挑概率高的词接下去，实现 “自动续写”。

### 二、早期教机器的 “笨办法”：靠 “数次数” 学规律

一开始科学家想：既然人是靠多听多说学语言，那让机器 “数词出现的次数” 不就行了？比如找 1000 篇文章，让机器统计每个词、每对词出现的次数，再算概率。具体怎么算？举两个例子：

#### 1. 算单个词的概率：比如 “deep” 出现的概率

假设 1000 篇文章总共有 100 万个词，其中 “deep” 出现了 2000 次，那机器就认为：下次遇到文本，出现 “deep” 的概率是 2000÷1000000 = 0.2%。

#### 2. 算 “前一个词给定，后一个词” 的概率：比如 “deep” 后面跟 “learning” 的概率

统计发现 “deep” 总共出现 2000 次，其中有 1500 次后面跟着 “learning”，那 “deep→learning” 的概率就是 1500÷2000 = 75%。这样机器看到 “deep”，就有 75% 的把握接 “learning”。

#### 但这个 “笨办法” 有 3 个大漏洞，根本行不通

- **漏洞 1：没见过的组合，概率直接算成 0**

比如 “deep learning is fun” 这句话，要是 1000 篇文章里从没出现过，机器会觉得这句话是 “人话” 的概率是 0—— 可明明这句话很正常啊！就像你没吃过 “芒果蘸酱油”，不能说它不是食物吧？

- **漏洞 2：要记的东西太多，机器 “内存爆炸”**

如果词表有 10 万个不同的词，要记 “两个词组合” 的次数，就得记 10 万 ×10 万 = 1 万亿种组合；记 “三个词组合”，就得记 100 万亿种 —— 再大的电脑内存也装不下。

- **漏洞 3：机器不懂 “近义词”，太死板**

机器不知道 “猫” 和 “猫科动物” 意思相近，比如统计到 “猫喜欢吃鱼” 出现 100 次，却没见过 “猫科动物喜欢吃鱼”，就会觉得后者概率是 0，可明明这两句话意思差不多啊！

#### 后来加了个 “补丁”：拉普拉斯平滑，但治标不治本

为了避免 “没见过的组合概率为 0”，科学家想了个办法：给所有没见过的组合 “加个极小的次数”。比如没见过 “deep learning is fun”，就假装它出现过 1 次；没见过的 “两个词组合”，都假装出现过 0.1 次。这样概率就不会是 0 了，但还是解决不了 “内存爆炸” 和 “不懂近义词” 的问题 —— 相当于给破了的裤子打补丁，看着能穿，其实还是不舒服。

### 三、简化版方案：n 元语法，让机器 “少记点东西”

既然记 “所有前面的词” 太麻烦，那不如让机器 “只记最近的几个词”—— 这就是 “n 元语法” 的核心思路。“n” 代表 “要记的前面的词的个数”，主要分三种：

|   |   |   |   |   |
|---|---|---|---|---|
|类型|核心逻辑：只记前几个词|举例子：算 “我吃饭” 的概率|优点|缺点|
|一元语法|不记前面的词，每个词独立|概率 = “我” 的概率 × “吃” 的概率 × “饭” 的概率|记的东西最少，计算快|完全不符合实际（比如 “我→吃→饭” 是合理的，“饭→吃→我” 是不合理的，但一元语法算出来概率一样）|
|二元语法|只记前 1 个词|概率 = “我” 的概率 × “吃” 的概率（给定 “我”） × “饭” 的概率（给定 “吃”）|比一元语法合理，记的东西也不多|还是不够准（比如 “我吃” 后面接 “饭” 合理，接 “石头” 不合理，但如果没见过 “我吃→石头”，还是可能算错）|
|三元语法|只记前 2 个词|概率 = “我” 的概率 × “吃” 的概率（给定 “我”） × “饭” 的概率（给定 “我吃”）|比二元语法更准|要记的组合又变多了，比如 10 万个词，要记 10 万 ×10 万 ×10 万 = 1000 万亿种三元组合，内存又要爆炸|

简单说：n 越小，机器越 “轻松” 但越 “笨”；n 越大，机器越 “准” 但越 “累”—— 根本没有完美的平衡，所以这个方案还是不行。

### 四、用《时光机器》的数据做实验：发现语言的 “隐藏规律”

科学家拿《时光机器》这本书的文本当 “实验材料”，统计了很多数据，发现了 3 个关键现象，这些现象直接证明 “靠数次数学语言” 根本行不通：

#### 现象 1：最常出现的词都是 “没用的废话词”，但还不能丢

统计发现《时光机器》里，出现次数最多的 10 个词是 “the”“i”“and”“of” 这些（比如 “the” 出现了 2261 次，“i” 出现 1267 次）—— 这些词叫 “停用词”，没什么实际语义，就像中文里的 “的”“了”“是”。但机器不能丢这些词，比如丢了 “the”，“the time machine” 就变成 “time machine”，语法就错了。

#### 现象 2：词的频率掉得飞快，“少数词霸占大部分出现次数”

第 1 名 “the” 出现 2261 次，第 10 名 “my” 只出现 440 次（不到第 1 名的 1/5），第 100 名的词可能只出现几十次，第 1000 名的词可能只出现几次 —— 就像班里考试，少数人考 90 分以上，大部分人考 60-80 分，还有很多人不及格。这意味着 “靠数次数” 的话，大部分词的统计次数都很少，算出来的概率根本不准。

#### 现象 3：所有词、词组合都符合 “齐普夫定律”，少见的组合太多了

把 “词的排名” 和 “出现次数” 都换成对数（比如排名 1→log1=0，排名 10→log10=1；次数 2261→log2261≈3.35，次数 440→log440≈2.64），画在图上会形成一条直线 —— 这就是 “齐普夫定律”。这个定律说明：**排名越靠后的词，出现次数掉得越有规律，而且少见的词和组合占了绝大多数**。比如《时光机器》里，三元组合（三个词放一起）中，大部分组合只出现 1 次，靠 “数次数” 根本记不过来，更别说算概率了。

这三个现象合起来说明：靠 “数次数”“n 元语法” 学语言，根本走不通，必须换更聪明的方法 —— 这就是后面要学 “循环神经网络”“深度学习” 的原因。

### 五、给机器 “喂长文本” 的实用技巧：拆成小段再喂，两种拆法各有优劣

机器一次处理不了一整本《时光机器》（就像你一次吃不下一个蛋糕，得切成小块），所以要把长文本拆成 “固定长度的小段”（比如每段 5 个词），再一批批喂给机器。网页给了两种拆法，各有优缺点，适合不同场景：

#### 1. 第一种拆法：随机抽样 —— 追求 “多样性”，不管顺序

**具体怎么拆？** 比如把《时光机器》的文本拆成 “5 个词一段”，然后像洗牌一样把这些小段打乱，每次随机挑 2 段（假设 “批量大小 = 2”）组成一批喂机器。

举个例子：原本文本是 “0,1,2,3,4,5,6,7,8,9,10,11,12,13,14”（数字代表词），拆成 5 词一段是 “[0,1,2,3,4]”“[5,6,7,8,9]”“[10,11,12,13,14]”，打乱后可能挑 “[5,6,7,8,9]” 和 “[0,1,2,3,4]” 组成一批。

**优点**：机器能看到各种不同的小段组合，学的东西更 “杂”，不容易 “学偏”（比如不会只记住开头的文本，忽略后面的）。

**缺点**：打乱后，小段之间的 “上下文关系” 没了 —— 比如 “[0,1,2,3,4]” 后面本来跟着 “[5,6,7,8,9]”，打乱后可能和 “[10,11,12,13,14]” 放一起，机器不知道它们原本的顺序，学不到 “连贯的语义”。

#### 2. 第二种拆法：顺序分区 —— 追求 “连贯性”，不打乱顺序

**具体怎么拆？** 还是拆成 5 词一段，但不打乱，按原文顺序一批批挑。比如文本 “0-14” 拆成 3 段，批量大小 = 2 的话，第一批挑 “[0,1,2,3,4]” 和 “[5,6,7,8,9]”，第二批挑 “[10,11,12,13,14]”（不够 2 段就补全或舍弃）。

**优点**：保留了小段之间的 “上下文关系”，机器知道 “[0-4]” 后面跟着 “[5-9]”，能学出更连贯的语义 —— 比如学小说时，能知道 “上一段讲主角出门，下一段讲主角到了公园”。

**缺点**：机器看到的组合太 “单一”，容易 “学偏”—— 比如只学了开头的文本规律，后面的没学到，或者被某一段的特殊规律带歪（比如某一段全是对话，机器就以为所有文本都是对话）。

#### 最后打包成 “工具”：方便后续使用

为了不用每次拆文本都写一遍代码，网页把这两种拆法做成了一个 “工具类”（叫 SeqDataLoader），还写了个 “加载数据的函数”（叫 load_data_time_machine）。后面学深度学习时，直接调用这个函数，就能拿到拆好的文本小段和词表，不用再自己写拆文本的代码了 —— 相当于提前把 “切蛋糕的工具” 准备好，要用的时候直接拿。

### 六、总结：这页教程的 “核心逻辑链”

咱们把前面的内容串起来，其实就是一条很清晰的 “问题→尝试解决→发现不行→再找新方法” 的逻辑链：

1. **目标**：让机器学会语言规律（语言模型）；

2. **第一次尝试**：靠 “数次数” 算概率，发现有 3 个漏洞（概率为 0、内存爆炸、不懂近义词）；

3. **第二次尝试**：用 “n 元语法” 简化，发现 n 小了太笨、n 大了内存不够，还是不行；

4. **用数据验证**：拿《时光机器》统计，发现词频符合齐普夫定律，少见组合太多，进一步证明前面的方法行不通；

5. **解决 “喂数据” 的问题**：把长文本拆成小段，给两种拆法（随机抽样、顺序分区），并打包成工具；

6. **引出后续内容**：既然传统方法都不行，那只能用 “深度学习”（比如循环神经网络）来解决 —— 这就是下一节要讲的内容。

简单说，这页教程就像 “铺路石”：先告诉你 “过去的路走不通”，再帮你把 “走新路需要的工具（拆文本的方法）” 准备好，为后面学 “循环神经网络” 打基础。
# 《8.3. 语言模型和数据集》网页总结


## 一、章节定位与核心目标

本章节属于《动手学深度学习 2.0.0》中 “8. 循环神经网络” 的第三小节，承接 8.2 节文本预处理内容，核心目标是介绍语言模型的基础原理、自然语言的统计规律，以及长序列数据的读取方法，为后续循环神经网络在语言建模中的应用奠定数据与理论基础。

## 二、语言模型核心概念与学习方法

### 1. 语言模型定义

- 目标：估计文本序列的联合概率 \( P(x_1, x_2, \dots, x_T) \)，其中 \( x_t \) 是文本序列在时间步 \( t \) 处的词元（如单词、字符）。

- 应用价值：可生成自然文本（如对话续写），解决语音识别歧义（如区分 “to recognize speech” 与 “to wreck a nice beach”）、辅助文档摘要等任务。

### 2. 概率计算逻辑

根据概率链式法则，序列联合概率可分解为条件概率乘积：$$$\( P(x_1, x_2, \dots, x_T) = \prod_{t=1}^T P(x_t \mid x_1, \dots, x_{t-1}) \)$$

示例：“deep learning is fun” 的概率 = $$\( P(\text{deep}) \times P(\text{learning} \mid \text{deep}) \times P(\text{is} \mid \text{deep, learning}) \times P(\text{fun} \mid \text{deep, learning, is}) \)$$

### 3. 传统参数估计与问题

- 基础方法：通过语料库中词的 “相对词频” 估计概率，如 $$\( \hat{P}(\text{learning} \mid \text{deep}) = \frac{n(\text{deep, learning})}{n(\text{deep})} \)（\( n(x) \) $$为词 / 词对出现次数）。

- 核心问题：
- 

1. 低频词组合稀疏：多词组合（如三元、四元）在语料中可能从未出现，导致概率估计为 0。

2. 存储成本高：需存储所有词、词对、多词组合的计数。

3. 忽略语义关联：无法利用 “猫” 与 “猫科动物” 等语义相关词的上下文共性。

- 改进策略：拉普拉斯平滑，通过在计数中添加小常量（如 \( \epsilon_1, \epsilon_2 \)）避免零概率，示例：\( \hat{P}(x) = \frac{n(x) + \epsilon_1 / m}{n + \epsilon_1} \)（\( m \) 为唯一单词数，\( n \) 为总单词数）。

## 三、马尔可夫模型与 n 元语法

### 1. 马尔可夫性质应用

通过 “截断序列依赖关系” 简化条件概率计算，常见假设：

- 一阶马尔可夫：\( P(x_{t+1} \mid x_t, \dots, x_1) = P(x_{t+1} \mid x_t) \)

- 高阶马尔可夫：依赖前 \( k \) 个词元，对应不同 “n 元语法” 模型。

### 2. n 元语法分类

|   |   |   |
|---|---|---|
|模型类型|核心假设|概率计算示例（序列 “x1,x2,x3,x4”）|
|一元语法（unigram）|词元独立|\( P(x1)P(x2)P(x3)P(x4) \)|
|二元语法（bigram）|依赖前 1 个词元|\( P(x1)P(x2 \mid x1)P(x3 \mid x2)P(x4 \mid x3) \)|
|三元语法（trigram）|依赖前 2 个词元|\( P(x1)P(x2 \mid x1)P(x3 \mid x1,x2)P(x4 \mid x2,x3) \)|

## 四、自然语言统计规律（基于《时光机器》数据集）

### 1. 词频基本特征

- 停用词占比高：前 10 个高频词均为停用词（如 “the” 出现 2261 次、“i” 1267 次、“and” 1245 次），虽语义简单但模型需保留。

- 词频衰减快：第 10 个高频词的频率不足第 1 个的 1/5。

### 2. 齐普夫定律（Zipf’s Law）

- 规律：第 \( i \) 个最常用词的频率 \( n_i \propto \frac{1}{i^\alpha} \)，双对数坐标下呈直线（\( \log n_i = -\alpha \log i + c \)，\( \alpha \) 为指数，\( c \) 为常数）。

- 扩展：二元语法、三元语法的词元组合也遵循齐普夫定律，但 \( \alpha \) 更小（受序列长度影响）。

### 3. 多语法频率对比

- 三元语法低频组合更多：如 “the time traveller” 仅出现 59 次，大量合理三元组合在语料中稀疏，进一步说明传统统计方法的局限性，需深度学习模型改进。

## 五、长序列数据读取方法

由于模型无法一次性处理超长序列（如整本《时光机器》），需将序列拆分为固定长度（\( \text{num_steps} \)）的子序列，核心两种策略：

### 1. 随机采样（seq_data_iter_random）

- 逻辑：从随机偏移量开始划分序列，打乱子序列起始索引，相邻小批量的子序列在原始序列中不保证相邻。

- 输入参数：

- corpus：原始词元序列

- batch_size：每个小批量的子序列数量

- num_steps：每个子序列的时间步数（长度）

- 优势：随机性强，覆盖更多子序列组合；劣势：破坏序列连续性。

### 2. 顺序分区（seq_data_iter_sequential）

- 逻辑：从随机偏移量开始，按顺序拆分序列，相邻小批量的子序列在原始序列中保持相邻。

- 优势：保留序列连续性，适合对上下文依赖敏感的场景；劣势：随机性较弱。

### 3. 数据迭代器封装

- 类 SeqDataLoader：整合两种采样方法，根据 use_random_iter 选择策略，同时加载语料库与词表。

- 函数 load_data_time_machine：简化调用，返回数据迭代器与词表，与其他数据集加载函数（如 load_data_fashion_mnist）接口一致。

## 六、章节小结

1. 语言模型是自然语言处理的核心，需估计序列联合概率，传统方法依赖词频统计但受限于稀疏性。

2. n 元语法通过截断依赖简化计算，但无法解决低频组合问题。

3. 齐普夫定律揭示自然语言的词频分布规律，适用于单语法与多语法。

4. 长序列读取的两种核心策略：随机采样（高随机性）、顺序分区（保连续性），需根据任务选择。

## 七、练习要点（引导深入思考）

1. 四元语法存储量计算：需存储 10 万个一元词频、\( 10^5 \times 10^5 = 10^{10} \) 个二元频率、\( 10^{15} \) 个三元频率、\( 10^{20} \) 个四元频率，凸显存储瓶颈。

2. 对话建模挑战：需考虑多轮上下文、 speaker 身份、对话逻辑，不能仅依赖单序列建模。

3. 齐普夫定律指数估计：可通过双对数坐标拟合直线斜率计算 \( \alpha \)。

4. 其他读取方法：按句子边界拆分（保证句子完整性）、滑动窗口（步长小于 num_steps）。

5. 随机偏移量优化：解决固定偏移导致的子序列覆盖不全问题，但无法完全均匀分布，可通过多次随机偏移叠加改进。

6. 完整句子抽样问题：句子长度不一导致子序列长度波动，需 padding 或截断处理。