

### 神经网络的 “对称性”：为什么要打破，以及怎么打破

“对称性” 是神经网络的 “天生属性”，指的是 “多个参数在功能上完全重复，导致网络无法发挥全部能力”，就像一个团队里所有人都做同样的事，效率反而低下。

#### 1. 对称性的具体表现：“参数重复→功能冗余”

以 “1 个隐藏层（2 个隐藏单元）的多层感知机” 为例：

- 隐藏层有两个权重 w1 和 w2，输出层有两个权重 v1 和 v2；

- 如果初始时 w1=w2、v1=v2，那么前向传播时：

隐藏单元 1 输出 = sigmoid (w1× 输入 + b1)

隐藏单元 2 输出 = sigmoid (w2× 输入 + b2) = 隐藏单元 1 输出（因为 w1=w2、b1=b2）

最终输出 = v1× 隐藏单元 1 输出 + v2× 隐藏单元 2 输出 = (v1+v2)× 隐藏单元 1 输出

- 这意味着：两个隐藏单元实际只起到了 “一个单元” 的作用，网络的 “表达能力” 被硬生生砍半 —— 哪怕你设计了 100 个隐藏单元，只要参数初始相同，最终也只能当 1 个用。

#### 2. 对称性的危害：限制网络学习能力

- **无法学习多样特征**：比如识别猫的图片时，一个隐藏单元负责学习 “耳朵特征”，另一个负责学习 “眼睛特征”。若参数对称，两个单元都会学 “耳朵特征”，永远学不到 “眼睛特征”；

- **优化陷入局部最优**：对称参数会让梯度也对称（反向传播时 w1 和 w2 的梯度相同），导致参数更新后仍然保持对称，永远无法跳出 “功能冗余” 的状态。

#### 3. 打破对称性的核心方法：“随机初始化”+“正则化”

- **随机初始化参数**：这是最基础的方法。比如用 “均值为 0、方差为 0.01 的正态分布” 初始化权重，让 w1 和 w2 的初始值不同（比如 w1=0.02，w2=-0.01）。这样前向传播时，两个隐藏单元的输出就会有差异，反向传播的梯度也会不同，参数更新后会越来越 “不一样”，逐渐形成各自的功能；

- **暂退法（Dropout）**：训练时随机 “关闭” 一部分隐藏单元（比如 50% 的概率让隐藏单元输出为 0）。即使参数有轻微对称，被关闭的单元不同，也会导致剩余单元的功能被迫 “差异化”，间接打破对称；

- **不同的初始化策略**：比如对不同层用不同的方差初始化（如 Xavier 初始化），确保每层参数的 “初始差异度”，从源头避免对称。