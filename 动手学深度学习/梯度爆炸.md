

### 梯度爆炸的形成原因（从参数初始化到数值失控）

梯度爆炸与梯度消失是 “同一种逻辑的反向”—— 不是梯度被 “乘小”，而是被 “乘大”，最终数值超过计算机能表示的范围（比如 10^308 以上），导致参数更新 “失控”。

#### 1. 核心数学根源：“梯度连乘” 导致数值膨胀

和梯度消失类似，梯度爆炸也是 “梯度连乘” 的结果，但触发条件相反：

- 当 “激活函数导数 × 权重参数” 的乘积大于 1 时，每传递一层，梯度就会被放大一次；

- 若网络层数多，连乘后梯度会呈 “指数级增长”，最终突破数值上限（比如变成 10^100）。

#### 2. 最常见诱因：参数初始化过大

很多初学者会犯一个错误：把权重初始化为较大的数值（比如从均值为 0、方差为 1 的正态分布中采样）。这会导致 “前向传播的输入值过大”，进而引发梯度爆炸。

举个例子：

假设用正态分布（方差 1）初始化权重，隐藏层输入 = 权重 × 前一层输出。若前一层输出均值为 1，那么隐藏层输入的均值为 0，但方差为 “权重方差 × 前一层输出方差”=1×1=1，输入值可能达到 ±3（正态分布 99.7% 的数值在 ±3 范围内）。

如果用 ReLU 激活函数（输入为正时导数为 1），反向传播时梯度 = 后一层梯度 × 1 × 权重。若权重均值为 0 但方差为 1，那么梯度会被 “随机放大”—— 比如某层权重有 10 个元素，梯度可能被放大到原来的 10 倍，10 层后梯度就会放大到 10^10，远超计算机处理范围。

#### 3. 实际危害：参数 “越更越乱”

梯度爆炸时，参数更新幅度会异常大：比如原本合理的权重是 0.1，一次更新后可能变成 100，甚至 1000。这会导致：

- 前向传播的输出值 “跳变”：比如原本预测概率是 0.5，更新后变成 1000（远超概率的 0-1 范围）；

- 模型无法收敛：损失值像 “过山车” 一样，时而骤降时而骤升，永远找不到最优解；

- 数值溢出错误：计算机无法表示过大的数值，会直接返回 “NaN（不是数字）”，导致训练中断。

