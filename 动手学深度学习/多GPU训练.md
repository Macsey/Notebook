| 拆分方式       | 核心逻辑                          | 优势                         | 劣势                             | 适用场景                              |
| ---------- | ----------------------------- | -------------------------- | ------------------------------ | --------------------------------- |
| 网络拆分       | 每个 GPU 处理网络的不同层，数据按层传递        | 可训练更大网络，单个 GPU 显存占用低       | GPU 间同步频繁，数据传输量大（激活值、梯度），负载难匹配 | 框架 / 系统支持多 GPU 互联，且网络层结构适合拆分的特殊场景 |
| 层内拆分       | 拆分单一层的计算任务（如卷积层通道数、全连接层输出单元数） | 显存线性扩展，支持更大网络              | 需大量同步 / 屏障操作，数据传输量可能超过网络拆分     | GPU 显存极小（如早期 2GB 显存设备）的历史场景       |
| 数据拆分（数据并行） | 所有 GPU 执行相同网络逻辑，仅拆分小批量数据      | 实现最简单，同步仅在小批量训练后进行，可适配任意模型 | 无法训练更大模型（单个 GPU 需存完整参数）        | 主流场景，只要 GPU 显存足够容纳单模型参数           |
### 训练迭代步骤（以`k`个 GPU 为例）

1. **数据拆分**：将当前小批量数据均匀分成`k`份，分配给每个 GPU；
2. **局部计算**：每个 GPU 用分配到的数据计算损失和模型参数的局部梯度；
3. **梯度聚合**：汇总`k`个 GPU 的局部梯度，得到当前小批量的全局随机梯度；
4. **梯度分发**：将聚合后的全局梯度广播到所有 GPU；
5. **参数更新**：每个 GPU 用全局梯度更新自身维护的模型参数，确保所有 GPU 参数同步。

### 关键注意事项

- **批量大小调整**：多 GPU 训练时，总批量大小需设为`k×单GPU批量大小`，保证每个 GPU 工作量均匀；
- **学习率适配**：批量增大后需适当提高学习率，确保训练收敛；
- **批量规范化调整**：每个 GPU 需保留独立的批量规范化参数，避免跨设备统计偏差。




**分布式训练（Distributed Training）**策略分类。

为了让你彻底理解，我们不仅要看定义，还要看它们是为了解决什么问题而诞生的。通常，我们需要多张 GPU 只有两个原因：

1. **算得慢**：数据太多，一张卡算不过来（需要**速度**）。
    
2. **装不下**：模型太大，一张卡的显存爆了（需要**容量**）。
    

### 1. 数据拆分（Data Parallelism）—— 最主流的方案

这是目前工业界（包括你如果自己跑代码）90% 情况下默认使用的模式。

- 形象比喻： “影分身之术” 或 “多个阅卷老师”。
    
    假设有一堆试卷（数据）要改。只有一个老师改太慢了。于是，如果你有 4 张 GPU，你就复制出 4 个一模一样的老师。每个老师领走 $\frac{1}{4}$ 的试卷，各自在自己的房间里改。改完后，大家聚在一起开个会，把评分标准统一一下。
    
- **技术细节：**
    
    1. **模型复制：** 每一张 GPU 上都保存一份**完整**的模型参数（Weights）。
        
    2. **数据切分：** 假设一个 Batch Size 是 128，你有 4 张卡。系统会把这个 Batch 切成 4 份，每张卡拿 32 个数据。
        
    3. **独立计算：** 每张卡分别进行前向传播（算 Loss）和反向传播（算梯度 Gradients）。
        
    4. **关键步骤（All-Reduce）：** 在更新参数之前，所有显卡必须通信，把各自算出来的梯度加起来求平均。
        
    5. **同步更新：** 拿到平均梯度后，所有卡同时更新参数。保证下一轮开始时，大家手里的模型还是一模一样的。
        
- **核心优势：**
    
    - **简单：** 代码改动极小（PyTorch 里也就是裹一层 `DistributedDataParallel`）。
        
    - **高效：** 计算时互不打扰，只有最后一步需要通信。
        
- **致命弱点：**
    
    - **显存墙：** 每张卡都必须装得下**整个模型**。如果你的模型是 GPT-3（1750 亿参数），单卡显存根本装不下，这种方法就直接失效了。
        


### 2. 网络拆分（Network Splitting / Pipeline Parallelism）—— 解决“装不下”的问题

当模型大到单卡装不下时，我们必须把模型切开。网络拆分是按**深度（层数）**切。

- 形象比喻： “工厂流水线”。
    
    造一辆车很复杂。GPU 1 负责装底盘（第 1-10 层），GPU 2 负责装发动机（第 11-20 层），GPU 3 负责喷漆（第 21-30 层）。
    
- **技术细节：**
    
    - **模型切分：** 不同的 GPU 显存里存的是模型**不同层**的参数。
        
    - **数据流向：** 数据 Batch 先进入 GPU 1，算完后，把中间结果（Activations）传给 GPU 2，GPU 2 接着算... 直到最后 GPU 出结果。
        
    - **通信内容：** 主要是层与层之间的**激活值（前向）**和**梯度（反向）**。
        
- 核心痛点：气泡（Bubble）
    
    这是流水线最大的问题。当 GPU 1 在处理数据时，GPU 2、3、4 都在干等（Idle）。这会导致大量的计算资源浪费。
    
    - _优化方案：_ **流水线并行（Pipeline Parallelism）**。把一个大 Batch 再切成无数个 Micro-batches，让流水线尽可能填满，减少空转时间。
        
- 适用场景：
    
    超大模型（如 Transformer 架构的大语言模型），层数极深，单卡存不下。
    


### 3. 层内拆分（Layer Splitting / Tensor Parallelism）—— 暴力美学

这是三种方式里最硬核、实现最复杂的，但对于现代大模型（LLM）至关重要。你表格里提到的“早期 2GB 显存”是历史背景，但在今天，**它是训练 ChatGPT 等巨型模型的基石**。

- 形象比喻： “两人抬钢琴”。
    
    网络拆分是“我做完了给你做”，层内拆分是“这层太重了，我们一起抬”。
    
    不仅模型切开了，连矩阵乘法这个动作都被切开了。
    
- 技术细节：
    
    假设某一层是一个巨大的矩阵乘法 $Y = X \times W$。
    
    - **切分权重 $W$：** 我们把大矩阵 $W$ 竖着切成两半 $[W_1, W_2]$。
        
    - **并行计算：**
        
        - GPU 1 存 $W_1$，计算 $Y_1 = X \times W_1$
            
        - GPU 2 存 $W_2$，计算 $Y_2 = X \times W_2$
            
    - **结果拼接：** 最后把 $Y_1$ 和 $Y_2$ 拼起来得到 $Y$。
        
- 为什么需要它？
    
    对于像 GPT-3 这样宽度的网络，单层的矩阵巨大无比。
    
    - 如果用**数据并行**：显存不够。
        
    - 如果用**网络拆分**：由于中间结果太大，GPU 间传输由于太频繁会卡死。
        
    - **层内拆分**允许我们将巨大的单层运算分散到多张卡上，利用 NVLink 这种超高速互连技术，实现“看起来像一张超级显卡”的效果。
        
- **缺点：**
    
    - 通信极其频繁（每一层算完都要对齐数据），对显卡间的带宽（NVLink）要求极高。如果用普通的 PCIe 或以太网，速度会慢到无法接受。
        


### 总结与现代应用

为了方便记忆，我们可以把它们对应到**切蛋糕**的方式：

1. **数据拆分（Data Parallelism）：**
    
    - **怎么切：** 蛋糕（模型）不切，每个人发一个小蛋糕，最后大家把吃的感觉汇总。
        
    - **关键点：** 最常用，扩充算力。
        
2. **网络拆分（Pipeline Parallelism）：**
    
    - **怎么切：** 蛋糕横着切（按层）。我吃第一层奶油，你吃第二层海绵胚。
        
    - **关键点：** 解决显存不足，但有流水线等待时间。
        
3. **层内拆分（Tensor Parallelism）：**
    
    - **怎么切：** 蛋糕竖着切（切开每一层）。这一层的左半边我吃，右半边你吃。
        
    - **关键点：** 解决单层过大，通信要求极高。
        

#### 现实中的大模型训练

在训练像 GPT-4、Llama 3 这种万亿参数模型时，并不是三选一，而是**三种全都要**：

- **在机器内部（高带宽 NVLink）：** 使用 **层内拆分**，把大模型的一层摊平在 8 张卡上。
    
- **在机器之间（网络连接）：** 使用 **网络拆分**，把深层网络切成几段，分布在不同的机柜。
    
- **在整体副本上：** 使用 **数据并行**，复制多份上述的流水线，喂入海量数据。
    
