| 拆分方式       | 核心逻辑                          | 优势                         | 劣势                             | 适用场景                              |
| ---------- | ----------------------------- | -------------------------- | ------------------------------ | --------------------------------- |
| 网络拆分       | 每个 GPU 处理网络的不同层，数据按层传递        | 可训练更大网络，单个 GPU 显存占用低       | GPU 间同步频繁，数据传输量大（激活值、梯度），负载难匹配 | 框架 / 系统支持多 GPU 互联，且网络层结构适合拆分的特殊场景 |
| 层内拆分       | 拆分单一层的计算任务（如卷积层通道数、全连接层输出单元数） | 显存线性扩展，支持更大网络              | 需大量同步 / 屏障操作，数据传输量可能超过网络拆分     | GPU 显存极小（如早期 2GB 显存设备）的历史场景       |
| 数据拆分（数据并行） | 所有 GPU 执行相同网络逻辑，仅拆分小批量数据      | 实现最简单，同步仅在小批量训练后进行，可适配任意模型 | 无法训练更大模型（单个 GPU 需存完整参数）        | 主流场景，只要 GPU 显存足够容纳单模型参数           |
### 训练迭代步骤（以`k`个 GPU 为例）

1. **数据拆分**：将当前小批量数据均匀分成`k`份，分配给每个 GPU；
2. **局部计算**：每个 GPU 用分配到的数据计算损失和模型参数的局部梯度；
3. **梯度聚合**：汇总`k`个 GPU 的局部梯度，得到当前小批量的全局随机梯度；
4. **梯度分发**：将聚合后的全局梯度广播到所有 GPU；
5. **参数更新**：每个 GPU 用全局梯度更新自身维护的模型参数，确保所有 GPU 参数同步。

### 关键注意事项

- **批量大小调整**：多 GPU 训练时，总批量大小需设为`k×单GPU批量大小`，保证每个 GPU 工作量均匀；
- **学习率适配**：批量增大后需适当提高学习率，确保训练收敛；
- **批量规范化调整**：每个 GPU 需保留独立的批量规范化参数，避免跨设备统计偏差。