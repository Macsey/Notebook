[[门控循环网络]]
[[长短期记忆网络]]
[[深度循环神经网络]]
[[双向循环神经网络]]
[[机器翻译与数据集]]
[[编码器-解码器架构]]
[[序列到序列学习]]
[[束搜索]]




































## 9.1 门控循环单元 
### 1. 核心痛点：标准 RNN 的局限性
普通的循环神经网络（RNN）在处理长序列时面临两个主要挑战：
* **梯度异常**：易发生梯度消失（早期信息无法传导到后期）或梯度爆炸（权重更新不稳定）。
* **记忆缺乏灵活性**：难以在长距离上保留关键的早期信息（例如一句话开头的“不”，决定了句尾的情感），也难以在必要时跳过无关词元或重置状态。

### 2. GRU 的解决方案
GRU 是 LSTM 的一种流线型变体。它在保留了“门控”这一核心思想的前提下，减少了参数量，计算速度更快，是目前工业界非常流行的选择。

### 3. 核心机制：门控隐状态
GRU 放弃了 LSTM 中独立的“记忆元”，直接对“隐状态”进行精细控制。

* **双门设计**：
    * **重置门 (Reset Gate, $R_t$)**：*决定“过去”有多少需要被遗忘*。
        * **作用**：控制上一时间步的隐状态 $H_{t-1}$ 有多少信息能进入当前的候选状态。
        * **直观理解**：如果当前词是句号，重置门可能趋近于 0，表示“清空上下文，准备下一句”。
    * **更新门 (Update Gate, $Z_t$)**：*决定“新旧”信息的融合比例*。
        * **作用**：决定当前隐状态 $H_t$ 中，有多少保留自旧状态 $H_{t-1}$，有多少来自新计算的候选状态 $\tilde{H}_t$。
        * **直观理解**：类似于一个滑动变阻器，左边是“保持记忆”，右边是“写入新知”。

### 4. 数学与逻辑流程
所有门控均使用 **Sigmoid** 激活（输出 0~1），数据变换均使用 **Tanh** 激活（输出 -1~1）。

1.  **计算门控**：基于当前输入 $X_t$ 和上一隐状态 $H_{t-1}$ 生成 $R_t$ 和 $Z_t$。
2.  **生成候选隐状态 ($\tilde{H}_t$)**：
    $$
    \tilde{H}_t = \tanh(X_t W_{xh} + (R_t \odot H_{t-1}) W_{hh} + b_h)
    $$
    *注：这里 $R_t$ 就像一个过滤器，如果 $R_t \approx 0$，则旧状态被屏蔽，模型退化为只看当前输入的 MLP。*
3.  **最终隐状态更新**：
    $$
    H_t = Z_t \odot H_{t-1} + (1 - Z_t) \odot \tilde{H}_t
    $$
    *注：这是凸组合运算。若 $Z_t \approx 1$，模型完全忽略当前输入，直接复制旧状态（从而解决梯度消失，实现长距离依赖）。*

---

## 9.2 长短期记忆网络 (LSTM)

### 1. 设计思路
LSTM 旨在通过引入一个独立的**记忆元 (Memory Cell, $C_t$)** 来彻底解决长程依赖问题。与 GRU 不同，LSTM 明确区分了“记忆（内部状态）”和“输出（外部状态/隐状态）”。

### 2. 关键组件
LSTM 拥有三个门控和一个记忆通道，实现了对信息的精确读写：

| 组件       | 符号            | 激活函数     | 功能描述                                                                    |
| :------- | :------------ | :------- | :---------------------------------------------------------------------- |
| **遗忘门**  | $F_t$         | Sigmoid  | **“应该遗忘多少历史？”**<br>根据当前输入和上一隐状态，决定旧记忆 $C_{t-1}$ 中哪些部分是过时的（输出 0 代表完全遗忘）。 |
| **输入门**  | $I_t$         | Sigmoid  | **“应该写入多少新知？”**<br>判断当前的候选信息 $\tilde{C}_t$ 哪些是值得存入长期记忆的。                |
| **输出门**  | $O_t$         | Sigmoid  | **“应该对外展示什么？”**<br>控制长期记忆 $C_t$ 的哪些部分应该被激活并作为隐状态 $H_t$ 输出给下一层。          |
| **候选记忆** | $\tilde{C}_t$ | **Tanh** | **“当前时刻产生的新信息”**<br>基于当前输入计算出的原始信息流，值域在 [-1, 1]。                        |

### 3. 信息流转逻辑
1.  **记忆更新 (Cell State Update)**：
    $$
    C_t = F_t \odot C_{t-1} + I_t \odot \tilde{C}_t
    $$
    *解释：今天的记忆 = (昨天记忆 $\times$ 不遗忘比例) + (今天新知 $\times$ 重要程度)。*
2.  **隐状态输出 (Hidden State Output)**：
    $$
    H_t = O_t \odot \tanh(C_t)
    $$
    *解释：隐状态是经过筛选后的记忆展现。*

---

## 9.3 深层循环神经网络 (Deep RNN)
### 1. 为什么要加深？
浅层的 RNN/LSTM 只能捕捉简单的序列关系。通过堆叠多个循环层，模型能够学习数据的**层级表示**：
* **底层**：捕捉字面特征（如词性、局部短语）。
* **高层**：捕捉抽象语义（如句法结构、情感倾向、逻辑推理）。

### 2. 架构设计
* **结构**：包含 `num_layers` 个隐藏层。
* **信息传递路径**：
    1.  **时间维度（水平）**：第 $l$ 层在 $t$ 时刻的状态，传递给第 $l$ 层在 $t+1$ 时刻。
    2.  **深度维度（垂直）**：第 $l$ 层在 $t$ 时刻的输出（隐状态），作为第 $l+1$ 层在 $t$ 时刻的**输入**。

### 3. 公式表达
对于第 $l$ 层 ($l > 1$) 的隐状态 $H_t^{(l)}$：
$$
H_t^{(l)} = \phi(H_t^{(l-1)} W_{xh}^{(l)} + H_{t-1}^{(l)} W_{hh}^{(l)} + b_h^{(l)})
$$
* **输入来源**：下一层的当前输出 $H_t^{(l-1)}$ 和 本层的上一时刻状态 $H_{t-1}^{(l)}$。


## 9.4 双向循环神经网络 (Bi-RNN)

### 1. 本质
单向 RNN 的局限在于它只能“向后看”。但在很多任务中，理解一个词需要同时参考它前面的词和后面的词（例如完形填空、实体识别）。Bi-RNN 通过**并行**运行两个方向的 RNN 来解决这个问题。

### 2. 运行机制
* **前向层 (Forward)**：从序列 $1 \to T$ 运行，计算 $H_t^{\rightarrow}$，捕获上文信息。
* **反向层 (Backward)**：从序列 $T \to 1$ 运行，计算 $H_t^{\leftarrow}$，捕获下文信息。
* **特征融合**：在时间步 $t$，将两个方向的隐状态进行**拼接 (Concatenate)**。
    $$
    H_t = [H_t^{\rightarrow}, H_t^{\leftarrow}]
    $$

### 3. 适用性（关键！）

| 必须使用的场景 (需全篇理解)            | 绝对禁止的场景 (需预测未来)                        |
| :------------------------- | :------------------------------------- |
| **序列标注**：词性标注、命名实体识别 (NER) | **语言模型**：预测下一个词 (Next Word Prediction) |
| **文本分类**：情感分析、垃圾邮件检测       | **时间序列预测**：股票预测、天气预报 (未来数据不可知)         |
| **机器翻译 (编码阶段)**：理解源句子全貌    | **实时生成**：同声传译 (不能等待整句说完)               |
| **问答系统**：从文中寻找答案           |                                        |

---

## 9.6 编码器-解码器架构 (Encoder-Decoder)

### 1. 挑战
传统的 CNN 或 RNN 通常要求输入输出具有固定的对应关系。然而，在机器翻译、对话系统等任务中，**输入序列长度**和**输出序列长度**通常是不一致且不可预知的。

### 2. 架构
这是一个“压缩-解压”的过程：

* **编码器 (Encoder)**：
    * **职责**：阅读理解。将变长的输入序列（如一句中文）通过 RNN/LSTM 逐步处理。
    * **产出**：最终将整个句子的语义压缩为一个固定形状的**上下文变量 (Context Variable / Semantic Vector)**。这通常是编码器最后一个时间步的隐状态。

* **解码器 (Decoder)**：
    * **职责**：生成表达。以编码器的上下文变量为初始状态，逐个生成目标序列的词元（如英文单词）。
    * **依赖**：每一步生成都依赖于：1. 上下文变量；2. 上一步生成的词；3. 解码器自身的隐状态。

**常见应用**：机器翻译、文本摘要、语音转文字。

---

## 9.8 束搜索
**—— 序列生成的“中庸之道”**

### 1. 搜索策略的缺点
当解码器在每个时间步预测下一个词时，面临选择难题：
* **贪心搜索 ：每次只选概率最大的那个词。
    * *缺点*：目光短浅，容易陷入局部最优（例如，开头选了个很常用的词，导致后面无法通顺地接下去）。
* **穷举搜索**：计算所有可能的排列组合。
    * *缺点*：计算量是指数级爆炸，实际上不可行。

### 2. 束搜索机制
这是一种启发式搜索算法，在精度和计算量之间寻找平衡。
* **束宽 **：在每个时间步，不是只保留 1 个最优解，而是保留**前 $k$ 个**概率最高的候选序列。
* **过程**：在下一时间步，基于这 $k$ 个候选序列继续扩展，计算所有 $k \times |V|$ 个可能的新路径，然后再次保留概率最高的 $k$ 个。

### 3. 长度惩罚
* **问题**：由于长序列的概率是多个小于 1 的概率相乘，序列越长，数值越接近 0。模型天生倾向于生成极短的句子。
* **解决方案**：引入长度惩罚因子。
    $$
    \text{Score} = \frac{\log P(\text{seq})}{\text{length}^\alpha}
    $$
    通过除以长度的 $\alpha$ 次方（通常 $\alpha=0.75$ 左右），适当降低对长序列的惩罚，使长短句的评判更公平。
