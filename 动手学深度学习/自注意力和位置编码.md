
## 1. 章节概览

本章主要探讨如何在序列数据中利用注意力机制捕捉**内部依赖**以及如何处理**序列顺序**问题，是理解 Transformer 架构的基石。

- **核心问题**：
    
    1. 如何利用注意力机制捕捉序列内部的依赖关系？
        
    2. 如何在无循环（Recurrent）结构中注入序列的顺序信息？
        
- **对比视角**：分析 CNN、RNN 与自注意力在处理序列数据时的优劣。
    

## 2. 核心内容

### 2.1 自注意力 

#### **定义与原理**

自注意力是注意力机制的一种特殊形式，其核心特征在于 **Query (查询)、Key (键)、Value (值) 均来自同一个输入序列**。

**机制**：序列中的每个词元（Token）都会关注序列中的所有其他词元。
数学表达：
设输入序列为 $x_1, \dots, x_n$ (其中 $x_i \in \mathbb{R}^d$)，输出序列 $y_1, \dots, y_n$ 的计算如下：$$y_i = f(x_i, (x_1, x_1), \dots, (x_n, x_n)) \in \mathbb{R}^d$$$f$ 为注意力汇聚函数


### 2.2 对比：CNN vs RNN vs Self-Attention
![](assets/自注意力和位置编码/file-20251213183753710.png)

三种架构在处理序列数据时的性能指标对比如下：

| **架构**   | **计算复杂度**                | **并行度 (顺序操作)** | **最大路径长度 (远距离依赖)** | **核心特点**                               |
| -------- | ------------------------ | -------------- | ------------------ | -------------------------------------- |
| **CNN**  | $O(k \cdot n \cdot d^2)$ | $O(1)$ (高并行)   | $O(n/k)$           | 分层捕捉局部依赖，需多层堆叠才能覆盖长距离。($k$=卷积核大小)      |
| **RNN**  | $O(n \cdot d^2)$         | $O(n)$ (无并行)   | $O(n)$             | 逐词元处理，天然捕捉顺序，但并行差，易梯度消失/爆炸。            |
| **自注意力** | $O(n^2 \cdot d)$         | $O(1)$ (高并行)   | $O(1)$             | **捕捉长距离依赖能力最强**，直接连接所有词元。但序列过长时计算成本极高。 |

**总结**：自注意力在并行性和长距离依赖捕捉上最优，但在长序列（$n$ 很大）场景下计算压力最大。

### 2.3 位置编码 

#### **设计动机**

由于自注意力机制并行计算输入序列，它具有**排列不变性**（Permutation Invariant），即无法区分“我爱你”和“你爱我”中词元的先后顺序。因此，必须显式注入位置信息。
#### **实现：固定位置编码 (正弦/余弦函数)**
操作：构建一个与输入 $X$ 形状相同的矩阵 $P$，执行加法操作：
$$\text{Input} = X + P$$
公式：对于矩阵 $P$ 中的第 $i$ 行（位置），第 $j$ 列（维度），计算如下：
$$p_{i, 2j} = \sin\left(\frac{i}{10000^{2j/d}}\right)$$$$p_{i, 2j+1} = \cos\left(\frac{i}{10000^{2j/d}}\right)$$
#### **编码特性**

1. **绝对位置信息**：
    
    - 利用三角函数的不同频率来编码位置。
        
    - **高维度 (右侧)**：频率低，变化慢。
        
    - **低维度 (左侧)**：频率高，变化快。
        
    - _类比_：类似二进制表示法（低位比特交替快，高位比特交替慢），用连续浮点数空间高效表达位置。
        
2. **相对位置信息**：
    
    - 对于固定的偏移量 $\delta$，位置 $i+\delta$ 的编码可以表示为位置 $i$ 编码的**线性投影**。
        
    - 这意味着模型可以学习到词元之间的相对距离（如“后一个词”或“前三个词”）。
        

---

## 3. 小结
### self-attention特点：

1. **全量依赖**：自注意力的 $Q, K, V$ 同源，使其能捕捉序列内部任意两个词元间的关系。
    
2. **优劣权衡**：自注意力具备最高并行度和最短路径长度（$O(1)$），但计算复杂度随序列长度呈平方级增长（$O(n^2)$），不适合超长序列。
    
3. **顺序注入**：位置编码（如正弦/余弦编码）解决了自注意力丢失顺序信息的问题，且兼具绝对和相对位置表达能力。
### **适用场景**：
**CNN**: 局部特征明显。
**RNN**: 资源受限或强序列依赖。
 **自注意力**: 长距离依赖捕捉需求高，且计算资源充足。


