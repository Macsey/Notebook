
## 1. 章节概览

本章主要探讨如何在序列数据中利用注意力机制捕捉**内部依赖**以及如何处理**序列顺序**问题，是理解 Transformer 架构的基石。

- **核心问题**：
    
    1. 如何利用注意力机制捕捉序列内部的依赖关系？
        
    2. 如何在无循环（Recurrent）结构中注入序列的顺序信息？
        
- **对比视角**：分析 CNN、RNN 与自注意力在处理序列数据时的优劣。
    

## 2. 核心内容

### 2.1 自注意力 

#### **定义与原理**

自注意力是注意力机制的一种特殊形式，其核心特征在于 **Query (查询)、Key (键)、Value (值) 均来自同一个输入序列**。

**机制**：序列中的每个词元（Token）都会关注序列中的所有其他词元。
数学表达：
设输入序列为 $x_1, \dots, x_n$ (其中 $x_i \in \mathbb{R}^d$)，输出序列 $y_1, \dots, y_n$ 的计算如下：$$y_i = f(x_i, (x_1, x_1), \dots, (x_n, x_n)) \in \mathbb{R}^d$$$f$ 为注意力汇聚函数


### 2.2 对比：CNN vs RNN vs Self-Attention
![](assets/自注意力和位置编码/file-20251213183753710.png)

三种架构在处理序列数据时的性能指标对比如下：

| **架构**   | **计算复杂度**                | **并行度 (顺序操作)** | **最大路径长度 (远距离依赖)** | **核心特点**                               |
| -------- | ------------------------ | -------------- | ------------------ | -------------------------------------- |
| **CNN**  | $O(k \cdot n \cdot d^2)$ | $O(1)$ (高并行)   | $O(n/k)$           | 分层捕捉局部依赖，需多层堆叠才能覆盖长距离。($k$=卷积核大小)      |
| **RNN**  | $O(n \cdot d^2)$         | $O(n)$ (无并行)   | $O(n)$             | 逐词元处理，天然捕捉顺序，但并行差，易梯度消失/爆炸。            |
| **自注意力** | $O(n^2 \cdot d)$         | $O(1)$ (高并行)   | $O(1)$             | **捕捉长距离依赖能力最强**，直接连接所有词元。但序列过长时计算成本极高。 |

**总结**：自注意力在并行性和长距离依赖捕捉上最优，但在长序列（$n$ 很大）场景下计算压力最大。

### 2.3 位置编码 

#### **设计动机**

由于自注意力机制并行计算输入序列，它具有**排列不变性**（Permutation Invariant），即无法区分“我爱你”和“你爱我”中词元的先后顺序。因此，必须显式注入位置信息。
#### **实现：固定位置编码 (正弦/余弦函数)**
操作：构建一个与输入 $X$ 形状相同的矩阵 $P$，执行加法操作：
$$\text{Input} = X + P$$
公式：对于矩阵 $P$ 中的第 $i$ 行（位置），第 $j$ 列（维度），计算如下：
$$p_{i, 2j} = \sin\left(\frac{i}{10000^{2j/d}}\right)$$$$p_{i, 2j+1} = \cos\left(\frac{i}{10000^{2j/d}}\right)$$
#### **编码特性**

1. **绝对位置信息**：
    
    - 利用三角函数的不同频率来编码位置。
        
    - **高维度 (右侧)**：频率低，变化慢。
        
    - **低维度 (左侧)**：频率高，变化快。
        
    - _类比_：类似二进制表示法（低位比特交替快，高位比特交替慢），用连续浮点数空间高效表达位置。
        
2. **相对位置信息**：
    
    - 对于固定的偏移量 $\delta$，位置 $i+\delta$ 的编码可以表示为位置 $i$ 编码的**线性投影**。
        
    - 这意味着模型可以学习到词元之间的相对距离（如“后一个词”或“前三个词”）。
        

---

## 3. 小结
### self-attention特点：

1. **全量依赖**：自注意力的 $Q, K, V$ 同源，使其能捕捉序列内部任意两个词元间的关系。
    
2. **优劣权衡**：自注意力具备最高并行度和最短路径长度（$O(1)$），但计算复杂度随序列长度呈平方级增长（$O(n^2)$），不适合超长序列。
    
3. **顺序注入**：位置编码（如正弦/余弦编码）解决了自注意力丢失顺序信息的问题，且兼具绝对和相对位置表达能力。
### **适用场景**：
**CNN**: 局部特征明显。
**RNN**: 资源受限或强序列依赖。
 **自注意力**: 长距离依赖捕捉需求高，且计算资源充足。




我们可以这样简单的理解它们的角色分工：

- **自注意力 (Self-Attention)**：解决**“理解”**的问题（弄清楚这句话内部的词与词之间的关系）。
- **位置编码 (Positional Encoding)**：解决**“顺序”**的问题（弄清楚谁在谁前面，因为 Transformer 本身是个“脸盲”，分不清先后）。

### 第一部分：自注意力 (Self-Attention)

我们在前面讲过 $Q, K, V$ 的概念。那么，“自”注意力里的“自”到底是什么意思？

#### 1. “自”的含义

在普通的 Attention（比如 Seq2Seq 里的 Bahdanau Attention）中：

- **Query** 来自 **Decoder**（我要翻译的下一个词）。
    
- **Key/Value** 来自 **Encoder**（原句子）。
    
- 这是**两拨不同的人**在交互（Cross-Attention）。
    

而在 **Self-Attention** 中：

- **Query, Key, Value** 全部来自 **同一个输入 $X$**。
    
- 公式：$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$。
    
- **比喻**：这就像在一个会议室里，**大家都是同事（来自同一个句子）**。每个人（Query）都要去和其他所有人（Key）握手，看看跟谁关系好，然后收集信息（Value）。
    

#### 2. 为什么要“自己关注自己”？

是为了解决**歧义**和**长距离依赖**。

举个例子："The animal didn't cross the street because **it** was too tired."

当模型读到 **"it"** 的时候，它怎么知道指代的是 "animal" 还是 "street"？

- **Self-Attention 的过程**：
    
    1. 拿 "it" 的 $Q$ 去扫描句子里的每个词。
        
    2. 扫描到 "street" 时，发现语义不匹配（马路不会累），权重低。
        
    3. 扫描到 "animal" 时，发现语义匹配（动物会累），**权重极高**。
        
    4. 最后 "it" 的向量更新时，就会吸收大量 "animal" 的信息。
        
- **结果**：经过这一层计算，"it" 这个词的向量里，就融合了 "animal" 的含义。
    

---

### 第二部分：位置编码 (Positional Encoding)

这是 Transformer 最反直觉、但也最天才的设计之一。

#### 1. 为什么必须要有它？

**因为 Transformer 是并行的（Parallel）。**

- **RNN/LSTM**：它是天生懂顺序的。它必须先读第一个词，再读第二个词。如果你把句子打乱 "I love you" 变成 "You love I"，RNN 算出来的结果完全不同。
    
- **Transformer**：它是一次性把整句话扔进网络。在 Self-Attention 的公式 $\text{softmax}(QK^T)V$ 里，并没有任何一项计算依赖于“谁在谁前面”。
    
    - 如果不加位置编码，对 Transformer 来说，**"Tom chased Jerry"** 和 **"Jerry chased Tom"** 是一模一样的（就像把一袋子词倒在地板上，没有顺序）。
        

所以，我们需要**人工**给每个词贴上一个“号码牌”，告诉模型它排第几。

#### 2. 怎么加？(Add vs Concat)

Transformer 采用的是**相加 (Element-wise Addition)**。

- **输入 Embedding**：代表词的含义（比如 512维）。
    
- **位置 Embedding**：代表位置的信息（也是 512维）。
    
- **最终输入** = 词义 + 位置。
    

你可能会问：**直接加起来，不会把原来的词义搞乱吗？**

- **直观解释**：不会。因为维度很高（512维），词义分布在一个子空间，位置分布在另一个子空间，它们混合后，模型依然能把它们区分开（这就好比你给黑白照片涂上颜色，既保留了轮廓，又增加了色彩信息）。
    

#### 3. 这一串奇怪的公式 (Sin/Cos)

论文中并没有使用简单的 1, 2, 3, 4 整数（因为句子长度不一，数值太大难归一化），也没有用可学习的参数（虽然也可以），而是用了正弦和余弦函数：

$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$

$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$$

**为什么要用 Sin/Cos？（面试常考）**

1. **每个位置的编码都是唯一的**：这就相当于给了每个位置一个独特的“纹身”。
    
2. **值是有界的**：都在 $[-1, 1]$ 之间，不会因为句子太长导致数值爆炸。
    
3. **相对位置线性关系 (最重要)**：
    
    - 我们希望模型不仅知道“绝对位置”（第3个），还能推算出“相对位置”（第5个在第3个后面2位）。
        
    - 根据三角函数公式：$\sin(A+B) = \sin A \cos B + \cos A \sin B$。
        
    - 这意味着，$PE(pos+k)$ 可以被表示为 $PE(pos)$ 的一个线性变换。**这让模型非常容易学到“词与词之间的距离”**。
        

---

### 三、 总结与对比

|**概念**|**核心任务**|**形象比喻**|**缺失后果**|
|---|---|---|---|
|**自注意力**|**理解上下文**。让词与词产生关联，解决指代和多义词问题。|**开会讨论**：大家根据话题，决定听谁的意见多一点。|模型变成“词袋模型”，只懂单个词，读不懂句子逻辑。|
|**位置编码**|**标记顺序**。弥补并行计算带来的序列信息缺失。|**座位号**：进场前给每个人发个号牌，防止乱坐。|"Tom bites Dog" 和 "Dog bites Tom" 被当成同一句话。|

一句话串联：

Transformer 先给每个词发个位置号码牌（位置编码），然后让这些词带着号码牌进入会场，开始互相交流（自注意力），从而彻底理解整句话的含义。