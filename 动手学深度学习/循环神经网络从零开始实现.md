# 用 “说人话” 的方式讲懂：循环神经网络从零实现

咱们先抛开 “循环神经网络”“隐状态” 这些吓人的词，用一个生活场景打比方：假设你要模仿作家写《时光机器》的续集，得先读透原著（学规律），再根据开头几个字（比如 “时间旅行者”）往下写。循环神经网络（RNN）干的就是这事 —— 先 “读” 文本学规律，再 “续写” 文字。接下来咱们一步步拆，保证每个环节都能懂。

## 一、先搞明白：咱们要做什么？

核心目标很简单：**让电脑学会 “拼字”**，比如给它开头 “time”，它能接着写出 “traveller...”，而且风格像《时光机器》原著。

为了实现这个目标，要走 3 步：

1. 把原著文本 “拆碎” 成电脑能懂的格式（数据准备）；

2. 搭一个 “学规律” 的模型（RNN 核心）；

3. 让模型练手、纠错，最后能用（训练与预测）。

## 二、第一步：把文本 “喂” 给电脑 —— 数据准备

电脑不认识 “a”“b” 这种字母，也不认识 “time” 这种单词，得先把文本 “翻译” 成数字，再整理成它能处理的 “批量套餐”。

### 1. 把文字变成数字：词表（vocab）

比如《时光机器》里有 “t、i、m、e、r...” 这些字符，咱们给每个字符编个号：

- “t”→0，“i”→1，“m”→2，“e”→3，“r”→4...

这个 “字符→编号” 的对照表，就是 “词表（vocab）”。这样一来，“time” 就变成了 [0,1,2,3]，电脑终于能 “读” 了。

### 2. 做 “批量套餐”：训练迭代器（train_iter）

咱们不能让电脑一次读完整本书（内存装不下），也不能一个字符一个读（太慢），所以要切成 “小份”：

- 比如每次读 32 句话（批量大小 batch_size=32），每句话读 35 个字符（时间步数 num_steps=35）。

这样就生成了 “训练迭代器”，像食堂的 “套餐窗口”，每次给电脑递一份 32×35 的数字表格，电脑按份学。

### 3. 给数字 “加维度”：独热编码

电脑学规律时，需要知道 “每个字符和其他字符的区别”。比如 “t（0）” 和 “i（1）”，不能只让电脑知道 “0≠1”，还要让它们变成 “互不重叠的向量”：

- “t（0）”→[1,0,0,0,...]（只有第 0 位是 1，其他都是 0）；

- “i（1）”→[0,1,0,0,...]（只有第 1 位是 1，其他都是 0）。

这个操作叫 “独热编码”，目的是让电脑更清楚 “每个字符的独特性”。

## 三、第二步：搭 “学规律” 的模型 ——RNN 核心

模型就像一个 “学写字的机器人”，得有 3 个关键部件：“记东西的脑子”（隐状态）、“算规律的公式”（参数）、“把想法变输出的嘴”（输出层）。

### 1. 给机器人装 “脑子”：隐状态（H）

你写句子时，会记住 “前半句说了啥”（比如写了 “time”，就知道下一个可能是 “traveller”）。模型的 “隐状态 H” 就是干这个的 —— 记住 “前面的字符是什么”，用来算 “下一个字符该是什么”。

- 初始时 “脑子是空的”，所以 H 一开始是全 0 的数字表（比如 32×100，32 是批量大小，100 是 “记忆单元数”，越多记的越细）；

- 每读一个字符，H 就会更新一次（比如读了 “t”，H 变成 [0.2, 0.5, ...]；再读 “i”，H 又变成 [0.3, 0.1, ...]），相当于 “脑子不断更新记忆”。

### 2. 给机器人装 “算规律的公式”：参数（W_xh、W_hh、W_hq 等）

模型学规律，本质是学 “怎么用前面的字符和记忆，算下一个字符”。这个 “计算规则” 由一堆参数决定，就像机器人的 “思考逻辑”：

- **W_xh**：把 “当前字符”（比如 “t” 的独热向量）转换成 “能和记忆结合的格式”；

- **W_hh**：把 “之前的记忆（H）” 转换成 “能和当前字符结合的格式”；

- **b_h**：给结合后的结果 “调个偏移”（比如让结果更符合实际规律）；

- 计算记忆更新：H = tanh (当前字符 ×W_xh + 旧 H×W_hh + b_h)。

这里的 “tanh” 是个 “压缩函数”，把结果限制在 [-1,1] 之间，避免 “记忆值太离谱”；

- **W_hq、b_q**：把更新后的记忆（H）转换成 “每个字符的概率”（比如 “下一个是 t 的概率 20%，是 r 的概率 50%”）。

### 3. 把部件装成完整机器人：RNNModelScratch 类

咱们把 “参数”“记忆初始化”“计算逻辑” 打包成一个 “机器人模板”（类），方便后续调用：

- 初始化时，先给机器人装 “参数”（W_xh、W_hh 等）；

- 调用时，输入字符→独热编码→更新记忆→输出下一个字符的概率；

- 每次开始学新内容时，先把 “记忆清零”（begin_state 函数）。

## 四、第三步：让机器人 “练手”—— 训练与预测

刚搭好的机器人是 “小白”，得通过 “做题 - 纠错 - 改进” 循环练手，最后才能学会写句子。

### 1. 先教机器人 “续写”：预测功能（predict_ch8）

咱们先教机器人 “根据开头写后续”，比如给开头 “time”，让它写后面 10 个字符。步骤很简单：

- **预热期**：先读 “t→i→m→e”，只更新记忆（H），不输出 —— 相当于让机器人先 “记住开头的意思”；

- **预测期**：根据当前记忆（H），算 “下一个字符的概率”，选概率最高的那个（比如概率最高的是 “r”），然后用 “r” 更新记忆，再算下一个，直到写够 10 个字符。

刚开始机器人写的是 “乱码”（比如 “timeiiiiii”），因为还没学规律。

### 2. 帮机器人 “纠错”：训练的关键技巧

训练时会遇到两个大问题，咱们得解决：

#### （1）问题 1：机器人 “脑子乱了”—— 梯度爆炸

机器人学的时候，会 “回头改之前的错误”（反向传播）。如果句子太长（比如 100 个字符），“改错的力度” 会越来越大（梯度值飙升），导致 “脑子一片混乱”（模型参数乱变，预测更差）。

**解决办法：梯度裁剪**

就像给 “改错力度” 设个上限（比如上限 1），如果 “力度超过 1”，就按比例缩小（比如力度是 2，就缩小到 1），保证 “改错不发疯”。

#### （2）问题 2：机器人 “记混批次”—— 隐状态处理

训练时是按 “批量” 学的（比如一批 32 句话），如果学下一批时，还带着上一批的 “记忆”，就会 “记混”（比如上一批是 “time”，下一批是 “the”，记忆里还留着 “time” 的信息）。

**解决办法：分情况清记忆**

- 如果批次是 “随机选的”（比如第一批是第 1-35 字符，第二批是第 100-134 字符）：学每批前都把记忆清零；

- 如果批次是 “按顺序来的”（比如第一批 1-35，第二批 36-70）：学下一批时，用上一批的最后记忆（但要 “切断梯度”，避免改错时牵连上一批）。

#### （3）正式训练：循环 “做题 - 纠错”

训练流程像 “老师带学生刷题”：

1. 准备 “题库”（train_iter）、“评分标准”（交叉熵损失，算预测和实际的差距）、“改进方法”（SGD 优化器，按差距调整参数）；

2. 每次学一批题（32×35 字符）：

- 清零记忆→读字符→算预测→算差距（损失）→反向改参数（先裁剪梯度，再调整）；

3. 重复学 500 轮（num_epochs=500），每轮结束看 “成绩”（困惑度，越低越好，理想是 1）；

4. 每 10 轮让机器人 “写一句”（比如用 “time traveller” 开头），看进步 —— 从 “乱码” 慢慢变成 “像原著的句子”。

## 五、最后：训练效果与总结

### 1. 训练后机器人能干嘛？

- 成绩：困惑度能降到 1 左右（接近 “完美预测”）；

- 续写：给 “time traveller” 开头，能写出 “time traveller for so it will be convenient to speak of him...”，和《时光机器》风格很像；

- 速度：用 GPU 的话，每秒能学几万字符，效率不低。

### 2. 关键知识点回顾

- RNN 的核心是 “用隐状态记前面的信息”，解决 “按顺序学” 的问题；

- 独热编码是 “让电脑懂字符”，梯度裁剪是 “防止模型发疯”；

- 训练时要注意 “清记忆”，预测时要先 “预热记忆”。

### 3. 可以再优化的方向

- 换 “更好的记忆方式”：比如用 Word2Vec 替代独热编码（让字符的数字更有意义，比如 “time” 和 “traveller” 的数字更接近）；

- 调 “续写策略”：不选 “概率最高的字符”，而是 “按概率随机选”（比如 50% 概率选 “r”，30% 选 “t”），让续写更灵活；

- 换 “更聪明的模型”：比如 LSTM、GRU（解决 RNN “记不住长句子” 的问题）—— 不过这是后面章节的内容啦。

到这一步，你就明白 “循环神经网络从零实现” 到底是怎么回事了：本质是给电脑装 “记忆” 和 “计算逻辑”，让它通过大量练习，学会 “按顺序拼字”。