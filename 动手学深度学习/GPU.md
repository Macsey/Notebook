
咱们平时用电脑时，默认都是靠 CPU 干活。但训练深度学习模型（比如识别图片、跑神经网络）时，CPU 就像 “一个人干很多细活”，速度慢得让人着急。而 GPU 呢，就像 “一群人一起干同类活”，特别擅长处理深度学习里大量重复的计算 —— 这也是为啥现在搞深度学习几乎离不开 GPU。下面咱们就顺着文档内容，用最通俗的话把 “怎么用 GPU 做深度学习” 讲明白。

## 一、先搞懂：GPU 是啥？为啥要用它？

你可以把 CPU 想象成 “全能手”，啥活都能干（比如打开软件、写文档、算数学题），但一次只能专心干几件事；而 GPU 原本是为了 “快速处理图像” 设计的（比如玩游戏时渲染画面），它的特点是 “有很多小核心”—— 就像一个工厂里有几十上百个工人，虽然每个工人只会干简单重复的活，但一起上就能飞快完成大量任务。

深度学习模型（比如神经网络）刚好需要做很多 “重复计算”：比如给一张图片里的每个像素做同样的运算，或者给成千上万的参数更新数值。这时候 GPU 的 “多核心优势” 就体现出来了 —— 原本 CPU 要算几小时的任务，GPU 可能几分钟就搞定。

文档里也提到：“自 2000 年以来，GPU 性能每十年增长 1000 倍”—— 这速度比 CPU 快多了，所以搞深度学习必须学会用 GPU。

## 二、第一步：确认你的电脑有 GPU，且能用上

不是所有电脑都有能跑深度学习的 GPU！目前主流的是**NVIDIA 显卡**（因为它有一套叫 “CUDA” 的工具，能让深度学习框架直接调用 GPU），AMD 显卡或笔记本自带的 “集成显卡” 基本用不了。

### 1. 先检查：有没有 GPU？

文档里给了个命令叫 nvidia-smi（可以在电脑的 “命令提示符” 或 “终端” 里输入），输完后会看到类似下面的内容：

```
+-----------------------------------------------------------------------------+| GPU  Name        状态   | 显存使用  | 占用率  ||===============================+======================+======================||   0  Tesla V100   正常   |  885MiB / 16160MiB |    0%     ||   1  Tesla V100   正常   |    0MiB / 16160MiB |    0%     |+-----------------------------------------------------------------------------+
```

这里的 “GPU 0”“GPU 1” 就是你电脑上的显卡（数字从 0 开始算），“显存” 就是 GPU 专用的内存（类似 CPU 的内存，用来存数据和计算结果）。如果输完命令提示 “找不到”，要么是没有 NVIDIA 显卡，要么是没装驱动。

### 2. 再装 “工具”：让深度学习框架认 GPU

有了 GPU 还不够，得装两个东西：

- **NVIDIA 驱动**：让电脑能识别显卡（就像给打印机装驱动才能用一样）；

- **CUDA 工具包**：让深度学习框架（比如 PyTorch、TensorFlow）能 “指挥” GPU 干活。

装完后，还要给框架装 “GPU 版本”：比如你之前装的是 “PyTorch CPU 版”，就得先删掉，再装 “PyTorch GPU 版”（文档里给了例子，比如pip install mxnet-cu100，这里的 “cu100” 就是对应 CUDA 10.0 版本）。

## 三、核心概念：“设备”—— 数据和模型要放在哪？

深度学习里有个关键词叫 “设备（Device）”，其实就是 “数据 / 模型存在哪里、在哪里计算”，主要分两种：

- **CPU 设备**：默认选项，数据存在电脑内存里，用 CPU 计算；

- **GPU 设备**：需要手动指定，数据存在 GPU 显存里，用 GPU 计算。

为啥要区分？因为**数据和模型必须在同一个设备上才能计算**！比如：你把模型放在 GPU 上，却把要计算的数据放在 CPU 上，电脑就会 “懵”—— 不知道该在哪算，直接报错。

### 1. 怎么表示 “设备”？

不同框架的写法有点不一样，但意思差不多：

|   |   |   |   |
|---|---|---|---|
|框架|CPU 设备怎么写|GPU 设备怎么写（第 0 块）|第 1 块 GPU 怎么写|
|PyTorch|torch.device('cpu')|torch.device('cuda')|torch.device('cuda:1')|
|TensorFlow|tf.device('/CPU:0')|tf.device('/GPU:0')|tf.device('/GPU:1')|
|MXNet|npx.cpu()|npx.gpu()|npx.gpu(1)|

文档里还给了两个 “偷懒函数”：try_gpu() 和 try_all_gpus()，作用是 “能找 GPU 就用 GPU，没有就用 CPU”—— 比如你写 try_gpu(1)，如果电脑有第 2 块 GPU（因为从 0 开始算）就用它，没有就自动用 CPU，这样代码在不同电脑上都能跑。

### 2. 怎么查 “数据 / 模型在哪个设备上”？

就像你要确认 “文件存在 C 盘还是 D 盘” 一样，得知道数据在哪：

- 比如用 PyTorch 创建一个数据 x = torch.tensor([1,2,3])，查它的设备就用 x.device，默认会显示 “cpu”；

- 如果数据在 GPU 上，会显示 “cuda:0”（表示第 0 块 GPU）。

## 四、关键操作：把数据和模型 “放到 GPU 上”

这是最核心的一步！只有把数据和模型都放到 GPU 上，才能用 GPU 计算。

### 1. 把 “数据” 放到 GPU 上

比如你要创建一个 2 行 3 列的全 1 数据，想让它存在第 0 块 GPU 上：

- PyTorch 写法：X = torch.ones(2, 3, device=try_gpu())（用device指定设备）；

- TensorFlow 写法：with try_gpu(): X = tf.ones((2, 3))（用 “上下文” 指定设备）；

- 创建后查一下 X.device，会显示 “cuda:0”，说明成功了。

如果想把数据从 “CPU 移到 GPU”，或者从 “GPU 0 移到 GPU 1”，就用 “复制” 操作：

- PyTorch 里用 Z = X.cuda(1)（把 X 从原来的设备复制到 GPU 1 上，新变量叫 Z）；

- 注意：复制不是 “移动”—— 原来的 X 还在原来的设备上，Z 是新的副本，会占用新设备的显存。

文档里特别提醒：**不要频繁在 CPU 和 GPU 之间复制数据**！因为数据传输速度比计算慢得多 —— 就像你在两个工厂之间运原材料，运的时间比加工时间还长，反而变慢了。

### 2. 把 “模型” 放到 GPU 上

深度学习模型（比如神经网络）其实就是一堆 “参数”（比如权重、偏置），把模型放到 GPU 上，本质是把这些参数放到 GPU 上。

比如你建一个简单的神经网络（只有一个线性层）：

- PyTorch 写法：

```
net = nn.Sequential(nn.Linear(3, 1))  # 建模型（输入3个特征，输出1个结果）net = net.to(device=try_gpu())        # 把模型放到GPU上
```

- 放完后，查一下模型参数的设备：net[0].weight.data.device，会显示 “cuda:0”，说明参数在 GPU 上了。

### 3. 计算时的 “潜规则”

只要数据和模型都在 GPU 上，计算结果会自动存在 GPU 上：

比如用上面的模型net计算数据X（都在 GPU 0 上），结果net(X)也会在 GPU 0 上，不用再手动指定。

## 五、避坑指南：这些错误别犯！

1. **数据和模型不在一个设备上**：比如模型在 GPU 上，数据在 CPU 上，计算时会报错。解决办法：先查两者的设备，不一样就复制过去。

2. **显存不够用**：GPU 的显存比电脑内存小（比如常见的 GPU 显存是 8G、16G），如果数据太大（比如一次加载太多图片），会提示 “显存不足”。解决办法：减小每次计算的数据量（比如 “批量大小” 调小），或者删除不用的变量释放显存。

3. **频繁打印 / 转换数据**：比如你在 GPU 上算完一个结果，马上用print()打印，或者转换成 NumPy 格式 —— 这会把数据从 GPU 复制回 CPU，不仅慢，还可能卡住。解决办法：尽量在 GPU 上完成所有计算，最后只把最终结果复制回 CPU。

## 六、总结：用 GPU 的 “三步法”

1. **准备工作**：确认有 NVIDIA GPU，装驱动和 CUDA，装框架的 GPU 版本；

2. **放数据**：创建数据时指定 GPU 设备，或把 CPU 数据复制到 GPU；

3. **放模型**：把模型参数放到和数据相同的 GPU 上；

4. **计算**：直接用模型算数据，结果自动在 GPU 上，最后按需复制回 CPU。

只要记住 “数据和模型必须在同一设备”，并且少做 “跨设备复制”，就能充分发挥 GPU 的速度优势啦！